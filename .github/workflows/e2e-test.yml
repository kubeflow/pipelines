name: KFP E2E Pipeline tests
env:
  E2E_TESTS_DIR: "./backend/test/end2end"
  REPORTS_DIR: "./backend/test/end2end/reports"
  TESTS_LABEL: "E2E"
  NUMBER_OF_PARALLEL_NODES: 15
  NAMESPACE: "kubeflow"

on:
  push:
    branches: [master]

  pull_request:
    paths:
      - '.github/workflows/e2e-test.yml'
      - '.github/resources/**'
      - 'api/**'
      - 'go.mod'
      - 'go.sum'
      - 'backend/**'
      - 'proxy/**'
      - 'manifests/kustomize/**'
      - '!**/*.md'
      - '!**/OWNERS'


  workflow_dispatch:
    inputs:
      test_label:
        description: "Test label that you want to filter on and run"
        default: 'ApiServerTests'
        required: true
        type: string
      number_of_parallel_tests:
        description: "Number of ginkgo nodes that you want run in parallel, it essentially is equivalent to number of parallel tests with some caveats"
        default: 10
        required: true
        type: number
      namespace:
        description: "Namespace where you want to create your pipelines in"
        default: "kubeflow"
        required: true
        type: string

jobs:
  build:
    uses: ./.github/workflows/image-builds-with-cache.yml

  end-to-end-critical-scenario-tests:
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        k8s_version: ["v1.31.0"]
        cache_enabled: [true, false]
        argo_version: [ "v3.6.10", "v3.5.15"]
        include:
          - k8s_version: "v1.29.2"
            cache_enabled: false
            argo_version: "v3.5.15"
      fail-fast: false
    name: End to End Critical Scenario Tests - K8s ${{ matrix.k8s_version }} cacheEnabled=${{ matrix.cache_enabled }} argoVersion=${{  matrix.argo_version}}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Free up disk space
        run: ./.github/resources/scripts/free-disk-space.sh

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Create KFP cluster
        id: create-kfp-cluster
        uses: ./.github/actions/kfp-cluster
        with:
          k8s_version: ${{ matrix.k8s_version }}
          image_path: ${{ needs.build.outputs.IMAGE_PATH }}
          image_tag: ${{ needs.build.outputs.IMAGE_TAG }}
          image_registry: ${{ needs.build.outputs.IMAGE_REGISTRY }}
          cache_enabled: ${{ matrix.cache_enabled }}
          argo_version: ${{ matrix.argo_version }}
        continue-on-error: true

      - name: Forward API port
        id: forward-api-port
        if: ${{ steps.create-kfp-cluster.outcome == 'success' }}
        run: ./.github/resources/scripts/forward-port.sh "kubeflow" "ml-pipeline" 8888 8888
        continue-on-error: true


      - name: E2E Critical Scenarios Tests
        id: e2e-critical-tests
        if: ${{ steps.forward-api-port.outcome == 'success' }}
        working-directory: ${{ env.E2E_TESTS_DIR }}
        run: |
          NUMBER_OF_NODES=${{ env.NUMBER_OF_PARALLEL_NODES }}
          TEST_LABEL="E2ECritical"
          NAMESPACE=${{ env.NAMESPACE }}
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            NUMBER_OF_NODES=${{ inputs.number_of_parallel_tests }}
            TEST_LABEL=${{ inputs.test_label }}
            NAMESPACE=${{ inputs.namespace }}
          fi
          go run github.com/onsi/ginkgo/v2/ginkgo -r -v --cover -p --keep-going --github-output=true --nodes=$NUMBER_OF_NODES -v --label-filter=$TEST_LABEL -- -namespace=$NAMESPACE -cacheEnabled=${{ matrix.cache_enabled }}
        continue-on-error: true

      - name: Collect Pod logs in case of Test Failures
        id: collect-logs
        if: ${{ steps.e2e-critical-tests.outcome != 'success' || steps.create-kfp-cluster.outcome != 'success' }}
        run: |
          NAMESPACE=${{ env.NAMESPACE }}
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            NAMESPACE=${{ inputs.namespace }}
          fi
          ./.github/resources/scripts/collect-logs.sh --ns $NAMESPACE --output /tmp/tmp_pod_log.txt

      - name: Publish Test Summary
        id: publish
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: (!cancelled()) && steps.collect-logs.outcome != 'failure'
        with:
          files: |
            ${{ env.REPORTS_DIR }}/*.xml

      - name: Install Junit2Html plugin and generate report
        if: (!cancelled()) && steps.collect-logs.outcome != 'failure'
        run: |
          go run github.com/alexec/junit2html@latest < ${{ env.REPORTS_DIR }}/e2e_junit.xml > ${{ env.REPORTS_DIR }}/test-report.html
        continue-on-error: true

      - name: Generate UUID
        id: uuid_gen
        run: echo "uuid=$(uuidgen)" >> $GITHUB_OUTPUT

      - name: Upload HTML Report
        id: upload
        uses: actions/upload-artifact@v4
        if: (!cancelled())
        with:
          name: HTML Report - ${{ github.run_id }}_${{ github.job }}_${{ steps.uuid_gen.outputs.uuid }}
          path: ${{ env.REPORTS_DIR }}/test-report.html
          retention-days: 30
        continue-on-error: true

      - name: Mark Workflow failure if test step failed
        if: steps.e2e-critical-tests.outcome != 'success' && !cancelled()
        run: exit 1

      - name: Mark Workflow failure if test reporting failed
        if: (steps.publish.outcome == 'failure' || steps.upload.outcome != 'success') && !cancelled()
        run: exit 1


  end-to-end-proxy-tests:
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        k8s_version: ["v1.31.0"]
        cache_enabled: [false]
        proxy: [true]
    name: End to End Proxy Tests - K8s ${{ matrix.k8s_version }} cacheEnabled=${{ matrix.cache_enabled }} proxyEnabled=${{ matrix.proxy }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Free up disk space
        run: ./.github/resources/scripts/free-disk-space.sh

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Create KFP cluster
        id: create-kfp-cluster
        uses: ./.github/actions/kfp-cluster
        with:
          k8s_version: ${{ matrix.k8s_version }}
          image_path: ${{ needs.build.outputs.IMAGE_PATH }}
          image_tag: ${{ needs.build.outputs.IMAGE_TAG }}
          image_registry: ${{ needs.build.outputs.IMAGE_REGISTRY }}
          cache_enabled: ${{ matrix.cache_enabled }}
          proxy: ${{ matrix.proxy }}
        continue-on-error: true

      - name: Forward API port
        id: forward-api-port
        if: ${{ steps.create-kfp-cluster.outcome == 'success' }}
        run: ./.github/resources/scripts/forward-port.sh "kubeflow" "ml-pipeline" 8888 8888
        continue-on-error: true


      - name: E2E Proxy Tests
        id: e2e-proxy-tests
        if: ${{ steps.forward-api-port.outcome == 'success' }}
        working-directory: ${{ env.E2E_TESTS_DIR }}
        run: |
          NUMBER_OF_NODES=${{ env.NUMBER_OF_PARALLEL_NODES }}
          TEST_LABEL="E2EProxy"
          NAMESPACE=${{ env.NAMESPACE }}
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            NUMBER_OF_NODES=${{ inputs.number_of_parallel_tests }}
            TEST_LABEL=${{ inputs.test_label }}
            NAMESPACE=${{ inputs.namespace }}
          fi
          go run github.com/onsi/ginkgo/v2/ginkgo -r -v --cover -p --keep-going --github-output=true --nodes=$NUMBER_OF_NODES -v --label-filter=$TEST_LABEL -- -namespace=$NAMESPACE -useProxy=${{ matrix.proxy }} -cacheEnabled=${{ matrix.cache_enabled }}
        continue-on-error: true

      - name: Collect Pod logs in case of Test Failures
        id: collect-logs
        if: ${{ steps.e2e-proxy-tests.outcome != 'success' || steps.create-kfp-cluster.outcome != 'success' }}
        run: |
          NAMESPACE=${{ env.NAMESPACE }}
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            NAMESPACE=${{ inputs.namespace }}
          fi
          ./.github/resources/scripts/collect-logs.sh --ns $NAMESPACE --output /tmp/tmp_pod_log.txt

      - name: Publish Test Summary
        id: publish
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: (!cancelled()) && steps.collect-logs.outcome != 'failure'
        with:
          files: |
            ${{ env.REPORTS_DIR }}/*.xml

      - name: Install Junit2Html plugin and generate report
        if: (!cancelled()) && steps.collect-logs.outcome != 'failure'
        run: |
          go run github.com/alexec/junit2html@latest < ${{ env.REPORTS_DIR }}/e2e_junit.xml > ${{ env.REPORTS_DIR }}/test-report.html
        continue-on-error: true

      - name: Generate UUID
        id: uuid_gen
        run: echo "uuid=$(uuidgen)" >> $GITHUB_OUTPUT

      - name: Upload HTML Report
        id: upload
        uses: actions/upload-artifact@v4
        if: (!cancelled())
        with:
          name: HTML Report - ${{ github.run_id }}_${{ github.job }}_${{ steps.uuid_gen.outputs.uuid }}
          path: ${{ env.REPORTS_DIR }}/test-report.html
          retention-days: 30
        continue-on-error: true

      - name: Mark Workflow failure if test step failed
        if: steps.e2e-critical-tests.outcome != 'success' && !cancelled()
        run: exit 1

      - name: Mark Workflow failure if test reporting failed
        if: (steps.publish.outcome == 'failure' || steps.upload.outcome != 'success') && !cancelled()
        run: exit 1


  end-to-end-scenario-tests:
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        include:
          - k8s_version: "v1.31.0"
            cache_enabled: false
            proxy: false
    name: End to End Scenario Tests - K8s ${{ matrix.k8s_version }} cacheEnabled=${{ matrix.cache_enabled }} proxyEnabled=${{ matrix.proxy }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Free up disk space
        run: ./.github/resources/scripts/free-disk-space.sh

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Create KFP cluster
        id: create-kfp-cluster
        uses: ./.github/actions/kfp-cluster
        with:
          k8s_version: ${{ matrix.k8s_version }}
          image_path: ${{ needs.build.outputs.IMAGE_PATH }}
          image_tag: ${{ needs.build.outputs.IMAGE_TAG }}
          image_registry: ${{ needs.build.outputs.IMAGE_REGISTRY }}
          cache_enabled: ${{ matrix.cache_enabled }}
          proxy: ${{ matrix.proxy }}
        continue-on-error: true

      - name: Forward API port
        id: forward-api-port
        if: ${{ steps.create-kfp-cluster.outcome == 'success' }}
        run: ./.github/resources/scripts/forward-port.sh "kubeflow" "ml-pipeline" 8888 8888
        continue-on-error: true


      - name: E2E Scenarios Tests
        id: e2e-tests
        if: ${{ steps.forward-api-port.outcome == 'success' }}
        working-directory: ${{ env.E2E_TESTS_DIR }}
        run: |
          NUMBER_OF_NODES=${{ env.NUMBER_OF_PARALLEL_NODES }}
          TEST_LABEL="E2ENonCritical"
          NAMESPACE=${{ env.NAMESPACE }}
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            NUMBER_OF_NODES=${{ inputs.number_of_parallel_tests }}
            TEST_LABEL=${{ inputs.test_label }}
            NAMESPACE=${{ inputs.namespace }}
          fi
          go run github.com/onsi/ginkgo/v2/ginkgo -r -v --cover -p --keep-going --github-output=true --nodes=$NUMBER_OF_NODES -v --label-filter=$TEST_LABEL -- -namespace=$NAMESPACE -useProxy=${{ matrix.proxy }} -cacheEnabled=${{ matrix.cache_enabled }}
        continue-on-error: true

      - name: Collect Pod logs in case of Test Failures
        id: collect-logs
        if: ${{ steps.e2e-tests.outcome != 'success' || steps.create-kfp-cluster.outcome != 'success' }}
        run: |
          NAMESPACE=${{ env.NAMESPACE }}
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            NAMESPACE=${{ inputs.namespace }}
          fi
          ./.github/resources/scripts/collect-logs.sh --ns $NAMESPACE --output /tmp/tmp_pod_log.txt

      - name: Publish Test Summary
        id: publish
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: (!cancelled()) && steps.collect-logs.outcome != 'failure'
        with:
          files: |
            ${{ env.REPORTS_DIR }}/*.xml

      - name: Install Junit2Html plugin and generate report
        if: (!cancelled()) && steps.collect-logs.outcome != 'failure'
        run: |
          go run github.com/alexec/junit2html@latest < ${{ env.REPORTS_DIR }}/e2e_junit.xml > ${{ env.REPORTS_DIR }}/test-report.html
        continue-on-error: true

      - name: Generate UUID
        id: uuid_gen
        run: echo "uuid=$(uuidgen)" >> $GITHUB_OUTPUT

      - name: Upload HTML Report
        id: upload
        uses: actions/upload-artifact@v4
        if: (!cancelled())
        with:
          name: HTML Report - ${{ github.run_id }}_${{ github.job }}_${{ steps.uuid_gen.outputs.uuid }}
          path: ${{ env.REPORTS_DIR }}/test-report.html
          retention-days: 30
        continue-on-error: true

      - name: Mark Workflow failure if test step failed
        if: steps.e2e-critical-tests.outcome != 'success' && !cancelled()
        run: exit 1

      - name: Mark Workflow failure if test reporting failed
        if: (steps.publish.outcome == 'failure' || steps.upload.outcome != 'success') && !cancelled()
        run: exit 1

