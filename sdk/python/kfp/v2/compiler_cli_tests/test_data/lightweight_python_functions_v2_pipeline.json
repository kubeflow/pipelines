{
  "pipelineSpec": {
    "deploymentSpec": {
      "executors": {
        "exec-train": {
          "container": {
            "image": "python:3.7",
            "command": [
              "sh",
              "-ec",
              "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
              "\nimport json\nimport inspect\nfrom typing import *\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Classes for input/output types in KFP SDK.\n\nThese are only compatible with v2 Pipelines.\n\"\"\"\n\nimport os\nfrom typing import Dict, List, Optional, Type, TypeVar\n\n\nclass Artifact(object):\n  \"\"\"Generic Artifact class.\n\n  This class is meant to represent the metadata around an input or output\n  machine-learning Artifact. Artifacts have URIs, which can either be a location\n  on disk (or Cloud storage) or some other resource identifier such as\n  an API resource name.\n\n  Artifacts carry a `metadata` field, which is a dictionary for storing\n  metadata related to this artifact.\n  \"\"\"\n  TYPE_NAME = \"system.Artifact\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    \"\"\"Initializes the Artifact with the given name, URI and metadata.\"\"\"\n    self.uri = uri or ''\n    self.name = name or ''\n    self.metadata = metadata or {}\n\n\nclass Model(Artifact):\n  \"\"\"An artifact representing an ML Model.\"\"\"\n  TYPE_NAME = \"system.Model\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n  @property\n  def framework(self) -> str:\n    return self._get_framework()\n\n  def _get_framework(self) -> str:\n    return self.metadata.get('framework', '')\n\n  @framework.setter\n  def framework(self, framework: str):\n    self._set_framework(framework)\n\n  def _set_framework(self, framework: str):\n    self.metadata['framework'] = framework\n\n\nclass Dataset(Artifact):\n  \"\"\"An artifact representing an ML Dataset.\"\"\"\n  TYPE_NAME = \"system.Dataset\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n\nclass Metrics(Artifact):\n  \"\"\"Represent a simple base Artifact type to store key-value scalar metrics.\n  \"\"\"\n  TYPE_NAME = \"system.Metrics\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n  def log_metric(self, metric: str, value: float):\n    \"\"\"Sets a custom scalar metric.\n\n    Args:\n      metric: Metric key\n      value: Value of the metric.\n    \"\"\"\n    self.metadata[metric] = value\n\nclass ClassificationMetrics(Artifact):\n  \"\"\"Represents Artifact class to store Classification Metrics.\"\"\"\n  TYPE_NAME = \"system.ClassificationMetrics\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n  def log_roc_reading(self, threshold: float, tpr: float, fpr: float):\n    \"\"\"Logs a single data point in the ROC Curve.\n\n    Args:\n      threshold: Thresold value for the data point.\n      tpr: True positive rate value of the data point.\n      fpr: False positive rate value of the data point.\n    \"\"\"\n\n    roc_reading = {'confidenceThreshold': threshold, 'recall': tpr, 'falsePositiveRate': fpr}\n    if 'confidenceMetrics' not in self.metadata.keys():\n      self.metadata['confidenceMetrics'] = []\n\n    self.metadata['confidenceMetrics'].append(roc_reading)\n\n  def load_roc_readings(self, readings: List[List[float]]):\n    \"\"\"Supports bulk loading ROC Curve readings.\n\n    Args:\n      readings: A 2-D list providing ROC Curve data points.\n                The expected order of the data points is:\n                  threshold, true_positive_rate, false_positive_rate.\n    \"\"\"\n    self.metadata['confidenceMetrics'] = []\n    for reading in readings:\n      if len(reading) != 3:\n        raise ValueError('Invalid ROC curve reading provided: {}. \\\n          Expected 3 values.'.format(reading))\n\n      self.log_roc_reading(reading[0], reading[1], reading[2])\n\n  def set_confusion_matrix_categories(self, categories: List[str]):\n    \"\"\"Stores confusion matrix categories.\n\n    Args:\n      categories: List of strings specifying the categories.\n    \"\"\"\n\n    self._categories =[]\n    annotation_specs = []\n    for category in categories:\n      annotation_spec = {\n        'displayName' : category\n      }\n      self._categories.append(category)\n      annotation_specs.append(annotation_spec)\n\n    self._matrix = [\n        [ 0 for i in range(len(self._categories)) ]\n          for j in range(len(self._categories)) ]\n\n    self._confusion_matrix = {}\n    self._confusion_matrix['annotationSpecs'] = annotation_specs\n    self._confusion_matrix['rows'] = self._matrix\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\n  def log_confusion_matrix_row(self, row_category: str, row: List[float]):\n    \"\"\"Logs a confusion matrix row.\n\n    Args:\n      row_category: Category to which the row belongs.\n      row: List of integers specifying the values for the row.\n\n     Raises:\n      ValueError: If row_category is not in the list of categories\n      set in set_categories call.\n    \"\"\"\n    if row_category not in self._categories:\n      raise ValueError('Invalid category: {} passed. Expected one of: {}'.\\\n        format(row_category, self._categories))\n\n    if len(row) != len(self._categories):\n      raise ValueError('Invalid row. Expected size: {} got: {}'.\\\n        format(len(self._categories), len(row)))\n\n    self._matrix[self._categories.index(row_category)] = row\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\n  def log_confusion_matrix_cell(self, row_category: str, col_category: str, value: int):\n    \"\"\"Logs a cell in the confusion matrix.\n\n    Args:\n      row_category: String representing the name of the row category.\n      col_category: String representing the name of the column category.\n      value: Int value of the cell.\n\n    Raises:\n      ValueError: If row_category or col_category is not in the list of\n       categories set in set_categories.\n    \"\"\"\n    if row_category not in self._categories:\n      raise ValueError('Invalid category: {} passed. Expected one of: {}'.\\\n        format(row_category, self._categories))\n\n    if col_category not in self._categories:\n      raise ValueError('Invalid category: {} passed. Expected one of: {}'.\\\n        format(row_category, self._categories))\n\n    self._matrix[self._categories.index(row_category)][\n      self._categories.index(col_category)] = value\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\n  def load_confusion_matrix(self, categories: List[str], matrix: List[List[int]]):\n    \"\"\"Supports bulk loading the whole confusion matrix.\n\n    Args:\n      categories: List of the category names.\n      matrix: Complete confusion matrix.\n\n    Raises:\n      ValueError: Length of categories does not match number of rows or columns.\n    \"\"\"\n    self.set_confusion_matrix_categories(categories)\n\n    if len(matrix) != len(categories):\n      raise ValueError('Invalid matrix: {} passed for categories: {}'.\\\n        format(matrix, categories))\n\n    for index in range(len(categories)):\n      if len(matrix[index]) != len(categories):\n        raise ValueError('Invalid matrix: {} passed for categories: {}'.\\\n          format(matrix, categories))\n\n      self.log_confusion_matrix_row(categories[index], matrix[index])\n\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\nT = TypeVar('T', bound=Artifact)\n\n_GCS_LOCAL_MOUNT_PREFIX = '/gcs/'\n\n\nclass _IOArtifact():\n  \"\"\"Internal wrapper class for representing Input/Output Artifacts.\"\"\"\n\n  def __init__(self, artifact_type: Type[T], artifact: Optional[T] = None):\n    self.type = artifact_type\n    self._artifact = artifact\n\n  def get(self) -> T:\n    return self._artifact\n\n  @property\n  def uri(self):\n    return self._artifact.uri\n\n  @uri.setter\n  def uri(self, uri):\n    self._artifact.uri = uri\n\n  @property\n  def path(self):\n    return self._get_path()\n\n  @path.setter\n  def path(self, path):\n    self._set_path(path)\n\n  def _get_path(self) -> str:\n    if self._artifact.uri.startswith('gs://'):\n      return _GCS_LOCAL_MOUNT_PREFIX + self._artifact.uri[len('gs://'):]\n\n  def _set_path(self, path):\n    if path.startswith(_GCS_LOCAL_MOUNT_PREFIX):\n      path = 'gs://' + path[len(_GCS_LOCAL_MOUNT_PREFIX):]\n    self._artifact.uri = path\n\n\nclass InputArtifact(_IOArtifact):\n\n  def __init__(self, artifact_type: Type[T], artifact: Optional[T] = None):\n    super().__init__(artifact_type=artifact_type, artifact=artifact)\n\n\nclass OutputArtifact(_IOArtifact):\n\n  def __init__(self, artifact_type: Type[T], artifact: Optional[T] = None):\n    super().__init__(artifact_type=artifact_type, artifact=artifact)\n    if artifact is not None:\n      os.makedirs(self.path, exist_ok=True)\n      self.path = os.path.join(self.path, 'data')\n\n\n_SCHEMA_TITLE_TO_TYPE: Dict[str, Artifact] = {\n    x.TYPE_NAME: x for x in [Artifact, Model, Dataset, Metrics, ClassificationMetrics]\n}\n\n\ndef create_runtime_artifact(runtime_artifact: Dict) -> Artifact:\n  \"\"\"Creates an Artifact instance from the specified RuntimeArtifact.\n\n  Args:\n    runtime_artifact: Dictionary representing JSON-encoded RuntimeArtifact.\n  \"\"\"\n  schema_title = runtime_artifact.get('type', {}).get('schemaTitle', '')\n\n  artifact_type = _SCHEMA_TITLE_TO_TYPE.get(schema_title)\n  if not artifact_type:\n    artifact_type = Artifact\n  return artifact_type(\n      uri=runtime_artifact.get('uri', ''),\n      name=runtime_artifact.get('name', ''),\n      metadata=runtime_artifact.get('metadata', {}),\n  )\n\nclass InputPath:\n    '''When creating component from function, :class:`.InputPath` should be used as function parameter annotation to tell the system to pass the *data file path* to the function instead of passing the actual data.'''\n    def __init__(self, type=None):\n        self.type = type\n\nclass OutputPath:\n    '''When creating component from function, :class:`.OutputPath` should be used as function parameter annotation to tell the system that the function wants to output data by writing it into a file with the given path instead of returning the data from the function.'''\n    def __init__(self, type=None):\n        self.type = type\n\nclass Executor():\n  \"\"\"Executor executes v2-based Python function components.\"\"\"\n\n  def __init__(self, executor_input: Dict, function_to_execute: Callable):\n    self._func = function_to_execute\n    self._input = executor_input\n    self._input_artifacts: Dict[str, InputArtifact] = {}\n    self._output_artifacts: Dict[str, OutputArtifact] = {}\n\n    for name, artifacts in self._input.get('inputs', {}).get('artifacts',\n                                                             {}).items():\n      artifacts_list = artifacts.get('artifacts')\n      if artifacts_list:\n        self._input_artifacts[name] = self._make_input_artifact(\n            artifacts_list[0])\n\n    for name, artifacts in self._input.get('outputs', {}).get('artifacts',\n                                                              {}).items():\n      artifacts_list = artifacts.get('artifacts')\n      if artifacts_list:\n        self._output_artifacts[name] = self._make_output_artifact(\n            artifacts_list[0])\n\n    self._return_annotation = inspect.signature(self._func).return_annotation\n    self._executor_output = {}\n\n  @classmethod\n  def _make_input_artifact(cls, runtime_artifact: Dict):\n    artifact = create_runtime_artifact(runtime_artifact)\n    return InputArtifact(artifact_type=type(artifact), artifact=artifact)\n\n  @classmethod\n  def _make_output_artifact(cls, runtime_artifact: Dict):\n    artifact = create_runtime_artifact(runtime_artifact)\n    return OutputArtifact(artifact_type=type(artifact), artifact=artifact)\n\n  def _get_input_artifact(self, name: str):\n    return self._input_artifacts.get(name)\n\n  def _get_output_artifact(self, name: str):\n    return self._output_artifacts.get(name)\n\n  def _get_input_parameter_value(self, parameter_name: str):\n    parameter = self._input.get('inputs', {}).get('parameters',\n                                                  {}).get(parameter_name, None)\n    if parameter is None:\n      return None\n\n    if parameter.get('stringValue'):\n      return parameter['stringValue']\n    elif parameter.get('intValue'):\n      return int(parameter['intValue'])\n    elif parameter.get('doubleValue'):\n      return float(parameter['doubleValue'])\n\n  def _get_output_parameter_path(self, parameter_name: str):\n    parameter_name = self._maybe_strip_path_suffix(parameter_name)\n    parameter = self._input.get('outputs',\n                                {}).get('parameters',\n                                        {}).get(parameter_name, None)\n    if parameter is None:\n      return None\n\n    return parameter.get('outputFile', None)\n\n  def _get_output_artifact_path(self, artifact_name: str):\n    artifact_name = self._maybe_strip_path_suffix(artifact_name)\n    output_artifact = self._output_artifacts.get(artifact_name)\n    if not output_artifact:\n      raise ValueError(\n          'Failed to get OutputArtifact path for artifact name {}'.format(\n              artifact_name))\n    return output_artifact.path\n\n  def _get_input_artifact_path(self, artifact_name: str):\n    artifact_name = self._maybe_strip_path_suffix(artifact_name)\n    input_artifact = self._input_artifacts.get(artifact_name)\n    if not input_artifact:\n      raise ValueError(\n          'Failed to get InputArtifact path for artifact name {}'.format(\n              artifact_name))\n    return input_artifact.path\n\n  def _write_output_parameter_value(self, name: str, value: Union[str, int,\n                                                                  float]):\n    if type(value) == str:\n      output = {\"stringValue\": value}\n    elif type(value) == int:\n      output = {\"intValue\": value}\n    elif type(value) == float:\n      output = {\"doubleValue\": value}\n    else:\n      raise RuntimeError(\n          'Expected value of type str, int or float, got {} instead for value {}'\n          .format(type(value), value))\n\n    if not self._executor_output.get('parameters'):\n      self._executor_output['parameters'] = {}\n\n    self._executor_output['parameters'][name] = output\n\n  def _write_output_artifact_payload(self, name: str, value: Any):\n    path = self._get_output_artifact_path(name)\n    with open(path, 'w') as f:\n      f.write(str(value))\n\n  @classmethod\n  def _is_parameter(cls, annotation: Any) -> bool:\n    return annotation in [str, int, float]\n\n  @classmethod\n  def _is_artifact(cls, annotation: Any) -> bool:\n    if type(annotation) == type:\n      return issubclass(annotation, Artifact)\n    return False\n\n  @classmethod\n  def _is_named_tuple(cls, annotation: Any) -> bool:\n    if type(annotation) == type:\n      return issubclass(annotation, tuple) and hasattr(\n          annotation, '_fields') and hasattr(annotation, '__annotations__')\n    return False\n\n  def _handle_single_return_value(self, output_name: str, annotation_type: Any,\n                                  return_value: Any):\n    if self._is_parameter(annotation_type):\n      if type(return_value) != annotation_type:\n        raise ValueError(\n            'Function `{}` returned value of type {}; want type {}'.format(\n                self._func.__name__, type(return_value), annotation_type))\n      self._write_output_parameter_value(output_name, return_value)\n    elif self._is_artifact(annotation_type):\n      self._write_output_artifact_payload(output_name, return_value)\n    else:\n      raise RuntimeError(\n          'Unknown return type: {}. Must be one of `str`, `int`, `float`, or a'\n          ' subclass of `Artifact`'.format(annotation_type))\n\n  def _write_executor_output(self, func_output: Optional[Any] = None):\n    if self._output_artifacts:\n      self._executor_output['artifacts'] = {}\n\n    for name, artifact in self._output_artifacts.items():\n      runtime_artifact = {\n          \"name\": artifact.get().name,\n          \"uri\": artifact.get().uri,\n          \"metadata\": artifact.get().metadata,\n      }\n      artifacts_list = {'artifacts': [runtime_artifact]}\n\n      self._executor_output['artifacts'][name] = artifacts_list\n\n    if func_output is not None:\n      if self._is_parameter(self._return_annotation) or self._is_artifact(\n          self._return_annotation):\n        # Note: single output is named `Output` in component.yaml.\n        self._handle_single_return_value('Output', self._return_annotation,\n                                         func_output)\n      elif self._is_named_tuple(self._return_annotation):\n        if len(self._return_annotation._fields) != len(func_output):\n          raise RuntimeError(\n              'Expected {} return values from function `{}`, got {}'.format(\n                  len(self._return_annotation._fields), self._func.__name__,\n                  len(func_output)))\n        for field in self._return_annotation._fields:\n          field_type = self._return_annotation.__annotations__[field]\n          field_value = getattr(func_output, field)\n          self._handle_single_return_value(field, field_type, field_value)\n      else:\n        raise RuntimeError(\n            'Unknown return type: {}. Must be one of `str`, `int`, `float`, a'\n            ' subclass of `Artifact`, or a NamedTuple collection of these types.'\n            .format(self._return_annotation))\n\n    import os\n    os.makedirs(os.path.dirname(self._input['outputs']['outputFile']),\n                exist_ok=True)\n    with open(self._input['outputs']['outputFile'], 'w') as f:\n      f.write(json.dumps(self._executor_output))\n\n  def _maybe_strip_path_suffix(self, name) -> str:\n    if name.endswith('_path'):\n      name = name[0:-len('_path')]\n    if name.endswith('_file'):\n      name = name[0:-len('_file')]\n    return name\n\n  def execute(self):\n    annotations = inspect.getfullargspec(self._func).annotations\n\n    # Function arguments.\n    func_kwargs = {}\n\n    for k, v in annotations.items():\n      if k == 'return':\n        continue\n\n      if v in [str, float, int]:\n        func_kwargs[k] = self._get_input_parameter_value(k)\n\n      if isinstance(v, OutputPath):\n        if v.type in [str, float, int]:\n          func_kwargs[k] = self._get_output_parameter_path(k)\n        else:\n          func_kwargs[k] = self._get_output_artifact_path(k)\n\n      if isinstance(v, InputPath):\n        func_kwargs[k] = self._get_input_artifact_path(k)\n\n      if isinstance(v, InputArtifact):\n        func_kwargs[k] = self._get_input_artifact(k)\n\n      if isinstance(v, OutputArtifact):\n        func_kwargs[k] = self._get_output_artifact(k)\n\n    result = self._func(**func_kwargs)\n    self._write_executor_output(result)\n\n\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use InputArtifact to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: InputArtifact(Dataset),\n    # An input parameter of type string.\n    message: str,\n    # Use OutputArtifact to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: OutputArtifact(Model),\n    # An input parameter of type int with a default value.\n    num_steps: int = 100):\n  '''Dummy Training step'''\n  with open(dataset_one_path, 'r') as input_file:\n    dataset_one_contents = input_file.read()\n\n  with open(dataset_two.path, 'r') as input_file:\n    dataset_two_contents = input_file.read()\n\n  line = \"dataset_one_contents: {} || dataset_two_contents: {} || message: {}\\n\".format(\n      dataset_one_contents, dataset_two_contents, message)\n\n  with open(model.path, 'w') as output_file:\n    for i in range(num_steps):\n      output_file.write(\"Step {}\\n{}\\n=====\\n\".format(i, line))\n\n  # Use model.get() to get a Model artifact, which has a .metadata dictionary\n  # to store arbitrary metadata for the output artifact.\n  model.get().metadata['accuracy'] = 0.9\n\n\ndef executor_main():\n  import argparse\n  import json\n\n  parser = argparse.ArgumentParser(description='Process some integers.')\n  parser.add_argument('--executor_input', type=str)\n  parser.add_argument('--function_to_execute', type=str)\n\n  args, _ = parser.parse_known_args()\n  executor_input = json.loads(args.executor_input)\n  function_to_execute = globals()[args.function_to_execute]\n\n  executor = Executor(executor_input=executor_input,\n                      function_to_execute=function_to_execute)\n\n  executor.execute()\n\n\nif __name__ == '__main__':\n  executor_main()\n"
            ],
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train",
              "--dataset-one-output-path",
              "{{$.inputs.artifacts['dataset_one'].path}}",
              "--dataset-two-output-path",
              "{{$.inputs.artifacts['dataset_two'].path}}",
              "--message-output-path",
              "{{$.inputs.parameters['message']}}",
              "--num-steps-output-path",
              "{{$.inputs.parameters['num_steps']}}",
              "--model-output-path",
              "{{$.outputs.artifacts['model'].path}}"
            ]
          }
        },
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess",
              "--message-output-path",
              "{{$.inputs.parameters['message']}}",
              "--output-dataset-one-output-path",
              "{{$.outputs.artifacts['output_dataset_one'].path}}",
              "--output-dataset-two-output-path",
              "{{$.outputs.artifacts['output_dataset_two'].path}}",
              "--output-parameter-output-path",
              "{{$.outputs.parameters['output_parameter'].output_file}}"
            ],
            "image": "python:3.7",
            "command": [
              "sh",
              "-ec",
              "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
              "\nimport json\nimport inspect\nfrom typing import *\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Classes for input/output types in KFP SDK.\n\nThese are only compatible with v2 Pipelines.\n\"\"\"\n\nimport os\nfrom typing import Dict, List, Optional, Type, TypeVar\n\n\nclass Artifact(object):\n  \"\"\"Generic Artifact class.\n\n  This class is meant to represent the metadata around an input or output\n  machine-learning Artifact. Artifacts have URIs, which can either be a location\n  on disk (or Cloud storage) or some other resource identifier such as\n  an API resource name.\n\n  Artifacts carry a `metadata` field, which is a dictionary for storing\n  metadata related to this artifact.\n  \"\"\"\n  TYPE_NAME = \"system.Artifact\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    \"\"\"Initializes the Artifact with the given name, URI and metadata.\"\"\"\n    self.uri = uri or ''\n    self.name = name or ''\n    self.metadata = metadata or {}\n\n\nclass Model(Artifact):\n  \"\"\"An artifact representing an ML Model.\"\"\"\n  TYPE_NAME = \"system.Model\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n  @property\n  def framework(self) -> str:\n    return self._get_framework()\n\n  def _get_framework(self) -> str:\n    return self.metadata.get('framework', '')\n\n  @framework.setter\n  def framework(self, framework: str):\n    self._set_framework(framework)\n\n  def _set_framework(self, framework: str):\n    self.metadata['framework'] = framework\n\n\nclass Dataset(Artifact):\n  \"\"\"An artifact representing an ML Dataset.\"\"\"\n  TYPE_NAME = \"system.Dataset\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n\nclass Metrics(Artifact):\n  \"\"\"Represent a simple base Artifact type to store key-value scalar metrics.\n  \"\"\"\n  TYPE_NAME = \"system.Metrics\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n  def log_metric(self, metric: str, value: float):\n    \"\"\"Sets a custom scalar metric.\n\n    Args:\n      metric: Metric key\n      value: Value of the metric.\n    \"\"\"\n    self.metadata[metric] = value\n\nclass ClassificationMetrics(Artifact):\n  \"\"\"Represents Artifact class to store Classification Metrics.\"\"\"\n  TYPE_NAME = \"system.ClassificationMetrics\"\n\n  def __init__(self,\n               name: Optional[str] = None,\n               uri: Optional[str] = None,\n               metadata: Optional[Dict] = None):\n    super().__init__(uri=uri, name=name, metadata=metadata)\n\n  def log_roc_reading(self, threshold: float, tpr: float, fpr: float):\n    \"\"\"Logs a single data point in the ROC Curve.\n\n    Args:\n      threshold: Thresold value for the data point.\n      tpr: True positive rate value of the data point.\n      fpr: False positive rate value of the data point.\n    \"\"\"\n\n    roc_reading = {'confidenceThreshold': threshold, 'recall': tpr, 'falsePositiveRate': fpr}\n    if 'confidenceMetrics' not in self.metadata.keys():\n      self.metadata['confidenceMetrics'] = []\n\n    self.metadata['confidenceMetrics'].append(roc_reading)\n\n  def load_roc_readings(self, readings: List[List[float]]):\n    \"\"\"Supports bulk loading ROC Curve readings.\n\n    Args:\n      readings: A 2-D list providing ROC Curve data points.\n                The expected order of the data points is:\n                  threshold, true_positive_rate, false_positive_rate.\n    \"\"\"\n    self.metadata['confidenceMetrics'] = []\n    for reading in readings:\n      if len(reading) != 3:\n        raise ValueError('Invalid ROC curve reading provided: {}. \\\n          Expected 3 values.'.format(reading))\n\n      self.log_roc_reading(reading[0], reading[1], reading[2])\n\n  def set_confusion_matrix_categories(self, categories: List[str]):\n    \"\"\"Stores confusion matrix categories.\n\n    Args:\n      categories: List of strings specifying the categories.\n    \"\"\"\n\n    self._categories =[]\n    annotation_specs = []\n    for category in categories:\n      annotation_spec = {\n        'displayName' : category\n      }\n      self._categories.append(category)\n      annotation_specs.append(annotation_spec)\n\n    self._matrix = [\n        [ 0 for i in range(len(self._categories)) ]\n          for j in range(len(self._categories)) ]\n\n    self._confusion_matrix = {}\n    self._confusion_matrix['annotationSpecs'] = annotation_specs\n    self._confusion_matrix['rows'] = self._matrix\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\n  def log_confusion_matrix_row(self, row_category: str, row: List[float]):\n    \"\"\"Logs a confusion matrix row.\n\n    Args:\n      row_category: Category to which the row belongs.\n      row: List of integers specifying the values for the row.\n\n     Raises:\n      ValueError: If row_category is not in the list of categories\n      set in set_categories call.\n    \"\"\"\n    if row_category not in self._categories:\n      raise ValueError('Invalid category: {} passed. Expected one of: {}'.\\\n        format(row_category, self._categories))\n\n    if len(row) != len(self._categories):\n      raise ValueError('Invalid row. Expected size: {} got: {}'.\\\n        format(len(self._categories), len(row)))\n\n    self._matrix[self._categories.index(row_category)] = row\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\n  def log_confusion_matrix_cell(self, row_category: str, col_category: str, value: int):\n    \"\"\"Logs a cell in the confusion matrix.\n\n    Args:\n      row_category: String representing the name of the row category.\n      col_category: String representing the name of the column category.\n      value: Int value of the cell.\n\n    Raises:\n      ValueError: If row_category or col_category is not in the list of\n       categories set in set_categories.\n    \"\"\"\n    if row_category not in self._categories:\n      raise ValueError('Invalid category: {} passed. Expected one of: {}'.\\\n        format(row_category, self._categories))\n\n    if col_category not in self._categories:\n      raise ValueError('Invalid category: {} passed. Expected one of: {}'.\\\n        format(row_category, self._categories))\n\n    self._matrix[self._categories.index(row_category)][\n      self._categories.index(col_category)] = value\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\n  def load_confusion_matrix(self, categories: List[str], matrix: List[List[int]]):\n    \"\"\"Supports bulk loading the whole confusion matrix.\n\n    Args:\n      categories: List of the category names.\n      matrix: Complete confusion matrix.\n\n    Raises:\n      ValueError: Length of categories does not match number of rows or columns.\n    \"\"\"\n    self.set_confusion_matrix_categories(categories)\n\n    if len(matrix) != len(categories):\n      raise ValueError('Invalid matrix: {} passed for categories: {}'.\\\n        format(matrix, categories))\n\n    for index in range(len(categories)):\n      if len(matrix[index]) != len(categories):\n        raise ValueError('Invalid matrix: {} passed for categories: {}'.\\\n          format(matrix, categories))\n\n      self.log_confusion_matrix_row(categories[index], matrix[index])\n\n    self.metadata['confusionMatrix'] = self._confusion_matrix\n\nT = TypeVar('T', bound=Artifact)\n\n_GCS_LOCAL_MOUNT_PREFIX = '/gcs/'\n\n\nclass _IOArtifact():\n  \"\"\"Internal wrapper class for representing Input/Output Artifacts.\"\"\"\n\n  def __init__(self, artifact_type: Type[T], artifact: Optional[T] = None):\n    self.type = artifact_type\n    self._artifact = artifact\n\n  def get(self) -> T:\n    return self._artifact\n\n  @property\n  def uri(self):\n    return self._artifact.uri\n\n  @uri.setter\n  def uri(self, uri):\n    self._artifact.uri = uri\n\n  @property\n  def path(self):\n    return self._get_path()\n\n  @path.setter\n  def path(self, path):\n    self._set_path(path)\n\n  def _get_path(self) -> str:\n    if self._artifact.uri.startswith('gs://'):\n      return _GCS_LOCAL_MOUNT_PREFIX + self._artifact.uri[len('gs://'):]\n\n  def _set_path(self, path):\n    if path.startswith(_GCS_LOCAL_MOUNT_PREFIX):\n      path = 'gs://' + path[len(_GCS_LOCAL_MOUNT_PREFIX):]\n    self._artifact.uri = path\n\n\nclass InputArtifact(_IOArtifact):\n\n  def __init__(self, artifact_type: Type[T], artifact: Optional[T] = None):\n    super().__init__(artifact_type=artifact_type, artifact=artifact)\n\n\nclass OutputArtifact(_IOArtifact):\n\n  def __init__(self, artifact_type: Type[T], artifact: Optional[T] = None):\n    super().__init__(artifact_type=artifact_type, artifact=artifact)\n    if artifact is not None:\n      os.makedirs(self.path, exist_ok=True)\n      self.path = os.path.join(self.path, 'data')\n\n\n_SCHEMA_TITLE_TO_TYPE: Dict[str, Artifact] = {\n    x.TYPE_NAME: x for x in [Artifact, Model, Dataset, Metrics, ClassificationMetrics]\n}\n\n\ndef create_runtime_artifact(runtime_artifact: Dict) -> Artifact:\n  \"\"\"Creates an Artifact instance from the specified RuntimeArtifact.\n\n  Args:\n    runtime_artifact: Dictionary representing JSON-encoded RuntimeArtifact.\n  \"\"\"\n  schema_title = runtime_artifact.get('type', {}).get('schemaTitle', '')\n\n  artifact_type = _SCHEMA_TITLE_TO_TYPE.get(schema_title)\n  if not artifact_type:\n    artifact_type = Artifact\n  return artifact_type(\n      uri=runtime_artifact.get('uri', ''),\n      name=runtime_artifact.get('name', ''),\n      metadata=runtime_artifact.get('metadata', {}),\n  )\n\nclass InputPath:\n    '''When creating component from function, :class:`.InputPath` should be used as function parameter annotation to tell the system to pass the *data file path* to the function instead of passing the actual data.'''\n    def __init__(self, type=None):\n        self.type = type\n\nclass OutputPath:\n    '''When creating component from function, :class:`.OutputPath` should be used as function parameter annotation to tell the system that the function wants to output data by writing it into a file with the given path instead of returning the data from the function.'''\n    def __init__(self, type=None):\n        self.type = type\n\nclass Executor():\n  \"\"\"Executor executes v2-based Python function components.\"\"\"\n\n  def __init__(self, executor_input: Dict, function_to_execute: Callable):\n    self._func = function_to_execute\n    self._input = executor_input\n    self._input_artifacts: Dict[str, InputArtifact] = {}\n    self._output_artifacts: Dict[str, OutputArtifact] = {}\n\n    for name, artifacts in self._input.get('inputs', {}).get('artifacts',\n                                                             {}).items():\n      artifacts_list = artifacts.get('artifacts')\n      if artifacts_list:\n        self._input_artifacts[name] = self._make_input_artifact(\n            artifacts_list[0])\n\n    for name, artifacts in self._input.get('outputs', {}).get('artifacts',\n                                                              {}).items():\n      artifacts_list = artifacts.get('artifacts')\n      if artifacts_list:\n        self._output_artifacts[name] = self._make_output_artifact(\n            artifacts_list[0])\n\n    self._return_annotation = inspect.signature(self._func).return_annotation\n    self._executor_output = {}\n\n  @classmethod\n  def _make_input_artifact(cls, runtime_artifact: Dict):\n    artifact = create_runtime_artifact(runtime_artifact)\n    return InputArtifact(artifact_type=type(artifact), artifact=artifact)\n\n  @classmethod\n  def _make_output_artifact(cls, runtime_artifact: Dict):\n    artifact = create_runtime_artifact(runtime_artifact)\n    return OutputArtifact(artifact_type=type(artifact), artifact=artifact)\n\n  def _get_input_artifact(self, name: str):\n    return self._input_artifacts.get(name)\n\n  def _get_output_artifact(self, name: str):\n    return self._output_artifacts.get(name)\n\n  def _get_input_parameter_value(self, parameter_name: str):\n    parameter = self._input.get('inputs', {}).get('parameters',\n                                                  {}).get(parameter_name, None)\n    if parameter is None:\n      return None\n\n    if parameter.get('stringValue'):\n      return parameter['stringValue']\n    elif parameter.get('intValue'):\n      return int(parameter['intValue'])\n    elif parameter.get('doubleValue'):\n      return float(parameter['doubleValue'])\n\n  def _get_output_parameter_path(self, parameter_name: str):\n    parameter_name = self._maybe_strip_path_suffix(parameter_name)\n    parameter = self._input.get('outputs',\n                                {}).get('parameters',\n                                        {}).get(parameter_name, None)\n    if parameter is None:\n      return None\n\n    return parameter.get('outputFile', None)\n\n  def _get_output_artifact_path(self, artifact_name: str):\n    artifact_name = self._maybe_strip_path_suffix(artifact_name)\n    output_artifact = self._output_artifacts.get(artifact_name)\n    if not output_artifact:\n      raise ValueError(\n          'Failed to get OutputArtifact path for artifact name {}'.format(\n              artifact_name))\n    return output_artifact.path\n\n  def _get_input_artifact_path(self, artifact_name: str):\n    artifact_name = self._maybe_strip_path_suffix(artifact_name)\n    input_artifact = self._input_artifacts.get(artifact_name)\n    if not input_artifact:\n      raise ValueError(\n          'Failed to get InputArtifact path for artifact name {}'.format(\n              artifact_name))\n    return input_artifact.path\n\n  def _write_output_parameter_value(self, name: str, value: Union[str, int,\n                                                                  float]):\n    if type(value) == str:\n      output = {\"stringValue\": value}\n    elif type(value) == int:\n      output = {\"intValue\": value}\n    elif type(value) == float:\n      output = {\"doubleValue\": value}\n    else:\n      raise RuntimeError(\n          'Expected value of type str, int or float, got {} instead for value {}'\n          .format(type(value), value))\n\n    if not self._executor_output.get('parameters'):\n      self._executor_output['parameters'] = {}\n\n    self._executor_output['parameters'][name] = output\n\n  def _write_output_artifact_payload(self, name: str, value: Any):\n    path = self._get_output_artifact_path(name)\n    with open(path, 'w') as f:\n      f.write(str(value))\n\n  @classmethod\n  def _is_parameter(cls, annotation: Any) -> bool:\n    return annotation in [str, int, float]\n\n  @classmethod\n  def _is_artifact(cls, annotation: Any) -> bool:\n    if type(annotation) == type:\n      return issubclass(annotation, Artifact)\n    return False\n\n  @classmethod\n  def _is_named_tuple(cls, annotation: Any) -> bool:\n    if type(annotation) == type:\n      return issubclass(annotation, tuple) and hasattr(\n          annotation, '_fields') and hasattr(annotation, '__annotations__')\n    return False\n\n  def _handle_single_return_value(self, output_name: str, annotation_type: Any,\n                                  return_value: Any):\n    if self._is_parameter(annotation_type):\n      if type(return_value) != annotation_type:\n        raise ValueError(\n            'Function `{}` returned value of type {}; want type {}'.format(\n                self._func.__name__, type(return_value), annotation_type))\n      self._write_output_parameter_value(output_name, return_value)\n    elif self._is_artifact(annotation_type):\n      self._write_output_artifact_payload(output_name, return_value)\n    else:\n      raise RuntimeError(\n          'Unknown return type: {}. Must be one of `str`, `int`, `float`, or a'\n          ' subclass of `Artifact`'.format(annotation_type))\n\n  def _write_executor_output(self, func_output: Optional[Any] = None):\n    if self._output_artifacts:\n      self._executor_output['artifacts'] = {}\n\n    for name, artifact in self._output_artifacts.items():\n      runtime_artifact = {\n          \"name\": artifact.get().name,\n          \"uri\": artifact.get().uri,\n          \"metadata\": artifact.get().metadata,\n      }\n      artifacts_list = {'artifacts': [runtime_artifact]}\n\n      self._executor_output['artifacts'][name] = artifacts_list\n\n    if func_output is not None:\n      if self._is_parameter(self._return_annotation) or self._is_artifact(\n          self._return_annotation):\n        # Note: single output is named `Output` in component.yaml.\n        self._handle_single_return_value('Output', self._return_annotation,\n                                         func_output)\n      elif self._is_named_tuple(self._return_annotation):\n        if len(self._return_annotation._fields) != len(func_output):\n          raise RuntimeError(\n              'Expected {} return values from function `{}`, got {}'.format(\n                  len(self._return_annotation._fields), self._func.__name__,\n                  len(func_output)))\n        for field in self._return_annotation._fields:\n          field_type = self._return_annotation.__annotations__[field]\n          field_value = getattr(func_output, field)\n          self._handle_single_return_value(field, field_type, field_value)\n      else:\n        raise RuntimeError(\n            'Unknown return type: {}. Must be one of `str`, `int`, `float`, a'\n            ' subclass of `Artifact`, or a NamedTuple collection of these types.'\n            .format(self._return_annotation))\n\n    import os\n    os.makedirs(os.path.dirname(self._input['outputs']['outputFile']),\n                exist_ok=True)\n    with open(self._input['outputs']['outputFile'], 'w') as f:\n      f.write(json.dumps(self._executor_output))\n\n  def _maybe_strip_path_suffix(self, name) -> str:\n    if name.endswith('_path'):\n      name = name[0:-len('_path')]\n    if name.endswith('_file'):\n      name = name[0:-len('_file')]\n    return name\n\n  def execute(self):\n    annotations = inspect.getfullargspec(self._func).annotations\n\n    # Function arguments.\n    func_kwargs = {}\n\n    for k, v in annotations.items():\n      if k == 'return':\n        continue\n\n      if v in [str, float, int]:\n        func_kwargs[k] = self._get_input_parameter_value(k)\n\n      if isinstance(v, OutputPath):\n        if v.type in [str, float, int]:\n          func_kwargs[k] = self._get_output_parameter_path(k)\n        else:\n          func_kwargs[k] = self._get_output_artifact_path(k)\n\n      if isinstance(v, InputPath):\n        func_kwargs[k] = self._get_input_artifact_path(k)\n\n      if isinstance(v, InputArtifact):\n        func_kwargs[k] = self._get_input_artifact(k)\n\n      if isinstance(v, OutputArtifact):\n        func_kwargs[k] = self._get_output_artifact(k)\n\n    result = self._func(**func_kwargs)\n    self._write_executor_output(result)\n\n\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use OutputArtifact to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: OutputArtifact(Dataset),\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str)):\n  '''Dummy preprocessing step'''\n\n  # Use OutputArtifact.path to access a local file path for writing.\n  # One can also use OutputArtifact.uri to access the actual URI file path.\n  with open(output_dataset_one.path, 'w') as f:\n    f.write(message)\n\n  # OutputPath is used to just pass the local file path of the output artifact\n  # to the function.\n  with open(output_dataset_two_path, 'w') as f:\n    f.write(message)\n\n  with open(output_parameter_path, 'w') as f:\n    f.write(message)\n\n\ndef executor_main():\n  import argparse\n  import json\n\n  parser = argparse.ArgumentParser(description='Process some integers.')\n  parser.add_argument('--executor_input', type=str)\n  parser.add_argument('--function_to_execute', type=str)\n\n  args, _ = parser.parse_known_args()\n  executor_input = json.loads(args.executor_input)\n  function_to_execute = globals()[args.function_to_execute]\n\n  executor = Executor(executor_input=executor_input,\n                      function_to_execute=function_to_execute)\n\n  executor.execute()\n\n\nif __name__ == '__main__':\n  executor_main()\n"
            ]
          }
        }
      }
    },
    "components": {
      "comp-preprocess": {
        "outputDefinitions": {
          "artifacts": {
            "output_dataset_two": {
              "artifactType": {
                "instanceSchema": "title: kfp.Dataset\ntype: object\nproperties:\n  payload_format:\n    type: string\n  container_format:\n    type: string\n"
              }
            },
            "output_dataset_one": {
              "artifactType": {
                "instanceSchema": "title: kfp.Dataset\ntype: object\nproperties:\n  payload_format:\n    type: string\n  container_format:\n    type: string\n"
              }
            }
          },
          "parameters": {
            "output_parameter": {
              "type": "STRING"
            }
          }
        },
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "message": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train": {
        "inputDefinitions": {
          "artifacts": {
            "dataset_two": {
              "artifactType": {
                "instanceSchema": "title: kfp.Dataset\ntype: object\nproperties:\n  payload_format:\n    type: string\n  container_format:\n    type: string\n"
              }
            },
            "dataset_one": {
              "artifactType": {
                "instanceSchema": "title: kfp.Dataset\ntype: object\nproperties:\n  payload_format:\n    type: string\n  container_format:\n    type: string\n"
              }
            }
          },
          "parameters": {
            "message": {
              "type": "STRING"
            },
            "num_steps": {
              "type": "INT"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "instanceSchema": "title: kfp.Model\ntype: object\nproperties:\n  framework:\n    type: string\n  framework_version:\n    type: string\n"
              }
            }
          }
        },
        "executorLabel": "exec-train"
      }
    },
    "sdkVersion": "kfp-1.5.0-rc.0",
    "root": {
      "dag": {
        "tasks": {
          "task-train": {
            "componentRef": {
              "name": "comp-train"
            },
            "dependentTasks": [
              "task-preprocess"
            ],
            "taskInfo": {
              "name": "task-train"
            },
            "inputs": {
              "artifacts": {
                "dataset_two": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_dataset_two",
                    "producerTask": "task-preprocess"
                  }
                },
                "dataset_one": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_dataset_one",
                    "producerTask": "task-preprocess"
                  }
                }
              },
              "parameters": {
                "num_steps": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "5"
                    }
                  }
                },
                "message": {
                  "taskOutputParameter": {
                    "producerTask": "task-preprocess",
                    "outputParameterKey": "output_parameter"
                  }
                }
              }
            }
          },
          "task-preprocess": {
            "taskInfo": {
              "name": "task-preprocess"
            },
            "inputs": {
              "parameters": {
                "message": {
                  "componentInputParameter": "message"
                }
              }
            },
            "componentRef": {
              "name": "comp-preprocess"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "message": {
            "type": "STRING"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "my-test-pipeline-beta"
    },
    "schemaVersion": "2.0.0"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "dummy_root"
  }
}