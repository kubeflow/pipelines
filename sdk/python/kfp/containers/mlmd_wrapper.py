# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import fire
from google.protobuf import json_format
import logging
from ml_metadata.proto import metadata_store_pb2
from ml_metadata.proto import metadata_store_service_pb2
import os
import sys
from typing import Dict, List, Union

from kfp.containers import entrypoint
from kfp.containers import mlmd_utils
from kfp.dsl import artifact
from kfp.pipeline_spec import pipeline_spec_pb2

MLMD_HOST_ENV = 'METADATA_GRPC_SERVICE_HOST'
MLMD_PORT_ENV = 'METADATA_GRPC_SERVICE_PORT'
PIPELINE_CONTEXT_ARG = 'pipeline_context'
PARAM_METADATA_SUFFIX = '_input_param_metadata_file'
ARTIFACT_METADATA_SUFFIX = '_input_artifact_metadata_file'
FIELD_NAME_SUFFIX = '_input_field_name'
ARGO_PARAM_SUFFIX = '_input_argo_param'
INPUT_URI_SUFFIX = '_input_uri'
PRODUCER_POD_ID_SUFFIX = '_pod_id'
OUTPUT_NAME_SUFFIX = '_input_output_name'
OUTPUT_ARTIFACT_PATH_SUFFIX = '_artifact_output_path'

METADATA_JSON_PATH = 'executor_output.json'


def _get_metadata_connection_config(
) -> metadata_store_pb2.MetadataStoreClientConfig:
  """Constructs the metadata grpc connection config.

  Returns:
    A metadata_store_pb2.MetadataStoreClientConfig object.
  """
  connection_config = metadata_store_pb2.MetadataStoreClientConfig()
  connection_config.host = os.getenv(MLMD_HOST_ENV)
  connection_config.port = int(os.getenv(MLMD_PORT_ENV))

  return connection_config


def _get_pipeline_value(
    value: Union[
      metadata_store_pb2.Value, int, str, float]) -> pipeline_spec_pb2.Value:
  result = pipeline_spec_pb2.Value()
  if isinstance(value, metadata_store_pb2.Value):
    if not value.HasField('value'):
      raise RuntimeError('value must specified for a MLMD value message.')
    if value.WhichOneof('value') == 'int_value':
      result.int_value = value.int_value
    elif value.WhichOneof('value') == 'double_value':
      result.double_value = value.double_value
    elif value.WhichOneof('value') == 'string_value':
      result.string_value = value.string_value
    else:
      raise TypeError('Get unknown type of value: {}'.format(value))
  elif isinstance(value, int):
    result.int_value = value
  elif isinstance(value, float):
    result.double_value = value
  elif isinstance(value, str):
    result.string_value = value
  else:
    raise TypeError('Get unknown type of value: {}'.format(value))
  return result


def _get_pipeline_value_mapping(
    mlmd_value_mapping) -> Dict[str, pipeline_spec_pb2.Value]:
  """Converts a mapping field with MLMD Value to Kubeflow Value."""
  return {k: _get_pipeline_value(v) for k, v in mlmd_value_mapping.items()}


def _get_runtime_artifact(
    mlmd_artifact_and_type: metadata_store_service_pb2.ArtifactAndType
) -> pipeline_spec_pb2.RuntimeArtifact:
  """Converts MLMD artifacts to pipeline RuntimeArtifact proto."""
  result = pipeline_spec_pb2.RuntimeArtifact(
      uri=mlmd_artifact_and_type.artifact.uri,
      type=mlmd_utils.get_artifact_type_schema(mlmd_artifact_and_type.type),
      properties=_get_pipeline_value_mapping(
          mlmd_artifact_and_type.artifact.properties),
      custom_properties=_get_pipeline_value_mapping(
          mlmd_artifact_and_type.artifact.custom_properties))
  return result


def _get_pipeline_artifact_list(
    mlmd_artifacts: List[metadata_store_service_pb2.ArtifactAndType]
) -> pipeline_spec_pb2.ArtifactList:
  """Converts MLMD artifacts to pipeline ArtifactList proto."""
  artifacts = [
      _get_runtime_artifact(a) for a in mlmd_artifacts]
  return pipeline_spec_pb2.ArtifactList(
      artifacts=artifacts)


def _get_artifacts_mapping(
    metadata_client: mlmd_utils.Metadata,
    producer_execution_id: Dict[str, str],
    artifacts_name: Dict[str, str],
    artifacts_uri: Dict[str, str],
    run_context: str
) -> Dict[str, pipeline_spec_pb2.ArtifactList]:
  """Gets the artifacts mapping by query metadata store."""
  # 1. Sanity check the input:
  #    - artifacts_name and producer_execution_id should have the same key set.
  #    - The key set of artifacts_uri should include that of artifacts_name and
  #      producer_execution_id, because artifact generated by v1 component does
  #      not produce metadata.
  if set(artifacts_name.keys()) != set(producer_execution_id.keys()):
    raise KeyError('Keys in artifact names and producer execution ids should be'
                   ' the same. Got names=%s, execution_ids=%s' % (
        artifacts_name, producer_execution_id))

  if not set(artifacts_name.keys()).issubset(set(artifacts_uri)):
    raise KeyError('Keys in artifacts_uri must include those in artifacts_name/'
                   'producer_execution_id. Got (%s) for artifacts_uri, and (%s)'
                   ' for artifacts_name/producer_execution_id.' % (
        artifacts_uri.keys(), artifacts_name.keys()))

  # 2. Construct mapping of artifact objects.
  result = {}
  for name, uri in artifacts_uri.items():
    if name in artifacts_name:
      # 2a. If artifact name and producer ID are provided, query metadata store
      # to get its metadata.
      qualified_artifacts = metadata_client.get_qualified_artifacts(
          run_name=run_context,
          producer_component_id=producer_execution_id[name],
          output_key=artifacts_name[name])
      assert len(
        qualified_artifacts) == 1, 'Got multiple artifacts for a single output'
      result[name] = _get_pipeline_artifact_list(qualified_artifacts)
    else:
      # 2b. If not, simply construct a raw Artifact object with its uri.
      result[name] = pipeline_spec_pb2.ArtifactList(
          artifacts=[
              _get_runtime_artifact(metadata_store_pb2.Artifact(uri=uri))])

  return result


def _prepare_output_artifacts_by_uris(
    output_uris: Dict[str, str]) -> Dict[str, pipeline_spec_pb2.ArtifactList]:
  """Gets output artifacts by specifying their URIs."""
  result = {}
  for name, uri in output_uris.items():
    artifacts = [pipeline_spec_pb2.RuntimeArtifact(uri=uri)]
    result[name] = pipeline_spec_pb2.ArtifactList(artifacts=artifacts)
  return result


def main(**kwargs):
  """The outer script for v2 KFP component.

  This function has a dynamic signature, which will be interpreted according to
  the I/O and data-passing contract of KFP Python function components. The
  parameter will be received from command line interface.

  For each declared parameter input of the user function, a command line
  arguments will be recognized:
  - {name of the parameter}_input_argo_param: The actual runtime value of the
     input parameter, which will be passed as an Argo parameter.

  For each declared artifact input of the user function, three command line args
  will be recognized:
  1. {name of the artifact}_input_uri: The actual uri of the input
     artifact.
  2. {name of the artifact}_pod_id: The pod id of the producer that generates
     this artifact.
  3. {name of the artifact}_input_output_name: The output name of the artifact,
     by which the artifact can be queried.
  If the producer is a new-styled KFP Python component, all 3 arguments will be
  used to query the metadata store to recover the artifact metadata to feed into
  UDF. Otherwise, a type-less Artifact will be recovered with the given URI.

  In addition, following two arguments are also required:
  1. pipeline_context: The context of the pipeline (usually the Argo run ID), in
     order to query the artifact being generated in the same pipeline run.
  2. function_name: The name of the user defined function.
  """
  logging.basicConfig(stream=sys.stdout, level=logging.INFO)
  logging.getLogger().setLevel(logging.INFO)

  if entrypoint.FN_NAME_ARG not in kwargs:
    raise RuntimeError('The function name must be provided.')
  if PIPELINE_CONTEXT_ARG not in kwargs:
    raise RuntimeError('The pipeline context must be provided.')

  # Group arguments according to suffixes.
  input_params_value = {}
  input_artifacts_pod_id = {}
  input_artifacts_name = {}
  input_artifacts_uri = {}
  output_artifacts_uri = {}

  for k, v in kwargs.items():
    if k.endswith(ARGO_PARAM_SUFFIX):
      param_name = k[:-len(PARAM_METADATA_SUFFIX)]
      input_params_value[param_name] = v
    elif k.endswith(PRODUCER_POD_ID_SUFFIX):
      param_name = k[:-len(PRODUCER_POD_ID_SUFFIX)]
      input_artifacts_pod_id[param_name] = v
    elif k.endswith(OUTPUT_NAME_SUFFIX):
      param_name = k[:-len(OUTPUT_NAME_SUFFIX)]
      input_artifacts_name[param_name] = v
    elif k.endswith(INPUT_URI_SUFFIX):
      artifact_name = k[:-len(INPUT_URI_SUFFIX)]
      input_artifacts_uri[artifact_name] = v
    elif k.endswith(OUTPUT_ARTIFACT_PATH_SUFFIX):
      artifact_name = k[:-len(OUTPUT_ARTIFACT_PATH_SUFFIX)]
      output_artifacts_uri[artifact_name] = v
    elif k not in (entrypoint.FN_NAME_ARG, PIPELINE_CONTEXT_ARG):
      logging.warning(
          'Got unexpected command line argument: %s=%s Ignoring', k, v)

  pipeline_context = kwargs[PIPELINE_CONTEXT_ARG]
  metadata_client = mlmd_utils.Metadata(
      connection_config=_get_metadata_connection_config())

  input_artifacts = _get_artifacts_mapping(
      metadata_client=metadata_client,
      producer_execution_id=input_artifacts_pod_id,
      artifacts_name=input_artifacts_name,
      artifacts_uri=input_artifacts_uri,
      run_context=pipeline_context)

  output_artifacts = _prepare_output_artifacts_by_uris(
      output_uris=output_artifacts_uri)

  executor_input = pipeline_spec_pb2.ExecutorInput()
  for k, v in input_params_value.items():
    executor_input.inputs.parameters[k].CopyFrom(_get_pipeline_value(v))
  for k, v in input_artifacts.items():
    executor_input.inputs.artifacts[k].CopyFrom(v)
  for k, v in output_artifacts.items():
    executor_input.outputs.artifacts[k].CopyFrom(v)

  function_name = kwargs[entrypoint.FN_NAME_ARG]
  entrypoint.main(
      executor_input_str=json_format.MessageToJson(executor_input),
      function_name=function_name,
      output_metadata_path=METADATA_JSON_PATH)


if __name__ == '__main__':
  fire.Fire(main)
