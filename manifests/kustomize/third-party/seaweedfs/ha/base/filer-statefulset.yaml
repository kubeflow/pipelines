apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: seaweedfs
    component: filer
  name: seaweedfs-filer
spec:
  serviceName: seaweedfs-filer
  replicas: 2
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
        app: seaweedfs
        component: filer
  template:
    metadata:
      labels:
        app: seaweedfs
        component: filer
        application-crd-id: kubeflow-pipelines
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: seaweedfs
                  component: filer
              topologyKey: kubernetes.io/hostname
      serviceAccountName: seaweedfs
      terminationGracePeriodSeconds: 60
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        fsGroup: 1000
      containers:
      - name: seaweedfs-filer
        image: 'chrislusf/seaweedfs'
        envFrom:
        - secretRef:
            name: mlpipeline-minio-artifact
        args:
        - 'filer'
        - '-port=8888'
        - '-iam'
        - '-master=seaweedfs-master-0.seaweedfs-master:9333,seaweedfs-master-1.seaweedfs-master:9333,seaweedfs-master-2.seaweedfs-master:9333'
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -ec
              - |
                # Only run this on one filer
                if [[ $(hostname) == seaweedfs-filer-0 ]]; then
                  exit 0
                fi
                # wait until seaweedfs filer is ready
                # Filer will be ready only after masters are ready.
                # We run this not directly on one of the masters, because that will not work for some reason.
                # This is closer to the single pod setup, where we wait for the s3 endpoint to get ready, which also 
                # directly depends on the filer.
                echo "Wait for cluster ready"
                for i in $(seq 1 120); do
                  if wget -q --spider http://127.0.0.1:8888/ ; then
                    break
                  fi
                  sleep 2
                done
                # create bucket if not exists (ignore error if exists)
                echo "s3.bucket.create --name mlpipeline" | /usr/bin/weed shell || true
                # configure admin user using keys from secret
                echo "s3.configure -user kubeflow-admin -access_key $accesskey -secret_key $secretkey -actions Admin -apply" | /usr/bin/weed shell
        volumeMounts:
          - name: data-filer
            mountPath: /data
        ports:
          - containerPort: 8888
            name: http-filer
          - containerPort: 18888
            name: grpc-filer
          - containerPort: 8333
            name: http-s3
          - containerPort: 8111
            name: http-iam
        env:
        # These two env vars are needed to enable weed shell to discover the master server
        - name: WEED_CLUSTER_SW_MASTER
          value: seaweedfs-master.swfs-test:9333
        - name: WEED_CLUSTER_DEFAULT
          value: sw
        readinessProbe:
          httpGet:
            path: /
            port: 8888
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 15
          successThreshold: 1
          failureThreshold: 100
          timeoutSeconds: 10
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 2Gi
        securityContext: # Using restricted profile
          allowPrivilegeEscalation: false
          privileged: false
          runAsNonRoot: true
          # image defaults to root user
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop:
            - ALL
  volumeClaimTemplates:
    - metadata:
        name: data-filer
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
