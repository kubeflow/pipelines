name: Serve a model with KServe 
description: Serve Models using KServe 
inputs:
  - {name: Action,                      type: String, default: 'create',                            description: 'Action to execute on KServe'}
  - {name: Model Name,                  type: String, default: '',                                  description: 'Name to give to the deployed InferenceService'}
  - {name: Namespace,                   type: String, default: '',                                  description: 'Kubernetes namespace where the InferenceService is deployed'}
  - {name: Autoscaling Target,          type: String, default: '0',                                 description: 'Autoscaling Target Number'}
  - {name: Enable Istio Sidecar,        type: Bool,   default: 'True',                              description: 'Whether to enable istio sidecar injection'}
  - {name: Enable ISVC Status,          type: Bool,   default: 'True',                              description: "Specifies whether to store the inference service status as the output parameter"}
  - {name: InferenceService YAML,       type: String, default: '{}',                                description: 'Raw InferenceService serialized YAML for deployment'}
  - {name: Watch Timeout,               type: String, default: '300',                               description: "Timeout seconds for watching until InferenceService becomes ready."}
  - {name: Service Account,             type: String, default: '',                                  description: 'K8s ServiceAccount to use to run the Predictor and Transformer pod'}
  - {name: Pred Min Replicas,           type: String, default: '-1',                                description: 'Minimum number of Predictor replicas'}
  - {name: Pred Max Replicas,           type: String, default: '-1',                                description: 'Maximum number of Predictor replicas'}
  - {name: Pred Model URI,              type: String, default: '',                                  description: 'Path of the S3, GCS or ABS compatible directory containing the Predictor model'}
  - {name: Pred Canary Traffic Percent, type: String, default: '100',                               description: 'The traffic split percentage between the candidate model and the last ready model'}
  - {name: Pred Framework,              type: String, default: '',                                  description: 'Machine Learning Framework for the Predictor'}
  - {name: Pred Runtime Version,        type: String, default: 'latest',                            description: 'Runtime Version of Machine Learning Framework'}
  - {name: Pred Resource Requests,      type: String, default: '{"cpu": "0.5", "memory": "512Mi"}', description: 'CPU and Memory requests for the Predictor'}
  - {name: Pred Resource Limits,        type: String, default: '{"cpu": "1", "memory": "1Gi"}',     description: 'CPU and Memory limits for the Predictor'}
  - {name: Pred Request Timeout,        type: String, default: '60',                                description: 'Specifies the number of seconds to wait before timing out a request to the Predictor.'}
  - {name: Pred Custom Model Spec,      type: String, default: '{}',                                description: 'Custom pod container spec in JSON to be used in the Predictor pod'}
  - {name: Transf Min Replicas,         type: String, default: '-1',                                description: 'Minimum number of Transformer replicas'}
  - {name: Transf Max Replicas,         type: String, default: '-1',                                description: 'Maximum number of Transformer replicas'}
  - {name: Transf Image,                type: String, default: '',                                  description: 'Docker image used for the Transformer pod container'}
  - {name: Transf Args,                 type: String, default: '[]',                                description: 'Arguments to the entrypoint of the Transformer pod container, overwrites CMD'}
  - {name: Transf URI,                  type: String, default: '',                                  description: 'Path of the S3, GCS or ABS compatible directory containing the Transformer. Not necessary if the whole pre-/postprocessing logic is in the docker image'}
  - {name: Transf Resource Requests,    type: String, default: '{"cpu": "0.5", "memory": "512Mi"}', description: 'CPU and Memory requests for the Transformer'}
  - {name: Transf Resource Limits,      type: String, default: '{"cpu": "1", "memory": "1Gi"}',     description: 'CPU and Memory limits for the Transformer'}
  - {name: Transf Request Timeout,      type: String, default: '60',                                description: 'Specifies the number of seconds to wait before timing out a request to the Transformer'}

outputs:
  - {name: InferenceService Status,   type: String,                                               description: 'Status JSON output of InferenceService'}
implementation:
  container:
    image: quay.io/aipipeline/kserve-component:v0.11.1
    command: ['python']
    args: [
      -u, kservedeployer.py,
      --action,                      {inputValue: Action},
      --model-name,                  {inputValue: Model Name},
      --namespace,                   {inputValue: Namespace},
      --autoscaling-target,          {inputValue: Autoscaling Target},
      --enable-istio-sidecar,        {inputValue: Enable Istio Sidecar},
      --enable-isvc-status,          {inputValue: Enable ISVC Status},
      --inferenceservice-yaml,       {inputValue: InferenceService YAML},
      --watch-timeout,               {inputValue: Watch Timeout},
      --service-account,             {inputValue: Service Account},
      --pred-min-replicas,           {inputValue: Pred Min Replicas},
      --pred-max-replicas,           {inputValue: Pred Max Replicas},
      --pred-model-uri,              {inputValue: Pred Model URI},
      --pred-canary-traffic-percent, {inputValue: Pred Canary Traffic Percent},
      --pred-framework,              {inputValue: Pred Framework},
      --pred-runtime-version,        {inputValue: Pred Runtime Version},
      --pred-resource-requests,      {inputValue: Pred Resource Requests},
      --pred-resource-limits,        {inputValue: Pred Resource Limits},
      --pred-request-timeout,        {inputValue: Pred Request Timeout},
      --pred-custom-model-spec,      {inputValue: Pred Custom Model Spec},
      --transf-min-replicas,         {inputValue: Transf Min Replicas},
      --transf-max-replicas,         {inputValue: Transf Max Replicas},
      --transf-uri,                  {inputValue: Transf URI},
      --transf-image,                {inputValue: Transf Image},
      --transf-args,                 {inputValue: Transf Args},
      --transf-resource-requests,    {inputValue: Transf Resource Requests},
      --transf-resource-limits,      {inputValue: Transf Resource Limits},
      --transf-request-timeout,      {inputValue: Transf Request Timeout},
      --output-path,                 {outputPath: InferenceService Status},
    ]
