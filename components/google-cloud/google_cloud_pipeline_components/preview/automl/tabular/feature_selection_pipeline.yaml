# PIPELINE DEFINITION
# Name: feature-selection
# Description: Defines pipeline for feature transform engine component.
# Inputs:
#    bigquery_staging_full_dataset_id: str [Default: '']
#    data_source_bigquery_table_path: str [Default: '']
#    data_source_csv_filenames: str [Default: '']
#    dataflow_disk_size_gb: int [Default: 40.0]
#    dataflow_machine_type: str [Default: 'n1-standard-16']
#    dataflow_max_num_workers: int [Default: 10.0]
#    dataflow_service_account: str [Default: '']
#    dataflow_subnetwork: str [Default: '']
#    dataflow_use_public_ips: bool [Default: True]
#    dataset_level_custom_transformation_definitions: list
#    dataset_level_transformations: list
#    encryption_spec_key_name: str [Default: '']
#    feature_selection_algorithm: str [Default: 'AMI']
#    feature_selection_execution_engine: str [Default: 'bigquery']
#    location: str
#    max_selected_features: int [Default: -1.0]
#    optimization_objective: str
#    predefined_split_key: str [Default: '']
#    prediction_type: str
#    project: str
#    root_dir: str
#    run_feature_selection: bool [Default: False]
#    stage_1_deadline_hours: float [Default: -1.0]
#    stage_2_deadline_hours: float [Default: -1.0]
#    stratified_split_key: str [Default: '']
#    target_column: str
#    test_fraction: float [Default: -1.0]
#    tf_auto_transform_features: dict
#    training_fraction: float [Default: -1.0]
#    validation_fraction: float [Default: -1.0]
#    weight_column: str [Default: '']
components:
  comp-feature-transform-engine:
    executorLabel: exec-feature-transform-engine
    inputDefinitions:
      parameters:
        autodetect_csv_schema:
          defaultValue: false
          description: 'If True, infers the column types

            when importing CSVs into BigQuery.'
          isOptional: true
          parameterType: BOOLEAN
        bigquery_staging_full_dataset_id:
          defaultValue: ''
          description: 'Dataset in

            "projectId.datasetId" format for storing intermediate-FTE BigQuery

            tables.  If the specified dataset does not exist in BigQuery, FTE will

            create the dataset. If no bigquery_staging_full_dataset_id is specified,

            all intermediate tables will be stored in a dataset created under the

            provided project in the input data source''s location during FTE

            execution called

            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".

            All tables generated by FTE will have a 30 day TTL.'
          isOptional: true
          parameterType: STRING
        data_source_bigquery_table_path:
          defaultValue: ''
          description: 'BigQuery input data

            source to run feature transform on.'
          isOptional: true
          parameterType: STRING
        data_source_csv_filenames:
          defaultValue: ''
          description: 'CSV input data source to run

            feature transform on.'
          isOptional: true
          parameterType: STRING
        dataflow_disk_size_gb:
          defaultValue: 40.0
          description: 'The disk size, in gigabytes, to use

            on each Dataflow worker instance. If not set, default to 40.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_machine_type:
          defaultValue: n1-standard-16
          description: 'The machine type used for dataflow

            jobs. If not set, default to n1-standard-16.'
          isOptional: true
          parameterType: STRING
        dataflow_max_num_workers:
          defaultValue: 25.0
          description: 'The number of workers to run the

            dataflow job. If not set, default to 25.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_service_account:
          defaultValue: ''
          description: 'Custom service account to run

            Dataflow jobs.'
          isOptional: true
          parameterType: STRING
        dataflow_subnetwork:
          defaultValue: ''
          description: 'Dataflow''s fully qualified subnetwork

            name, when empty the default subnetwork will be used. More details:

            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
          isOptional: true
          parameterType: STRING
        dataflow_use_public_ips:
          defaultValue: true
          description: 'Specifies whether Dataflow

            workers use public IP addresses.'
          isOptional: true
          parameterType: BOOLEAN
        dataset_level_custom_transformation_definitions:
          defaultValue: []
          description: "List of dataset-level custom transformation definitions. \
            \ Custom,\nbring-your-own dataset-level transform functions, where users\
            \ can define\nand import their own transform function and use it with\
            \ FTE's built-in\ntransformations. Using custom transformations is an\
            \ experimental feature\nand it is currently not supported during batch\
            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
            \ } ]"
          isOptional: true
          parameterType: LIST
        dataset_level_transformations:
          defaultValue: []
          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
            \ supported built-in\n  transformations:\n    Join: Joins features from\
            \ right_table_uri. For each join key, the\n      left table keys will\
            \ be included and the right table keys will\n      be dropped.\n     \
            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
            \        Arguments:\n            right_table_uri: Right table BigQuery\
            \ uri to join\n              with input_full_table_id.\n            join_keys:\
            \ Features to join on. For each\n              nested list, the first\
            \ element is a left table column\n              and the second is its\
            \ corresponding right table column.\n    TimeAggregate: Creates a new\
            \ feature composed of values of an\n      existing feature from a fixed\
            \ time period ago or in the future.\n      Ex: A feature for sales by\
            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
            \     Arguments:\n            time_difference: Number of time_difference_units\
            \ to\n              look back or into the future on our\n            \
            \  time_difference_target_column.\n            time_difference_units:\
            \ Units of time_difference to\n              look back or into the future\
            \ on our\n              time_difference_target_column. Must be one of\
            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
            \ *\n              'YEAR'\n            time_series_identifier_columns:\
            \ Names of the\n              time series identifier columns.\n      \
            \      time_column: Name of the time column.\n            time_difference_target_column:\
            \ Column we wish to get\n              the value of time_difference time_difference_units\
            \ in\n              the past or future.\n            output_column: Name\
            \ of our new time aggregate\n              feature.\n            is_future:\
            \ Whether we wish to look\n              forward in time. Defaults to\
            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
            \              Performs a partition by reduce operation (one of max,\n\
            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
            \              Getting avg sales (the reduce column) for each store\n\
            \              (partition_by_column) over the previous 5 days\n      \
            \        (time_column, time_ago_units, and time_ago).\n        Example:\
            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
            \   reduce_column: Column to apply the reduce operation\n            \
            \  on. Reduce operations include the\n                following: Max,\
            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
            \              partition by.\n            time_column: Time column for\
            \ the partition by\n              operation's window function.\n     \
            \       time_ago: Number of time_ago_units to look back on\n         \
            \     our target_column, starting from time_column\n              (inclusive).\n\
            \            time_ago_units: Units of time_ago to look back on\n     \
            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
            \     output_column: Name of our output feature."
          isOptional: true
          parameterType: LIST
        embedding_batch_prediction_accelerator_count:
          defaultValue: -1.0
          description: 'The number of accelerators to

            use to generate the embeddings. Default is 0.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        embedding_batch_prediction_accelerator_type:
          defaultValue: accelerator_type_unspecified
          description: 'The accelerator type to use to

            generate embeddings. If not provided, no accelerator is used. More

            details: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#acceleratortype'
          isOptional: true
          parameterType: STRING
        embedding_batch_prediction_batch_size:
          defaultValue: -1.0
          description: 'The batch size for embedding batch

            prediction job. Default = 1024.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        embedding_batch_prediction_machine_type:
          defaultValue: ''
          description: 'The machine type to be

            used to run the embedding batch prediction job. If not provided,

            `n1-highmem-32` will be used. For more details, see:

            https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types'
          isOptional: true
          parameterType: STRING
        embedding_batch_prediction_max_replica_count:
          defaultValue: -1.0
          description: 'The max replica count for

            embedding batch prediction job. Default = 50.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        embedding_batch_prediction_starting_replica_count:
          defaultValue: -1.0
          description: 'The starting replica count

            for embedding batch prediction job. Default = 20.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        embedding_prediction_server_docker_uri:
          defaultValue: ''
          description: 'The docker image inside which to

            run the embedding models to generate embeddings.'
          isOptional: true
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          description: Customer-managed encryption key.
          isOptional: true
          parameterType: STRING
        feature_selection_algorithm:
          defaultValue: AMI
          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
            \       Mutual Information Maximization): Reference paper: Mohamed\n \
            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
            \         and Chris Ding. \"Feature selection based on mutual information\n\
            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
            \         27, no.\n       8: 1226-1238."
          isOptional: true
          parameterType: STRING
        feature_selection_execution_engine:
          defaultValue: dataflow
          description: Execution engine to run feature selection, value can be dataflow,
            bigquery.
          isOptional: true
          parameterType: STRING
        forecasting_apply_windowing:
          defaultValue: true
          description: Whether to apply window strategy.
          isOptional: true
          parameterType: BOOLEAN
        forecasting_available_at_forecast_columns:
          defaultValue: []
          description: 'Forecasting

            available at forecast columns.'
          isOptional: true
          parameterType: LIST
        forecasting_context_window:
          defaultValue: -1.0
          description: Forecasting context window.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_forecast_horizon:
          defaultValue: -1.0
          description: Forecasting horizon.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_holiday_regions:
          defaultValue: []
          description: 'The geographical region based on which the

            holiday effect is applied in modeling by adding holiday categorical

            array feature that include all holidays matching the date. This option

            only allowed when data granularity is day. By default, holiday effect

            modeling is disabled. To turn it on, specify the holiday region using

            this option.

            Top level: * ''GLOBAL''

            Second level: continental regions: * ''NA'': North America

            * ''JAPAC'': Japan and Asia Pacific

            * ''EMEA'': Europe, the Middle East and Africa

            * ''LAC'': Latin America and the Caribbean

            Third level: countries from ISO 3166-1 Country codes.

            Valid regions: * ''GLOBAL'' * ''NA'' * ''JAPAC'' * ''EMEA'' * ''LAC''
            * ''AE''

            * ''AR'' * ''AT'' * ''AU'' * ''BE'' * ''BR'' * ''CA'' * ''CH'' * ''CL''
            * ''CN'' * ''CO''

            * ''CZ'' * ''DE'' * ''DK'' * ''DZ'' * ''EC'' * ''EE'' * ''EG'' * ''ES''
            * ''FI'' * ''FR''

            * ''GB'' * ''GR'' * ''HK'' * ''HU'' * ''ID'' * ''IE'' * ''IL'' * ''IN''
            * ''IR'' * ''IT''

            * ''JP'' * ''KR'' * ''LV'' * ''MA'' * ''MX'' * ''MY'' * ''NG'' * ''NL''
            * ''NO'' * ''NZ''

            * ''PE'' * ''PH'' * ''PK'' * ''PL'' * ''PT'' * ''RO'' * ''RS'' * ''RU''
            * ''SA'' * ''SE''

            * ''SG'' * ''SI'' * ''SK'' * ''TH'' * ''TR'' * ''TW'' * ''UA'' * ''US''
            * ''VE'' * ''VN''

            * ''ZA'''
          isOptional: true
          parameterType: LIST
        forecasting_predefined_window_column:
          defaultValue: ''
          description: Forecasting predefined window column.
          isOptional: true
          parameterType: STRING
        forecasting_time_column:
          defaultValue: ''
          description: Forecasting time column.
          isOptional: true
          parameterType: STRING
        forecasting_time_series_attribute_columns:
          defaultValue: []
          description: 'Forecasting

            time series attribute columns.'
          isOptional: true
          parameterType: LIST
        forecasting_time_series_identifier_column:
          description: '[Deprecated] A forecasting time series identifier column.
            Raises an

            exception if used - use the "time_series_identifier_column" field

            instead.'
          isOptional: true
          parameterType: STRING
        forecasting_time_series_identifier_columns:
          defaultValue: []
          description: The list of forecasting time series identifier columns.
          isOptional: true
          parameterType: LIST
        forecasting_unavailable_at_forecast_columns:
          defaultValue: []
          description: 'Forecasting

            unavailable at forecast columns.'
          isOptional: true
          parameterType: LIST
        forecasting_window_max_count:
          defaultValue: -1.0
          description: Forecasting window max count.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_window_stride_length:
          defaultValue: -1.0
          description: Forecasting window stride length.
          isOptional: true
          parameterType: NUMBER_INTEGER
        group_columns:
          isOptional: true
          parameterType: LIST
        group_temporal_total_weight:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        group_total_weight:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        legacy_transformations_path:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        location:
          description: Location for the created GCP services.
          parameterType: STRING
        materialized_examples_format:
          defaultValue: tfrecords_gzip
          description: 'The format to use for the

            materialized examples. Should be either ''tfrecords_gzip'' (default) or

            ''parquet''.'
          isOptional: true
          parameterType: STRING
        max_selected_features:
          defaultValue: 1000.0
          description: 'Maximum number of features to

            select.  If specified, the transform config will be purged by only using

            the selected features that ranked top in the feature ranking, which has

            the ranking value for all supported features. If the number of input

            features is smaller than max_selected_features specified, we will still

            run the feature selection process and generate the feature ranking, no

            features will be excluded.  The value will be set to 1000 by default if

            run_feature_selection is enabled.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_type:
          description: 'Model type, which we wish to engineer features

            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
            or

            tide. Defaults to the empty value, `None`.'
          isOptional: true
          parameterType: STRING
        multimodal_image_columns:
          defaultValue: []
          description: 'List of multimodal image

            columns. Defaults to an empty list.'
          isOptional: true
          parameterType: LIST
        multimodal_tabular_columns:
          defaultValue: []
          description: 'List of multimodal tabular

            columns. Defaults to an empty list'
          isOptional: true
          parameterType: LIST
        multimodal_text_columns:
          defaultValue: []
          description: 'List of multimodal text

            columns. Defaults to an empty list'
          isOptional: true
          parameterType: LIST
        multimodal_timeseries_columns:
          defaultValue: []
          description: 'List of multimodal timeseries

            columns. Defaults to an empty list'
          isOptional: true
          parameterType: LIST
        predefined_split_key:
          defaultValue: ''
          description: Predefined split key.
          isOptional: true
          parameterType: STRING
        prediction_type:
          defaultValue: ''
          description: 'Model prediction type. One of

            "classification", "regression", "time_series".'
          isOptional: true
          parameterType: STRING
        project:
          description: Project to run feature transform engine.
          parameterType: STRING
        root_dir:
          description: The Cloud Storage location to store the output.
          parameterType: STRING
        run_distill:
          defaultValue: false
          description: '(deprecated) Whether the distillation should be applied

            to the training.'
          isOptional: true
          parameterType: BOOLEAN
        run_feature_selection:
          defaultValue: false
          description: 'Whether the feature selection

            should be applied to the dataset.'
          isOptional: true
          parameterType: BOOLEAN
        stats_gen_execution_engine:
          defaultValue: dataflow
          description: 'Execution engine to perform

            statistics generation. Can be one of: "dataflow" (by default) or

            "bigquery". Using "bigquery" as the execution engine is experimental.'
          isOptional: true
          parameterType: STRING
        stratified_split_key:
          defaultValue: ''
          description: Stratified split key.
          isOptional: true
          parameterType: STRING
        target_column:
          defaultValue: ''
          description: Target column of input data.
          isOptional: true
          parameterType: STRING
        temporal_total_weight:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        test_fraction:
          defaultValue: -1.0
          description: Fraction of input data for testing.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        tf_auto_transform_features:
          defaultValue: {}
          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
            \ for each feature based on its data statistics.\nIf users do not want\
            \ auto type resolution, but want the set of\ntransformations for a given\
            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
            \ types. The following type hint\ndict keys are supported: * 'auto' *\
            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
            , \"feature3\"], }  Note that the target and\n    weight column may not\
            \ be included as an auto transformation unless\n    users are running\
            \ forecasting."
          isOptional: true
          parameterType: STRUCT
        tf_custom_transformation_definitions:
          defaultValue: []
          description: "List of\nTensorFlow-based custom transformation definitions.\
            \  Custom,\nbring-your-own transform functions, where users can define\
            \ and import\ntheir own transform function and use it with FTE's built-in\n\
            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
            \    transform function together with FTE's built-in transformations:\
            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
            :\n    [\"feature_1_multiplied_two\"] } ]"
          isOptional: true
          parameterType: LIST
        tf_transform_execution_engine:
          defaultValue: dataflow
          description: 'Execution engine to perform

            row-level TF transformations. Can be one of: "dataflow" (by default) or

            "bigquery". Using "bigquery" as the execution engine is experimental and

            is for allowlisted customers only. In addition, executing on "bigquery"

            only supports auto transformations (i.e., specified by

            tf_auto_transform_features) and will raise an error when

            tf_custom_transformation_definitions or tf_transformations_path is set.'
          isOptional: true
          parameterType: STRING
        tf_transformations_path:
          defaultValue: ''
          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
            \  In the following, we provide\nsome sample transform configurations\
            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
            \ of multiple transformations on a\nsingle column is also supported. For\
            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
            \ Extracts datetime featues from a column containing\n        timestamp\
            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
            \ A list with a single column to\n                perform the datetime\
            \ transformation on.\n              output_columns: Names of output\n\
            \                columns, one for each datetime_features element.\n  \
            \            time_format: Datetime format string. Time format is\n   \
            \             a combination of Date + Time Delimiter (optional) + Time\n\
            \                (optional) directives. Valid date directives are as\n\
            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
            \                are as follows * 'T' * ' ' Valid time directives are\
            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
            \ List of datetime\n                features to be extract. Each entry\
            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
            \      Log: Performs the natural log on a numeric column.\n          Example:\
            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
            \        input_columns: A list with a single column to\n             \
            \   perform the log transformation on.\n              output_columns:\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n      ZScale:\
            \ Performs Z-scale normalization on a numeric column.\n          Example:\
            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
            \       input_columns: A list with a single column to\n              \
            \  perform the z-scale transformation on.\n              output_columns:\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n      Vocabulary:\
            \ Converts strings to integers, where each unique string\n        gets\
            \ a unique integer representation.\n          Example:  .. code-block::\
            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
            \ A list with a single column to\n                perform the vocabulary\
            \ transformation on.\n              output_columns: A list with a single\n\
            \                output column name, corresponding to the output of our\n\
            \                transformation.\n              top_k: Number of the most\
            \ frequent words\n                in the vocabulary to use for generating\
            \ dictionary\n                lookup indices. If not specified, all words\
            \ in the\n                vocabulary will be used. Defaults to None.\n\
            \              frequency_threshold: Limit the vocabulary\n           \
            \     only to words whose number of occurrences in the input\n       \
            \         exceeds frequency_threshold. If not specified, all words\n \
            \               in the vocabulary will be included. If both top_k and\n\
            \                frequency_threshold are specified, a word must satisfy\n\
            \                both conditions to be included. Defaults to None.\n \
            \     Categorical: Transforms categorical columns to integer columns.\n\
            \          Example:  .. code-block:: python  { \"transformation\":\n \
            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
            : 10 }\n          Arguments:\n              input_columns: A list with\
            \ a single column to\n                perform the categorical transformation\
            \ on.\n              output_columns: A list with a single\n          \
            \      output column name, corresponding to the output of our\n      \
            \          transformation.\n              top_k: Number of the most frequent\
            \ words\n                in the vocabulary to use for generating dictionary\n\
            \                lookup indices. If not specified, all words in the\n\
            \                vocabulary will be used.\n              frequency_threshold:\
            \ Limit the vocabulary\n                only to words whose number of\
            \ occurrences in the input\n                exceeds frequency_threshold.\
            \ If not specified, all words\n                in the vocabulary will\
            \ be included. If both top_k and\n                frequency_threshold\
            \ are specified, a word must satisfy\n                both conditions\
            \ to be included.\n      Reduce: Given a column where each entry is a\
            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
            \          Example:  .. code-block:: python  { \"transformation\":\n \
            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
            \          Arguments:\n              input_columns: A list with a single\
            \ column to\n                perform the reduce transformation on.\n \
            \             output_columns: A list with a single\n                output\
            \ column name, corresponding to the output of our\n                transformation.\n\
            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
            \ of last k elements when\n                'LAST_K' reduce mode is used.\
            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
            \ strings into token\n        arrays.\n          Example:  .. code-block::\
            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
            \              input_columns: A list with a single column to\n       \
            \         perform the split string transformation on.\n              output_columns:\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n            \
            \  separator: Separator to split input string\n                into tokens.\
            \ Defaults to ' '.\n              missing_token: Missing token to use\
            \ when\n                no string is included. Defaults to ' _MISSING_\
            \ '.\n      NGram: Given a column of strings, splits strings into token\
            \ arrays\n        where each token is an integer.\n          Example:\
            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
            \              input_columns: A list with a single column to\n       \
            \         perform the n-gram transformation on.\n              output_columns:\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n            \
            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
            \          max_ngram_size: Maximum n-gram size. Must\n               \
            \ be a positive number and >= min_ngram_size. Defaults to\n          \
            \      2.\n              top_k: Number of the most frequent words\n  \
            \              in the vocabulary to use for generating dictionary\n  \
            \              lookup indices. If not specified, all words in the\n  \
            \              vocabulary will be used. Defaults to None.\n          \
            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
            \ only to words whose number of\n                occurrences in the input\
            \ exceeds frequency_threshold. If\n                not specified, all\
            \ words in the vocabulary will be\n                included. If both top_k\
            \ and frequency_threshold are\n                specified, a word must\
            \ satisfy both conditions to be\n                included. Defaults to\
            \ None.\n              separator: Separator to split input string\n  \
            \              into tokens. Defaults to ' '.\n              missing_token:\
            \ Missing token to use when\n                no string is included. Defaults\
            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
            \ such that elements <\n        min_value are assigned min_value, and\
            \ elements > max_value are\n        assigned max_value.\n          Example:\
            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
            \              input_columns: A list with a single column to\n       \
            \         perform the n-gram transformation on.\n              output_columns:\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n            \
            \  min_value: Number where all values below\n                min_value\
            \ are set to min_value. If no min_value is\n                provided,\
            \ min clipping will not occur. Defaults to None.\n              max_value:\
            \ Number where all values above\n                max_value are set to\
            \ max_value If no max_value is\n                provided, max clipping\
            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
            \ multi-hot encoding on a categorical\n        array column.\n       \
            \   Example:  .. code-block:: python  { \"transformation\":\n        \
            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
            \            of classes is determened by the largest number included in\n\
            \            the input if it is numeric or the total number of unique\n\
            \            values of the input if it is type str.  If the input is has\n\
            \            type str and an element contians separator tokens, the input\n\
            \            will be split at separator indices, and the each element\
            \ of\n            the split list will be considered a seperate class.\
            \ For\n            example,\n          Input:  .. code-block:: python\
            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
            ],          # Example 3 ]\n          Output (with default separator=\"\
            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
            \   input_columns: A list with a single column to\n                perform\
            \ the multi-hot-encoding on.\n              output_columns: A list with\
            \ a single\n                output column name, corresponding to the output\
            \ of our\n                transformation.\n              top_k: Number\
            \ of the most frequent words\n                in the vocabulary to use\
            \ for generating dictionary\n                lookup indices. If not specified,\
            \ all words in the\n                vocabulary will be used. Defaults\
            \ to None.\n              frequency_threshold: Limit the\n           \
            \     dictionary's vocabulary only to words whose number of\n        \
            \        occurrences in the input exceeds frequency_threshold. If\n  \
            \              not specified, all words in the vocabulary will be\n  \
            \              included. If both top_k and frequency_threshold are\n \
            \               specified, a word must satisfy both conditions to be\n\
            \                included. Defaults to None.\n              separator:\
            \ Separator to split input string\n                into tokens. Defaults\
            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
            \           input_columns: A list with a single column to\n          \
            \      perform max-abs-scale on.\n              output_columns: A list\
            \ with a single\n                output column name, corresponding to\
            \ the output of our\n                transformation.\n      Custom: Transformations\
            \ defined in\n        tf_custom_transformation_definitions are included\
            \ here in the\n        TensorFlow-based transformation configuration.\
            \  For example,\n        given the following tf_custom_transformation_definitions:\
            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
            \ the\n        following transformation:  .. code-block:: python  {\n\
            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
            \ that\n        input_columns must still be included in our arguments\
            \ and\n        output_columns is optional. All other arguments are those\n\
            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
            \ this\n        case. See tf_custom_transformation_definitions above.\n\
            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
            \ and tf_auto_transform_features\n        cannot both be specified."
          isOptional: true
          parameterType: STRING
        timestamp_split_key:
          defaultValue: ''
          description: Timestamp split key.
          isOptional: true
          parameterType: STRING
        training_fraction:
          defaultValue: -1.0
          description: Fraction of input data for training.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        validation_fraction:
          defaultValue: -1.0
          description: Fraction of input data for validation.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        weight_column:
          defaultValue: ''
          description: Weight column of input data.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_stats:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The stats of the dataset.
        feature_ranking:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'The ranking of features, all features supported in the

            dataset will be included. For "AMI" algorithm, array features won''t be

            available in the ranking as arrays are not supported yet.'
        instance_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        materialized_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: The materialized dataset.
        training_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        transform_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The transform output artifact.
      parameters:
        bigquery_downsampled_test_split_uri:
          description: 'BigQuery URI for the downsampled test

            split to pass to the batch prediction component during batch explain.'
          parameterType: STRING
        bigquery_test_split_uri:
          description: 'BigQuery URI for the test split to pass to the

            batch prediction component during evaluation.'
          parameterType: STRING
        bigquery_train_split_uri:
          description: 'BigQuery URI for the train split to pass to the

            batch prediction component during distillation.'
          parameterType: STRING
        bigquery_validation_split_uri:
          description: 'BigQuery URI for the validation split to

            pass to the batch prediction component during distillation.'
          parameterType: STRING
        gcp_resources:
          description: 'GCP resources created by this component. For more details,

            see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
        split_example_counts:
          description: 'JSON string of data split example counts for train,

            validate, and test splits.'
          parameterType: STRING
  comp-training-configurator-and-validator:
    executorLabel: exec-training-configurator-and-validator
    inputDefinitions:
      artifacts:
        dataset_stats:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Dataset stats generated by

            feature transform engine.'
        instance_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Schema of input data to the tf_model at

            serving time.'
        training_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        available_at_forecast_columns:
          defaultValue: []
          description: 'The names of the columns that are

            available at forecast time.'
          isOptional: true
          parameterType: LIST
        context_window:
          defaultValue: -1.0
          description: The length of the context window.
          isOptional: true
          parameterType: NUMBER_INTEGER
        enable_probabilistic_inference:
          defaultValue: false
          description: 'If probabilistic inference is

            enabled, the model will fit a distribution that captures the uncertainty

            of a prediction. At inference time, the predictive distribution is used

            to make a point prediction that minimizes the optimization objective.

            For example, the mean of a predictive distribution is the point

            prediction that minimizes RMSE loss. If quantiles are specified, then

            the quantiles of the distribution are also returned.'
          isOptional: true
          parameterType: BOOLEAN
        forecast_horizon:
          defaultValue: -1.0
          description: The length of the forecast horizon.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_model_type:
          defaultValue: ''
          description: The model types, e.g. l2l, seq2seq, tft.
          isOptional: true
          parameterType: STRING
        forecasting_transformations:
          defaultValue: {}
          description: 'Dict mapping auto and/or type-resolutions to

            feature columns. The supported types are auto, categorical, numeric,

            text, and timestamp.'
          isOptional: true
          parameterType: STRUCT
        group_columns:
          description: 'A list of time series attribute column

            names that define the time series hierarchy.'
          isOptional: true
          parameterType: LIST
        group_temporal_total_weight:
          defaultValue: 0.0
          description: 'The weight of the loss for

            predictions aggregated over both the horizon and time series in the same

            hierarchy group.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        group_total_weight:
          defaultValue: 0.0
          description: 'The weight of the loss for

            predictions aggregated over time series in the same group.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        optimization_objective:
          defaultValue: ''
          description: "Objective function the model is optimizing\ntowards. The training\
            \ process creates a model that maximizes/minimizes\nthe value of the objective\
            \ function over the validation set. The\nsupported optimization objectives\
            \ depend on the prediction type. If the\nfield is not set, a default objective\
            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
            \ - Maximize recall for a\n    specified precision value.\n  classification\
            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
          isOptional: true
          parameterType: STRING
        optimization_objective_precision_value:
          defaultValue: -1.0
          description: 'Required when

            optimization_objective is "maximize-recall-at-precision". Must be

            between 0 and 1, inclusive.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        optimization_objective_recall_value:
          defaultValue: -1.0
          description: 'Required when

            optimization_objective is "maximize-precision-at-recall". Must be

            between 0 and 1, inclusive.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        prediction_type:
          defaultValue: ''
          description: 'Model prediction type. One of "classification",

            "regression", "time_series".'
          isOptional: true
          parameterType: STRING
        quantiles:
          defaultValue: []
          description: All quantiles that the model need to predict.
          isOptional: true
          parameterType: LIST
        run_distill:
          defaultValue: false
          description: 'Whether the distillation should be applied to the

            training.'
          isOptional: true
          parameterType: BOOLEAN
        run_evaluation:
          defaultValue: false
          description: 'Whether we are running evaluation in the training

            pipeline.'
          isOptional: true
          parameterType: BOOLEAN
        split_example_counts:
          description: 'JSON string of data split example counts for

            train, validate, and test splits.'
          parameterType: STRING
        stage_1_deadline_hours:
          description: 'Stage 1 training budget in

            hours.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        stage_2_deadline_hours:
          description: 'Stage 2 training budget in

            hours.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        target_column:
          defaultValue: ''
          description: Target column of input data.
          isOptional: true
          parameterType: STRING
        temporal_total_weight:
          defaultValue: 0.0
          description: 'The weight of the loss for

            predictions aggregated over the horizon for a single time series.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        time_column:
          defaultValue: ''
          description: 'The column that indicates the time. Used by forecasting

            only.'
          isOptional: true
          parameterType: STRING
        time_series_attribute_columns:
          defaultValue: []
          description: 'The column names of the time series

            attributes.'
          isOptional: true
          parameterType: LIST
        time_series_identifier_column:
          description: '[Deprecated] The time series identifier

            column. Used by forecasting only. Raises exception if used -

            use the "time_series_identifier_column" field instead.'
          isOptional: true
          parameterType: STRING
        time_series_identifier_columns:
          defaultValue: []
          description: 'The list of time series identifier columns.

            Used by forecasting only.'
          isOptional: true
          parameterType: LIST
        unavailable_at_forecast_columns:
          defaultValue: []
          description: 'The names of the columns that are

            not available at forecast time.'
          isOptional: true
          parameterType: LIST
        weight_column:
          defaultValue: ''
          description: Weight column of input data.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        instance_baseline:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        metadata:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The tabular example gen metadata.
deploymentSpec:
  executors:
    exec-feature-transform-engine:
      container:
        args:
        - feature_transform_engine
        - '{"Concat": ["--project=", "{{$.inputs.parameters[''project'']}}"]}'
        - '{"Concat": ["--location=", "{{$.inputs.parameters[''location'']}}"]}'
        - '{"Concat": ["--dataset_level_custom_transformation_definitions=", "{{$.inputs.parameters[''dataset_level_custom_transformation_definitions'']}}"]}'
        - '{"Concat": ["--dataset_level_transformations=", "{{$.inputs.parameters[''dataset_level_transformations'']}}"]}'
        - '{"Concat": ["--forecasting_time_column=", "{{$.inputs.parameters[''forecasting_time_column'']}}"]}'
        - '{"IfPresent": {"InputName": "forecasting_time_series_identifier_column",
          "Then": {"Concat": ["--forecasting_time_series_identifier_column=", "{{$.inputs.parameters[''forecasting_time_series_identifier_column'']}}"]}}}'
        - '{"Concat": ["--forecasting_time_series_identifier_columns=", "{{$.inputs.parameters[''forecasting_time_series_identifier_columns'']}}"]}'
        - '{"Concat": ["--forecasting_time_series_attribute_columns=", "{{$.inputs.parameters[''forecasting_time_series_attribute_columns'']}}"]}'
        - '{"Concat": ["--forecasting_unavailable_at_forecast_columns=", "{{$.inputs.parameters[''forecasting_unavailable_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--forecasting_available_at_forecast_columns=", "{{$.inputs.parameters[''forecasting_available_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--forecasting_forecast_horizon=", "{{$.inputs.parameters[''forecasting_forecast_horizon'']}}"]}'
        - '{"Concat": ["--forecasting_context_window=", "{{$.inputs.parameters[''forecasting_context_window'']}}"]}'
        - '{"Concat": ["--forecasting_predefined_window_column=", "{{$.inputs.parameters[''forecasting_predefined_window_column'']}}"]}'
        - '{"Concat": ["--forecasting_window_stride_length=", "{{$.inputs.parameters[''forecasting_window_stride_length'']}}"]}'
        - '{"Concat": ["--forecasting_window_max_count=", "{{$.inputs.parameters[''forecasting_window_max_count'']}}"]}'
        - '{"Concat": ["--forecasting_holiday_regions=", "{{$.inputs.parameters[''forecasting_holiday_regions'']}}"]}'
        - '{"Concat": ["--forecasting_apply_windowing=", "{{$.inputs.parameters[''forecasting_apply_windowing'']}}"]}'
        - '{"Concat": ["--predefined_split_key=", "{{$.inputs.parameters[''predefined_split_key'']}}"]}'
        - '{"Concat": ["--stratified_split_key=", "{{$.inputs.parameters[''stratified_split_key'']}}"]}'
        - '{"Concat": ["--timestamp_split_key=", "{{$.inputs.parameters[''timestamp_split_key'']}}"]}'
        - '{"Concat": ["--training_fraction=", "{{$.inputs.parameters[''training_fraction'']}}"]}'
        - '{"Concat": ["--validation_fraction=", "{{$.inputs.parameters[''validation_fraction'']}}"]}'
        - '{"Concat": ["--test_fraction=", "{{$.inputs.parameters[''test_fraction'']}}"]}'
        - '{"Concat": ["--stats_gen_execution_engine=", "{{$.inputs.parameters[''stats_gen_execution_engine'']}}"]}'
        - '{"Concat": ["--tf_transform_execution_engine=", "{{$.inputs.parameters[''tf_transform_execution_engine'']}}"]}'
        - '{"IfPresent": {"InputName": "tf_auto_transform_features", "Then": {"Concat":
          ["--tf_auto_transform_features=", "{{$.inputs.parameters[''tf_auto_transform_features'']}}"]}}}'
        - '{"Concat": ["--tf_custom_transformation_definitions=", "{{$.inputs.parameters[''tf_custom_transformation_definitions'']}}"]}'
        - '{"Concat": ["--tf_transformations_path=", "{{$.inputs.parameters[''tf_transformations_path'']}}"]}'
        - '{"Concat": ["--legacy_transformations_path=", "{{$.inputs.parameters[''legacy_transformations_path'']}}"]}'
        - '{"Concat": ["--data_source_csv_filenames=", "{{$.inputs.parameters[''data_source_csv_filenames'']}}"]}'
        - '{"Concat": ["--data_source_bigquery_table_path=", "{{$.inputs.parameters[''data_source_bigquery_table_path'']}}"]}'
        - '{"Concat": ["--bigquery_staging_full_dataset_id=", "{{$.inputs.parameters[''bigquery_staging_full_dataset_id'']}}"]}'
        - '{"Concat": ["--target_column=", "{{$.inputs.parameters[''target_column'']}}"]}'
        - '{"Concat": ["--weight_column=", "{{$.inputs.parameters[''weight_column'']}}"]}'
        - '{"Concat": ["--prediction_type=", "{{$.inputs.parameters[''prediction_type'']}}"]}'
        - '{"IfPresent": {"InputName": "model_type", "Then": {"Concat": ["--model_type=",
          "{{$.inputs.parameters[''model_type'']}}"]}}}'
        - '{"Concat": ["--multimodal_tabular_columns=", "{{$.inputs.parameters[''multimodal_tabular_columns'']}}"]}'
        - '{"Concat": ["--multimodal_timeseries_columns=", "{{$.inputs.parameters[''multimodal_timeseries_columns'']}}"]}'
        - '{"Concat": ["--multimodal_text_columns=", "{{$.inputs.parameters[''multimodal_text_columns'']}}"]}'
        - '{"Concat": ["--multimodal_image_columns=", "{{$.inputs.parameters[''multimodal_image_columns'']}}"]}'
        - '{"Concat": ["--run_distill=", "{{$.inputs.parameters[''run_distill'']}}"]}'
        - '{"Concat": ["--run_feature_selection=", "{{$.inputs.parameters[''run_feature_selection'']}}"]}'
        - '{"Concat": ["--materialized_examples_format=", "{{$.inputs.parameters[''materialized_examples_format'']}}"]}'
        - '{"Concat": ["--max_selected_features=", "{{$.inputs.parameters[''max_selected_features'']}}"]}'
        - '{"Concat": ["--feature_selection_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/feature_selection_staging_dir"]}'
        - '{"Concat": ["--feature_selection_algorithm=", "{{$.inputs.parameters[''feature_selection_algorithm'']}}"]}'
        - '{"Concat": ["--feature_selection_execution_engine=", "{{$.inputs.parameters[''feature_selection_execution_engine'']}}"]}'
        - '{"Concat": ["--feature_ranking_path=", "{{$.outputs.artifacts[''feature_ranking''].uri}}"]}'
        - '{"Concat": ["--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.txt"]}'
        - '{"Concat": ["--stats_result_path=", "{{$.outputs.artifacts[''dataset_stats''].uri}}"]}'
        - '{"Concat": ["--transform_output_artifact_path=", "{{$.outputs.artifacts[''transform_output''].uri}}"]}'
        - '{"Concat": ["--transform_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform"]}'
        - '{"Concat": ["--materialized_examples_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/materialized"]}'
        - '{"Concat": ["--export_data_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/export"]}'
        - '{"Concat": ["--materialized_data_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/materialized_data"]}'
        - '{"Concat": ["--materialized_data_artifact_path=", "{{$.outputs.artifacts[''materialized_data''].uri}}"]}'
        - '{"Concat": ["--bigquery_train_split_uri_path=", "{{$.outputs.parameters[''bigquery_train_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_validation_split_uri_path=", "{{$.outputs.parameters[''bigquery_validation_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_test_split_uri_path=", "{{$.outputs.parameters[''bigquery_test_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_downsampled_test_split_uri_path=", "{{$.outputs.parameters[''bigquery_downsampled_test_split_uri''].output_file}}"]}'
        - '{"Concat": ["--split_example_counts_path=", "{{$.outputs.parameters[''split_example_counts''].output_file}}"]}'
        - '{"Concat": ["--instance_schema_path=", "{{$.outputs.artifacts[''instance_schema''].path}}"]}'
        - '{"Concat": ["--training_schema_path=", "{{$.outputs.artifacts[''training_schema''].path}}"]}'
        - --job_name=feature-transform-engine-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
        - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
        - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
        - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
        - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
        - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20230910_1325
        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230910_1325
        - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
        - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
        - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
        - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
        - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
        - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
        - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
        - '{"IfPresent": {"InputName": "group_columns", "Then": {"Concat": ["--group_columns=",
          "{{$.inputs.parameters[''group_columns'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
          "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
          ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
          ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
        - '{"Concat": ["--embedding_prediction_server_docker_uri=", "{{$.inputs.parameters[''embedding_prediction_server_docker_uri'']}}"]}'
        - '{"Concat": ["--embedding_batch_prediction_machine_type=", "{{$.inputs.parameters[''embedding_batch_prediction_machine_type'']}}"]}'
        - '{"Concat": ["--embedding_batch_prediction_accelerator_type=", "{{$.inputs.parameters[''embedding_batch_prediction_accelerator_type'']}}"]}'
        - '{"Concat": ["--embedding_batch_prediction_accelerator_count=", "{{$.inputs.parameters[''embedding_batch_prediction_accelerator_count'']}}"]}'
        - '{"Concat": ["--embedding_batch_prediction_starting_replica_count=", "{{$.inputs.parameters[''embedding_batch_prediction_starting_replica_count'']}}"]}'
        - '{"Concat": ["--embedding_batch_prediction_max_replica_count=", "{{$.inputs.parameters[''embedding_batch_prediction_max_replica_count'']}}"]}'
        - '{"Concat": ["--embedding_batch_prediction_batch_size=", "{{$.inputs.parameters[''embedding_batch_prediction_batch_size'']}}"]}'
        - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230910_1325
        resources:
          cpuLimit: 8.0
          memoryLimit: 30.0
    exec-training-configurator-and-validator:
      container:
        args:
        - training_configurator_and_validator
        - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
        - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
        - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
        - '{"Concat": ["--split_example_counts=", "{{$.inputs.parameters[''split_example_counts'']}}"]}'
        - '{"Concat": ["--target_column=", "{{$.inputs.parameters[''target_column'']}}"]}'
        - '{"Concat": ["--weight_column=", "{{$.inputs.parameters[''weight_column'']}}"]}'
        - '{"Concat": ["--prediction_type=", "{{$.inputs.parameters[''prediction_type'']}}"]}'
        - '{"Concat": ["--optimization_objective=", "{{$.inputs.parameters[''optimization_objective'']}}"]}'
        - '{"Concat": ["--optimization_objective_recall_value=", "{{$.inputs.parameters[''optimization_objective_recall_value'']}}"]}'
        - '{"Concat": ["--optimization_objective_precision_value=", "{{$.inputs.parameters[''optimization_objective_precision_value'']}}"]}'
        - '{"Concat": ["--metadata_path=", "{{$.outputs.artifacts[''metadata''].uri}}"]}'
        - '{"Concat": ["--instance_baseline_path=", "{{$.outputs.artifacts[''instance_baseline''].uri}}"]}'
        - '{"Concat": ["--run_evaluation=", "{{$.inputs.parameters[''run_evaluation'']}}"]}'
        - '{"Concat": ["--run_distill=", "{{$.inputs.parameters[''run_distill'']}}"]}'
        - '{"Concat": ["--enable_probabilistic_inference=", "{{$.inputs.parameters[''enable_probabilistic_inference'']}}"]}'
        - '{"IfPresent": {"InputName": "time_series_identifier_column", "Then": {"Concat":
          ["--time_series_identifier_column=", "{{$.inputs.parameters[''time_series_identifier_column'']}}"]}}}'
        - '{"Concat": ["--time_series_identifier_columns=", "{{$.inputs.parameters[''time_series_identifier_columns'']}}"]}'
        - '{"Concat": ["--time_column=", "{{$.inputs.parameters[''time_column'']}}"]}'
        - '{"Concat": ["--time_series_attribute_columns=", "{{$.inputs.parameters[''time_series_attribute_columns'']}}"]}'
        - '{"Concat": ["--available_at_forecast_columns=", "{{$.inputs.parameters[''available_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--unavailable_at_forecast_columns=", "{{$.inputs.parameters[''unavailable_at_forecast_columns'']}}"]}'
        - '{"IfPresent": {"InputName": "quantiles", "Then": {"Concat": ["--quantiles=",
          "{{$.inputs.parameters[''quantiles'']}}"]}}}'
        - '{"Concat": ["--context_window=", "{{$.inputs.parameters[''context_window'']}}"]}'
        - '{"Concat": ["--forecast_horizon=", "{{$.inputs.parameters[''forecast_horizon'']}}"]}'
        - '{"Concat": ["--forecasting_model_type=", "{{$.inputs.parameters[''forecasting_model_type'']}}"]}'
        - '{"Concat": ["--forecasting_transformations=", "{{$.inputs.parameters[''forecasting_transformations'']}}"]}'
        - '{"IfPresent": {"InputName": "stage_1_deadline_hours", "Then": {"Concat":
          ["--stage_1_deadline_hours=", "{{$.inputs.parameters[''stage_1_deadline_hours'']}}"]}}}'
        - '{"IfPresent": {"InputName": "stage_2_deadline_hours", "Then": {"Concat":
          ["--stage_2_deadline_hours=", "{{$.inputs.parameters[''stage_2_deadline_hours'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_columns", "Then": {"Concat": ["--group_columns=",
          "{{$.inputs.parameters[''group_columns'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
          "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
          ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
          ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230910_1325
pipelineInfo:
  description: Defines pipeline for feature transform engine component.
  name: feature-selection
root:
  dag:
    tasks:
      feature-transform-engine:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-feature-transform-engine
        inputs:
          parameters:
            bigquery_staging_full_dataset_id:
              componentInputParameter: bigquery_staging_full_dataset_id
            data_source_bigquery_table_path:
              componentInputParameter: data_source_bigquery_table_path
            data_source_csv_filenames:
              componentInputParameter: data_source_csv_filenames
            dataflow_disk_size_gb:
              componentInputParameter: dataflow_disk_size_gb
            dataflow_machine_type:
              componentInputParameter: dataflow_machine_type
            dataflow_max_num_workers:
              componentInputParameter: dataflow_max_num_workers
            dataflow_service_account:
              componentInputParameter: dataflow_service_account
            dataflow_subnetwork:
              componentInputParameter: dataflow_subnetwork
            dataflow_use_public_ips:
              componentInputParameter: dataflow_use_public_ips
            dataset_level_custom_transformation_definitions:
              componentInputParameter: dataset_level_custom_transformation_definitions
            dataset_level_transformations:
              componentInputParameter: dataset_level_transformations
            embedding_batch_prediction_accelerator_count:
              runtimeValue:
                constant: -1.0
            embedding_batch_prediction_accelerator_type:
              runtimeValue:
                constant: accelerator_type_unspecified
            embedding_batch_prediction_batch_size:
              runtimeValue:
                constant: -1.0
            embedding_batch_prediction_machine_type:
              runtimeValue:
                constant: ''
            embedding_batch_prediction_max_replica_count:
              runtimeValue:
                constant: -1.0
            embedding_batch_prediction_starting_replica_count:
              runtimeValue:
                constant: -1.0
            embedding_prediction_server_docker_uri:
              runtimeValue:
                constant: ''
            encryption_spec_key_name:
              componentInputParameter: encryption_spec_key_name
            feature_selection_algorithm:
              componentInputParameter: feature_selection_algorithm
            feature_selection_execution_engine:
              componentInputParameter: feature_selection_execution_engine
            forecasting_available_at_forecast_columns:
              runtimeValue:
                constant: []
            forecasting_context_window:
              runtimeValue:
                constant: -1.0
            forecasting_forecast_horizon:
              runtimeValue:
                constant: -1.0
            forecasting_holiday_regions:
              runtimeValue:
                constant: []
            forecasting_predefined_window_column:
              runtimeValue:
                constant: ''
            forecasting_time_column:
              runtimeValue:
                constant: ''
            forecasting_time_series_attribute_columns:
              runtimeValue:
                constant: []
            forecasting_time_series_identifier_columns:
              runtimeValue:
                constant: []
            forecasting_unavailable_at_forecast_columns:
              runtimeValue:
                constant: []
            forecasting_window_max_count:
              runtimeValue:
                constant: -1.0
            forecasting_window_stride_length:
              runtimeValue:
                constant: -1.0
            location:
              componentInputParameter: location
            materialized_examples_format:
              runtimeValue:
                constant: tfrecords_gzip
            max_selected_features:
              componentInputParameter: max_selected_features
            multimodal_image_columns:
              runtimeValue:
                constant: []
            multimodal_tabular_columns:
              runtimeValue:
                constant: []
            multimodal_text_columns:
              runtimeValue:
                constant: []
            multimodal_timeseries_columns:
              runtimeValue:
                constant: []
            predefined_split_key:
              componentInputParameter: predefined_split_key
            prediction_type:
              componentInputParameter: prediction_type
            project:
              componentInputParameter: project
            root_dir:
              componentInputParameter: root_dir
            run_feature_selection:
              componentInputParameter: run_feature_selection
            stratified_split_key:
              componentInputParameter: stratified_split_key
            target_column:
              componentInputParameter: target_column
            test_fraction:
              componentInputParameter: test_fraction
            tf_auto_transform_features:
              componentInputParameter: tf_auto_transform_features
            tf_custom_transformation_definitions:
              runtimeValue:
                constant: []
            tf_transform_execution_engine:
              runtimeValue:
                constant: dataflow
            tf_transformations_path:
              runtimeValue:
                constant: ''
            training_fraction:
              componentInputParameter: training_fraction
            validation_fraction:
              componentInputParameter: validation_fraction
            weight_column:
              componentInputParameter: weight_column
        taskInfo:
          name: feature-transform-engine
      training-configurator-and-validator:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-training-configurator-and-validator
        dependentTasks:
        - feature-transform-engine
        inputs:
          artifacts:
            dataset_stats:
              taskOutputArtifact:
                outputArtifactKey: dataset_stats
                producerTask: feature-transform-engine
            instance_schema:
              taskOutputArtifact:
                outputArtifactKey: instance_schema
                producerTask: feature-transform-engine
            training_schema:
              taskOutputArtifact:
                outputArtifactKey: training_schema
                producerTask: feature-transform-engine
          parameters:
            available_at_forecast_columns:
              runtimeValue:
                constant: []
            context_window:
              runtimeValue:
                constant: -1.0
            enable_probabilistic_inference:
              runtimeValue:
                constant: 0.0
            forecast_horizon:
              runtimeValue:
                constant: -1.0
            optimization_objective:
              componentInputParameter: optimization_objective
            optimization_objective_precision_value:
              runtimeValue:
                constant: -1.0
            optimization_objective_recall_value:
              runtimeValue:
                constant: -1.0
            prediction_type:
              componentInputParameter: prediction_type
            run_distill:
              runtimeValue:
                constant: 0.0
            run_evaluation:
              runtimeValue:
                constant: 0.0
            split_example_counts:
              taskOutputParameter:
                outputParameterKey: split_example_counts
                producerTask: feature-transform-engine
            stage_1_deadline_hours:
              componentInputParameter: stage_1_deadline_hours
            stage_2_deadline_hours:
              componentInputParameter: stage_2_deadline_hours
            target_column:
              componentInputParameter: target_column
            time_column:
              runtimeValue:
                constant: ''
            time_series_attribute_columns:
              runtimeValue:
                constant: []
            time_series_identifier_columns:
              runtimeValue:
                constant: []
            unavailable_at_forecast_columns:
              runtimeValue:
                constant: []
            weight_column:
              componentInputParameter: weight_column
        taskInfo:
          name: training-configurator-and-validator
  inputDefinitions:
    parameters:
      bigquery_staging_full_dataset_id:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      data_source_bigquery_table_path:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      data_source_csv_filenames:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      dataflow_disk_size_gb:
        defaultValue: 40.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      dataflow_machine_type:
        defaultValue: n1-standard-16
        isOptional: true
        parameterType: STRING
      dataflow_max_num_workers:
        defaultValue: 10.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      dataflow_service_account:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      dataflow_subnetwork:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      dataflow_use_public_ips:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      dataset_level_custom_transformation_definitions:
        isOptional: true
        parameterType: LIST
      dataset_level_transformations:
        isOptional: true
        parameterType: LIST
      encryption_spec_key_name:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      feature_selection_algorithm:
        defaultValue: AMI
        isOptional: true
        parameterType: STRING
      feature_selection_execution_engine:
        defaultValue: bigquery
        isOptional: true
        parameterType: STRING
      location:
        parameterType: STRING
      max_selected_features:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      optimization_objective:
        parameterType: STRING
      predefined_split_key:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      prediction_type:
        parameterType: STRING
      project:
        parameterType: STRING
      root_dir:
        parameterType: STRING
      run_feature_selection:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
      stage_1_deadline_hours:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      stage_2_deadline_hours:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      stratified_split_key:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      target_column:
        parameterType: STRING
      test_fraction:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      tf_auto_transform_features:
        isOptional: true
        parameterType: STRUCT
      training_fraction:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      validation_fraction:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      weight_column:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0-beta.17
