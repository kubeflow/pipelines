# PIPELINE DEFINITION
# Name: automl-tabular-bqml-arima-prediction
# Description: Forecasts using a BQML ARIMA_PLUS model.
# Inputs:
#    bigquery_destination_uri: str [Default: '']
#    data_source_bigquery_table_path: str [Default: '']
#    data_source_csv_filenames: str [Default: '']
#    encryption_spec_key_name: str [Default: '']
#    generate_explanation: bool [Default: False]
#    location: str
#    model_name: str
#    project: str
components:
  comp-bigquery-create-dataset:
    executorLabel: exec-bigquery-create-dataset
    inputDefinitions:
      parameters:
        dataset:
          parameterType: STRING
        exists_ok:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        location:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-bigquery-create-dataset-2:
    executorLabel: exec-bigquery-create-dataset-2
    inputDefinitions:
      parameters:
        dataset:
          parameterType: STRING
        exists_ok:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        location:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-bigquery-delete-dataset-with-prefix:
    executorLabel: exec-bigquery-delete-dataset-with-prefix
    inputDefinitions:
      parameters:
        dataset_prefix:
          parameterType: STRING
        delete_contents:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        project:
          parameterType: STRING
  comp-bigquery-query-job:
    executorLabel: exec-bigquery-query-job
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Describes the Cloud

            KMS encryption key that will be used to protect destination

            BigQuery table. The BigQuery Service Account associated with your

            project requires access to this encryption key. If

            encryption_spec_key_name are both specified in here and in

            job_configuration_query, the value in here will override the other

            one.'
          isOptional: true
          parameterType: STRING
        job_configuration_query:
          defaultValue: {}
          description: 'A json formatted string

            describing the rest of the job configuration.  For more details, see

            https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationQuery'
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: 'The labels associated with this job. You can

            use these to organize and group your jobs. Label keys and values can

            be no longer than 63 characters, can only containlowercase letters,

            numeric characters, underscores and dashes. International characters

            are allowed. Label values are optional. Label keys must start with a

            letter and each label in the list must have a different key.

            Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: 'Location for creating the BigQuery job. If not

            set, default to `US` multi-region.  For more details, see

            https://cloud.google.com/bigquery/docs/locations#specifying_your_location'
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to run the BigQuery query job. Defaults to the project
            in which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        query:
          defaultValue: ''
          description: 'SQL query text to execute. Only standard SQL is

            supported.  If query are both specified in here and in

            job_configuration_query, the value in here will override the other

            one.'
          isOptional: true
          parameterType: STRING
        query_parameters:
          defaultValue: []
          description: 'jobs.query parameters for

            standard SQL queries.  If query_parameters are both specified in here

            and in job_configuration_query, the value in here will override the

            other one.'
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        destination_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
          description: 'Describes the table where the query results should be stored.

            This property must be set for large results that exceed the maximum

            response size.

            For queries that produce anonymous (cached) results, this field will

            be populated by BigQuery.'
      parameters:
        gcp_resources:
          description: 'Serialized gcp_resources proto tracking the BigQuery job.

            For more details, see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
  comp-build-job-configuration-query:
    executorLabel: exec-build-job-configuration-query
    inputDefinitions:
      parameters:
        dataset_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        priority:
          defaultValue: INTERACTIVE
          isOptional: true
          parameterType: STRING
        project_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        table_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        write_disposition:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-exit-handler-1:
    dag:
      tasks:
        bigquery-create-dataset:
          cachingOptions: {}
          componentRef:
            name: comp-bigquery-create-dataset
          dependentTasks:
          - get-table-location
          - validate-inputs
          inputs:
            parameters:
              dataset:
                runtimeValue:
                  constant: tmp_{{$.pipeline_job_uuid}}
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: create-tmp-dataset
        bigquery-create-dataset-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-create-dataset-2
          dependentTasks:
          - get-table-location
          - maybe-replace-with-default
          - validate-inputs
          inputs:
            parameters:
              dataset:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: maybe-replace-with-default
              exists_ok:
                runtimeValue:
                  constant: true
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: create-prediction-dataset
        bigquery-query-job:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-query-job
          dependentTasks:
          - build-job-configuration-query
          - get-first-valid
          - get-model-metadata
          - get-table-location
          inputs:
            parameters:
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              job_configuration_query:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: build-job-configuration-query
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              pipelinechannel--get-first-valid-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-first-valid
              pipelinechannel--get-model-metadata-forecast_horizon:
                taskOutputParameter:
                  outputParameterKey: forecast_horizon
                  producerTask: get-model-metadata
              pipelinechannel--get-model-metadata-target_column:
                taskOutputParameter:
                  outputParameterKey: target_column
                  producerTask: get-model-metadata
              pipelinechannel--get-model-metadata-time_column:
                taskOutputParameter:
                  outputParameterKey: time_column
                  producerTask: get-model-metadata
              pipelinechannel--get-model-metadata-time_series_identifier_column:
                taskOutputParameter:
                  outputParameterKey: time_series_identifier_column
                  producerTask: get-model-metadata
              pipelinechannel--model_name:
                componentInputParameter: pipelinechannel--model_name
              project:
                componentInputParameter: pipelinechannel--project
              query:
                runtimeValue:
                  constant: "\n      SELECT\n        target.*,\n        STRUCT(prediction.time_series_adjusted_data\
                    \ AS value)\n          AS predicted_{{$.inputs.parameters['pipelinechannel--get-model-metadata-target_column']}},\n\
                    \        prediction.* EXCEPT (\n          {{$.inputs.parameters['pipelinechannel--get-model-metadata-time_series_identifier_column']}},\n\
                    \          time_series_timestamp,\n          time_series_adjusted_data\n\
                    \        ),\n      FROM\n        ML.EXPLAIN_FORECAST(\n      \
                    \    MODEL `{{$.inputs.parameters['pipelinechannel--model_name']}}`,\n\
                    \          STRUCT({{$.inputs.parameters['pipelinechannel--get-model-metadata-forecast_horizon']}}\
                    \ AS horizon)) AS prediction\n      RIGHT JOIN `{{$.inputs.parameters['pipelinechannel--get-first-valid-Output']}}`\
                    \ AS target\n        ON\n          CAST(target.{{$.inputs.parameters['pipelinechannel--get-model-metadata-time_series_identifier_column']}}\
                    \ AS STRING)\n            = CAST(prediction.{{$.inputs.parameters['pipelinechannel--get-model-metadata-time_series_identifier_column']}}\
                    \ AS STRING)\n          AND TIMESTAMP(target.{{$.inputs.parameters['pipelinechannel--get-model-metadata-time_column']}})\
                    \ = prediction.time_series_timestamp\n      WHERE target.{{$.inputs.parameters['pipelinechannel--get-model-metadata-target_column']}}\
                    \ IS NULL\n  "
          taskInfo:
            name: predictions-table
        build-job-configuration-query:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-build-job-configuration-query
          dependentTasks:
          - bigquery-create-dataset-2
          inputs:
            parameters:
              dataset_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-2-dataset_id'']}}'
              pipelinechannel--bigquery-create-dataset-2-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset-2
              pipelinechannel--bigquery-create-dataset-2-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset-2
              project_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-2-project_id'']}}'
              table_id:
                runtimeValue:
                  constant: predictions_{{$.pipeline_job_uuid}}
          taskInfo:
            name: build-job-configuration-query
        get-first-valid:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-first-valid
          dependentTasks:
          - load-table-from-uri
          inputs:
            parameters:
              pipelinechannel--data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
              pipelinechannel--load-table-from-uri-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: load-table-from-uri
              values:
                runtimeValue:
                  constant: '["{{$.inputs.parameters[''pipelinechannel--data_source_bigquery_table_path'']}}",
                    "{{$.inputs.parameters[''pipelinechannel--load-table-from-uri-Output'']}}"]'
          taskInfo:
            name: get-first-valid
        get-model-metadata:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-model-metadata
          dependentTasks:
          - get-table-location
          - validate-inputs
          inputs:
            parameters:
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              model:
                componentInputParameter: pipelinechannel--model_name
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: get-model-metadata
        get-table-location:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-table-location
          inputs:
            parameters:
              default_location:
                componentInputParameter: pipelinechannel--location
              project:
                componentInputParameter: pipelinechannel--project
              table:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
          taskInfo:
            name: get-table-location
        load-table-from-uri:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-load-table-from-uri
          dependentTasks:
          - bigquery-create-dataset
          - get-table-location
          inputs:
            parameters:
              destination:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-project_id'']}}.{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-dataset_id'']}}.csv_export'
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              project:
                componentInputParameter: pipelinechannel--project
              source_format:
                runtimeValue:
                  constant: CSV
              source_uris:
                componentInputParameter: pipelinechannel--data_source_csv_filenames
          taskInfo:
            name: load-table-from-uri
        maybe-replace-with-default:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-maybe-replace-with-default
          inputs:
            parameters:
              default:
                runtimeValue:
                  constant: prediction_{{$.pipeline_job_uuid}}
              value:
                componentInputParameter: pipelinechannel--bigquery_destination_uri
          taskInfo:
            name: maybe-replace-with-default
        validate-inputs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-validate-inputs
          inputs:
            parameters:
              bigquery_destination_uri:
                componentInputParameter: pipelinechannel--bigquery_destination_uri
              data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
              data_source_csv_filenames:
                componentInputParameter: pipelinechannel--data_source_csv_filenames
              source_model_uri:
                componentInputParameter: pipelinechannel--model_name
          taskInfo:
            name: validate-inputs
    inputDefinitions:
      parameters:
        pipelinechannel--bigquery_destination_uri:
          parameterType: STRING
        pipelinechannel--data_source_bigquery_table_path:
          parameterType: STRING
        pipelinechannel--data_source_csv_filenames:
          parameterType: STRING
        pipelinechannel--encryption_spec_key_name:
          parameterType: STRING
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
  comp-get-first-valid:
    executorLabel: exec-get-first-valid
    inputDefinitions:
      parameters:
        values:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-model-metadata:
    executorLabel: exec-get-model-metadata
    inputDefinitions:
      parameters:
        location:
          parameterType: STRING
        model:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        forecast_horizon:
          parameterType: NUMBER_INTEGER
        target_column:
          parameterType: STRING
        time_column:
          parameterType: STRING
        time_series_identifier_column:
          parameterType: STRING
  comp-get-table-location:
    executorLabel: exec-get-table-location
    inputDefinitions:
      parameters:
        default_location:
          defaultValue: ''
          description: Location to return if no table was given.
          isOptional: true
          parameterType: STRING
        project:
          description: The GCP project.
          parameterType: STRING
        table:
          description: The BigQuery table to get a location for.
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-load-table-from-uri:
    executorLabel: exec-load-table-from-uri
    inputDefinitions:
      parameters:
        destination:
          description: Table into which data is to be loaded.
          parameterType: STRING
        location:
          description: The GCP region.
          parameterType: STRING
        project:
          description: The GCP project.
          parameterType: STRING
        source_format:
          defaultValue: CSV
          description: 'The file format for the files being imported. Only CSV is

            supported.'
          isOptional: true
          parameterType: STRING
        source_uris:
          description: 'URIs of data files to be loaded; in format

            gs://<bucket_name>/<object_name_or_glob>.'
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-maybe-replace-with-default:
    executorLabel: exec-maybe-replace-with-default
    inputDefinitions:
      parameters:
        default:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        value:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-validate-inputs:
    executorLabel: exec-validate-inputs
    inputDefinitions:
      parameters:
        bigquery_destination_uri:
          isOptional: true
          parameterType: STRING
        data_granularity_unit:
          isOptional: true
          parameterType: STRING
        data_source_bigquery_table_path:
          isOptional: true
          parameterType: STRING
        data_source_csv_filenames:
          isOptional: true
          parameterType: STRING
        optimization_objective:
          isOptional: true
          parameterType: STRING
        predefined_split_key:
          isOptional: true
          parameterType: STRING
        source_model_uri:
          isOptional: true
          parameterType: STRING
        target_column:
          isOptional: true
          parameterType: STRING
        test_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        time_column:
          isOptional: true
          parameterType: STRING
        time_series_identifier_column:
          isOptional: true
          parameterType: STRING
        timestamp_split_key:
          isOptional: true
          parameterType: STRING
        training_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        validation_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        window_column:
          isOptional: true
          parameterType: STRING
        window_max_count:
          isOptional: true
          parameterType: NUMBER_INTEGER
        window_stride_length:
          isOptional: true
          parameterType: NUMBER_INTEGER
deploymentSpec:
  executors:
    exec-bigquery-create-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_create_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_create_dataset(\n    project: str,\n    location: str,\n\
          \    dataset: str,\n    exists_ok: bool = False,\n) -> NamedTuple('Outputs',\
          \ [('project_id', str), ('dataset_id', str)]):\n  \"\"\"Creates a BigQuery\
          \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
          \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project, location=location)\n  ref\
          \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
          \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
          \   ref.project, ref.dataset_id)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-bigquery-create-dataset-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_create_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_create_dataset(\n    project: str,\n    location: str,\n\
          \    dataset: str,\n    exists_ok: bool = False,\n) -> NamedTuple('Outputs',\
          \ [('project_id', str), ('dataset_id', str)]):\n  \"\"\"Creates a BigQuery\
          \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
          \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project, location=location)\n  ref\
          \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
          \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
          \   ref.project, ref.dataset_id)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-bigquery-delete-dataset-with-prefix:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_delete_dataset_with_prefix
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_delete_dataset_with_prefix(\n    project: str,\n   \
          \ dataset_prefix: str,\n    delete_contents: bool = False,\n) -> None:\n\
          \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
          \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
          \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
          \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-bigquery-query-job:
      container:
        args:
        - --type
        - BigqueryQueryJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --payload
        - '{"Concat": ["{", "\"configuration\": {", "\"query\": ", "{{$.inputs.parameters[''job_configuration_query'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}", "}"]}'
        - --job_configuration_query_override
        - '{"Concat": ["{", "\"query\": \"", "{{$.inputs.parameters[''query'']}}",
          "\"", ", \"query_parameters\": ", "{{$.inputs.parameters[''query_parameters'']}}",
          ", \"destination_encryption_configuration\": {", "\"kmsKeyName\": \"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.bigquery.query_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1
    exec-build-job-configuration-query:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - build_job_configuration_query
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef build_job_configuration_query(\n    project_id: str = '',\n \
          \   dataset_id: str = '',\n    table_id: str = '',\n    write_disposition:\
          \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
          \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
          \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
          \    config['destinationTable'] = {\n        'projectId': project_id,\n\
          \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
          \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
          \  return config\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-get-first-valid:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_first_valid
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_first_valid(values: str) -> str:\n  \"\"\"Returns the first\
          \ truthy value from the given serialized JSON list.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import json\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  for value in json.loads(values):\n    if value:\n      return value\n\
          \  raise ValueError('No valid values.')\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-get-model-metadata:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_model_metadata
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_model_metadata(\n    project: str,\n    location: str,\n\
          \    model: str,\n) -> NamedTuple(\n    'Outputs',\n    [\n        ('time_column',\
          \ str),\n        ('time_series_identifier_column', str),\n        ('target_column',\
          \ str),\n        ('forecast_horizon', int),\n    ],\n):\n  \"\"\"Retrieves\
          \ training options for a BQML model.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
          \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project, location=location)\n  options\
          \ = client.get_model(model).training_runs[0].training_options\n  return\
          \ collections.namedtuple(\n      'Outputs', [\n          'time_column',\n\
          \          'time_series_identifier_column',\n          'target_column',\n\
          \          'forecast_horizon',\n      ],\n  )(\n      options.time_series_timestamp_column,\n\
          \      options.time_series_id_column,\n      options.time_series_data_column,\n\
          \      options.horizon,\n  )\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-get-table-location:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_table_location
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_table_location(\n    project: str,\n    table: Optional[str],\n\
          \    default_location: str = '',\n) -> str:\n  \"\"\"Returns the region\
          \ the given table belongs to.\n\n  Args:\n    project: The GCP project.\n\
          \    table: The BigQuery table to get a location for.\n    default_location:\
          \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
          \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
          \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
          \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
          \  return client.get_table(table).location\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-load-table-from-uri:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_table_from_uri
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_table_from_uri(\n    project: str,\n    location: str,\n\
          \    source_uris: str,\n    destination: str,\n    source_format: str =\
          \ 'CSV',\n) -> str:\n  \"\"\"Creates a table from a list of URIs.\n\n  Args:\n\
          \    project: The GCP project.\n    location: The GCP region.\n    source_uris:\
          \ URIs of data files to be loaded; in format\n      gs://<bucket_name>/<object_name_or_glob>.\n\
          \    destination: Table into which data is to be loaded.\n    source_format:\
          \ The file format for the files being imported. Only CSV is\n      supported.\n\
          \n  Returns:\n    The destination table containing imported data.\n  \"\"\
          \"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  if not source_uris:\n    return ''\n\n  csv_list = [filename.strip()\
          \ for filename in source_uris.split(',')]\n  client = bigquery.Client(project=project,\
          \ location=location)\n  job_config = bigquery.LoadJobConfig(\n      autodetect=True,\
          \ source_format=source_format)\n  client.load_table_from_uri(\n      source_uris=csv_list,\n\
          \      destination=destination,\n      project=project,\n      location=location,\n\
          \      job_config=job_config).result()\n  return destination\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-maybe-replace-with-default:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - maybe_replace_with_default
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef maybe_replace_with_default(value: str, default: str = '') ->\
          \ str:\n  \"\"\"Replaces string with another value if it is a dash.\"\"\"\
          \n  return default if not value else value\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
    exec-validate-inputs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_inputs
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef validate_inputs(\n    time_column: Optional[str] = None,\n  \
          \  time_series_identifier_column: Optional[str] = None,\n    target_column:\
          \ Optional[str] = None,\n    data_source_bigquery_table_path: Optional[str]\
          \ = None,\n    training_fraction: Optional[float] = None,\n    validation_fraction:\
          \ Optional[float] = None,\n    test_fraction: Optional[float] = None,\n\
          \    predefined_split_key: Optional[str] = None,\n    timestamp_split_key:\
          \ Optional[str] = None,\n    data_source_csv_filenames: Optional[str] =\
          \ None,\n    source_model_uri: Optional[str] = None,\n    bigquery_destination_uri:\
          \ Optional[str] = None,\n    window_column: Optional[str] = None,\n    window_stride_length:\
          \ Optional[int] = None,\n    window_max_count: Optional[int] = None,\n \
          \   optimization_objective: Optional[str] = None,\n    data_granularity_unit:\
          \ Optional[str] = None,\n) -> None:\n  \"\"\"Checks training pipeline input\
          \ parameters are valid.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
          \  import re\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
          \n  project_pattern = r'([a-z0-9.-]+:)?[a-z][a-z0-9-_]{4,28}[a-z0-9]'\n\
          \  dataset_pattern = r'[a-zA-Z0-9_]+'\n  table_pattern = r'[^\\.\\:`]+'\n\
          \  dataset_uri_pattern = re.compile(\n      f'(bq://)?{project_pattern}[.:]{dataset_pattern}')\n\
          \  table_uri_pattern = re.compile(\n      f'(bq://)?{project_pattern}[.:]{dataset_pattern}[.:]{table_pattern}')\n\
          \n  # Validate BigQuery column and dataset names.\n  bigquery_column_parameters\
          \ = [\n      time_column,\n      time_series_identifier_column,\n      target_column,\n\
          \  ]\n  column_pattern = re.compile(r'[a-zA-Z_][a-zA-Z0-9_]{1,300}')\n \
          \ for column in bigquery_column_parameters:\n    if column and not column_pattern.fullmatch(column):\n\
          \      raise ValueError(f'Invalid column name: {column}.')\n  if (bigquery_destination_uri\
          \ and\n      not dataset_uri_pattern.fullmatch(bigquery_destination_uri)):\n\
          \    raise ValueError(\n        f'Invalid BigQuery dataset URI: {bigquery_destination_uri}.')\n\
          \  if (source_model_uri and not table_uri_pattern.fullmatch(source_model_uri)):\n\
          \    raise ValueError(f'Invalid BigQuery table URI: {source_model_uri}.')\n\
          \n  # Validate data source.\n  data_source_count = sum([bool(source) for\
          \ source in [\n      data_source_bigquery_table_path, data_source_csv_filenames]])\n\
          \  if data_source_count > 1:\n    raise ValueError(f'Expected 1 data source,\
          \ found {data_source_count}.')\n  if (data_source_bigquery_table_path\n\
          \      and not table_uri_pattern.fullmatch(data_source_bigquery_table_path)):\n\
          \    raise ValueError(\n        f'Invalid BigQuery table URI: {data_source_bigquery_table_path}.')\n\
          \  gcs_path_pattern = re.compile(r'gs:\\/\\/(.+)\\/([^\\/]+)')\n  if data_source_csv_filenames:\n\
          \    csv_list = [filename.strip()\n                for filename in data_source_csv_filenames.split(',')]\n\
          \    for gcs_path in csv_list:\n      if not gcs_path_pattern.fullmatch(gcs_path):\n\
          \        raise ValueError(f'Invalid path to CSV stored in GCS: {gcs_path}.')\n\
          \n  # Validate split spec.\n  fraction_splits = [\n      training_fraction,\n\
          \      validation_fraction,\n      test_fraction,\n  ]\n  fraction_splits\
          \ = [None if fraction == -1 else fraction\n                     for fraction\
          \ in fraction_splits]\n  split_count = sum([\n      bool(source)\n     \
          \ for source in [predefined_split_key,\n                     any(fraction_splits)]\n\
          \  ])\n  if split_count > 1:\n    raise ValueError(f'Expected 1 split type,\
          \ found {split_count}.')\n  if (predefined_split_key and\n      not column_pattern.fullmatch(predefined_split_key)):\n\
          \    raise ValueError(f'Invalid column name: {predefined_split_key}.')\n\
          \  if any(fraction_splits):\n    if not all(fraction_splits):\n      raise\
          \ ValueError(\n          f'All fractions must be non-zero. Got: {fraction_splits}.')\n\
          \    if sum(fraction_splits) != 1:\n      raise ValueError(\n          f'Fraction\
          \ splits must sum to 1. Got: {sum(fraction_splits)}.')\n  if (timestamp_split_key\
          \ and\n      not column_pattern.fullmatch(timestamp_split_key)):\n    raise\
          \ ValueError(f'Invalid column name: {timestamp_split_key}.')\n  if timestamp_split_key\
          \ and not all(fraction_splits):\n    raise ValueError('All fractions must\
          \ be non-zero for timestamp split.')\n\n  # Validate window config.\n  if\
          \ window_stride_length == -1:\n    window_stride_length = None\n  if window_max_count\
          \ == -1:\n    window_max_count = None\n  window_configs = [window_column,\
          \ window_stride_length, window_max_count]\n  window_config_count = sum([bool(config)\
          \ for config in window_configs])\n  if window_config_count > 1:\n    raise\
          \ ValueError(f'Expected 1 window config, found {window_config_count}.')\n\
          \  if window_column and not column_pattern.fullmatch(window_column):\n \
          \   raise ValueError(f'Invalid column name: {window_column}.')\n  if window_stride_length\
          \ and (window_stride_length < 1 or\n                               window_stride_length\
          \ > 1000):\n    raise ValueError('Stride must be between 1 and 1000. Got:\
          \ '\n                     f'{window_stride_length}.')\n  if window_max_count\
          \ and (window_max_count < 1000 or\n                           window_max_count\
          \ > int(1e8)):\n    raise ValueError('Max count must be between 1000 and\
          \ 100000000. Got: '\n                     f'{window_max_count}.')\n\n  #\
          \ Validate eval metric.\n  valid_optimization_objectives = ['rmse', 'mae',\
          \ 'rmsle']\n  if optimization_objective:\n    if optimization_objective\
          \ not in valid_optimization_objectives:\n      raise ValueError(\n     \
          \     'Optimization objective should be one of the following: '\n      \
          \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
          \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
          \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
          \    if data_granularity_unit not in valid_data_granularity_units:\n   \
          \   raise ValueError(\n          'Granularity unit should be one of the\
          \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240419_0625
pipelineInfo:
  description: Forecasts using a BQML ARIMA_PLUS model.
  name: automl-tabular-bqml-arima-prediction
root:
  dag:
    tasks:
      bigquery-delete-dataset-with-prefix:
        cachingOptions: {}
        componentRef:
          name: comp-bigquery-delete-dataset-with-prefix
        dependentTasks:
        - exit-handler-1
        inputs:
          parameters:
            dataset_prefix:
              runtimeValue:
                constant: tmp_{{$.pipeline_job_uuid}}
            delete_contents:
              runtimeValue:
                constant: true
            project:
              componentInputParameter: project
        taskInfo:
          name: delete-tmp-dataset
        triggerPolicy:
          strategy: ALL_UPSTREAM_TASKS_COMPLETED
      exit-handler-1:
        componentRef:
          name: comp-exit-handler-1
        inputs:
          parameters:
            pipelinechannel--bigquery_destination_uri:
              componentInputParameter: bigquery_destination_uri
            pipelinechannel--data_source_bigquery_table_path:
              componentInputParameter: data_source_bigquery_table_path
            pipelinechannel--data_source_csv_filenames:
              componentInputParameter: data_source_csv_filenames
            pipelinechannel--encryption_spec_key_name:
              componentInputParameter: encryption_spec_key_name
            pipelinechannel--location:
              componentInputParameter: location
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--project:
              componentInputParameter: project
        taskInfo:
          name: exit-handler-1
  inputDefinitions:
    parameters:
      bigquery_destination_uri:
        defaultValue: ''
        description: 'URI of the desired destination dataset. If not

          specified, a resource will be created under a new dataset in the project.'
        isOptional: true
        parameterType: STRING
      data_source_bigquery_table_path:
        defaultValue: ''
        description: 'The BigQuery table path of format

          bq://bq_project.bq_dataset.bq_table'
        isOptional: true
        parameterType: STRING
      data_source_csv_filenames:
        defaultValue: ''
        description: 'A string that represents a list of comma

          separated CSV filenames.'
        isOptional: true
        parameterType: STRING
      encryption_spec_key_name:
        defaultValue: ''
        description: The KMS key name.
        isOptional: true
        parameterType: STRING
      generate_explanation:
        defaultValue: false
        description: 'Generate explanation along with the batch prediction

          results. This will cause the batch prediction output to include

          explanations.'
        isOptional: true
        parameterType: BOOLEAN
      location:
        description: The GCP region for Vertex AI.
        parameterType: STRING
      model_name:
        description: ARIMA_PLUS BQML model URI.
        parameterType: STRING
      project:
        description: The GCP project that runs the pipeline components.
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0-rc.2
