# PIPELINE DEFINITION
# Name: prophet-predict
# Description: Creates a batch prediction using a Prophet model.
# Inputs:
#    bigquery_destination_uri: str [Default: '']
#    data_source_bigquery_table_path: str [Default: '']
#    data_source_csv_filenames: str [Default: '']
#    encryption_spec_key_name: str [Default: '']
#    location: str
#    machine_type: str [Default: 'n1-standard-2']
#    max_num_workers: int [Default: 10.0]
#    model_name: str
#    project: str
#    target_column: str
#    time_column: str
#    time_series_identifier_column: str
components:
  comp-bigquery-create-dataset:
    executorLabel: exec-bigquery-create-dataset
    inputDefinitions:
      parameters:
        dataset:
          parameterType: STRING
        exists_ok:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        location:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-bigquery-delete-dataset-with-prefix:
    executorLabel: exec-bigquery-delete-dataset-with-prefix
    inputDefinitions:
      parameters:
        dataset_prefix:
          parameterType: STRING
        delete_contents:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        project:
          parameterType: STRING
  comp-bigquery-query-job:
    executorLabel: exec-bigquery-query-job
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Describes the Cloud

            KMS encryption key that will be used to protect destination

            BigQuery table. The BigQuery Service Account associated with your

            project requires access to this encryption key. If

            encryption_spec_key_name are both specified in here and in

            job_configuration_query, the value in here will override the other

            one.'
          isOptional: true
          parameterType: STRING
        job_configuration_query:
          defaultValue: {}
          description: 'A json formatted string

            describing the rest of the job configuration.  For more details, see

            https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationQuery'
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: 'The labels associated with this job. You can

            use these to organize and group your jobs. Label keys and values can

            be no longer than 63 characters, can only containlowercase letters,

            numeric characters, underscores and dashes. International characters

            are allowed. Label values are optional. Label keys must start with a

            letter and each label in the list must have a different key.

            Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: 'Location for creating the BigQuery job. If not

            set, default to `US` multi-region.  For more details, see

            https://cloud.google.com/bigquery/docs/locations#specifying_your_location'
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to run the BigQuery query job. Defaults to the project
            in which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        query:
          defaultValue: ''
          description: 'SQL query text to execute. Only standard SQL is

            supported.  If query are both specified in here and in

            job_configuration_query, the value in here will override the other

            one.'
          isOptional: true
          parameterType: STRING
        query_parameters:
          defaultValue: []
          description: 'jobs.query parameters for

            standard SQL queries.  If query_parameters are both specified in here

            and in job_configuration_query, the value in here will override the

            other one.'
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        destination_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
          description: 'Describes the table where the query results should be stored.

            This property must be set for large results that exceed the maximum

            response size.

            For queries that produce anonymous (cached) results, this field will

            be populated by BigQuery.'
      parameters:
        gcp_resources:
          description: 'Serialized gcp_resources proto tracking the BigQuery job.

            For more details, see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
  comp-bigquery-query-job-2:
    executorLabel: exec-bigquery-query-job-2
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Describes the Cloud

            KMS encryption key that will be used to protect destination

            BigQuery table. The BigQuery Service Account associated with your

            project requires access to this encryption key. If

            encryption_spec_key_name are both specified in here and in

            job_configuration_query, the value in here will override the other

            one.'
          isOptional: true
          parameterType: STRING
        job_configuration_query:
          defaultValue: {}
          description: 'A json formatted string

            describing the rest of the job configuration.  For more details, see

            https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationQuery'
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: 'The labels associated with this job. You can

            use these to organize and group your jobs. Label keys and values can

            be no longer than 63 characters, can only containlowercase letters,

            numeric characters, underscores and dashes. International characters

            are allowed. Label values are optional. Label keys must start with a

            letter and each label in the list must have a different key.

            Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: 'Location for creating the BigQuery job. If not

            set, default to `US` multi-region.  For more details, see

            https://cloud.google.com/bigquery/docs/locations#specifying_your_location'
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to run the BigQuery query job. Defaults to the project
            in which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        query:
          defaultValue: ''
          description: 'SQL query text to execute. Only standard SQL is

            supported.  If query are both specified in here and in

            job_configuration_query, the value in here will override the other

            one.'
          isOptional: true
          parameterType: STRING
        query_parameters:
          defaultValue: []
          description: 'jobs.query parameters for

            standard SQL queries.  If query_parameters are both specified in here

            and in job_configuration_query, the value in here will override the

            other one.'
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        destination_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
          description: 'Describes the table where the query results should be stored.

            This property must be set for large results that exceed the maximum

            response size.

            For queries that produce anonymous (cached) results, this field will

            be populated by BigQuery.'
      parameters:
        gcp_resources:
          description: 'Serialized gcp_resources proto tracking the BigQuery job.

            For more details, see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
  comp-build-job-configuration-query:
    executorLabel: exec-build-job-configuration-query
    inputDefinitions:
      parameters:
        dataset_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        priority:
          defaultValue: INTERACTIVE
          isOptional: true
          parameterType: STRING
        project_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        table_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        write_disposition:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-build-job-configuration-query-2:
    executorLabel: exec-build-job-configuration-query-2
    inputDefinitions:
      parameters:
        dataset_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        priority:
          defaultValue: INTERACTIVE
          isOptional: true
          parameterType: STRING
        project_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        table_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        write_disposition:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-exit-handler-1:
    dag:
      tasks:
        bigquery-create-dataset:
          cachingOptions: {}
          componentRef:
            name: comp-bigquery-create-dataset
          dependentTasks:
          - get-table-location
          - validate-inputs
          inputs:
            parameters:
              dataset:
                runtimeValue:
                  constant: tmp_{{$.pipeline_job_uuid}}
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: create-tmp-dataset
        bigquery-query-job:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-query-job
          dependentTasks:
          - build-job-configuration-query
          - get-first-valid
          - get-table-location
          inputs:
            parameters:
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              job_configuration_query:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: build-job-configuration-query
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              pipelinechannel--get-first-valid-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-first-valid
              pipelinechannel--time_column:
                componentInputParameter: pipelinechannel--time_column
              pipelinechannel--time_series_identifier_column:
                componentInputParameter: pipelinechannel--time_series_identifier_column
              project:
                componentInputParameter: pipelinechannel--project
              query:
                runtimeValue:
                  constant: "\n      WITH\n        base_data AS (\n          SELECT\
                    \ * FROM `{{$.inputs.parameters['pipelinechannel--get-first-valid-Output']}}`\n\
                    \        )\n      SELECT\n        CAST({{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}}\
                    \ AS STRING) AS {{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}},\n\
                    \        ARRAY_AGG(TIMESTAMP({{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ ORDER BY {{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ AS {{$.inputs.parameters['pipelinechannel--time_column']}},\n\
                    \        \n        \n        \n      FROM base_data\n      GROUP\
                    \ BY {{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}}\n\
                    \  "
          taskInfo:
            name: remove-feature-columns
        bigquery-query-job-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-query-job-2
          dependentTasks:
          - build-job-configuration-query-2
          - get-table-location-2
          - table-to-uri-2
          inputs:
            parameters:
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              job_configuration_query:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: build-job-configuration-query-2
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location-2
              pipelinechannel--table-to-uri-2-uri:
                taskOutputParameter:
                  outputParameterKey: uri
                  producerTask: table-to-uri-2
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
              pipelinechannel--time_column:
                componentInputParameter: pipelinechannel--time_column
              pipelinechannel--time_series_identifier_column:
                componentInputParameter: pipelinechannel--time_series_identifier_column
              project:
                componentInputParameter: pipelinechannel--project
              query:
                runtimeValue:
                  constant: "\n      WITH\n        predictions AS (\n          SELECT\n\
                    \            {{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}},\n\
                    \            JSON_QUERY_ARRAY(prediction, '$.{{$.inputs.parameters['pipelinechannel--time_column']}}')\
                    \ AS {{$.inputs.parameters['pipelinechannel--time_column']}},\n\
                    \            JSON_EXTRACT(\n              prediction,\n      \
                    \        '$.predicted_on_{{$.inputs.parameters['pipelinechannel--time_column']}}'\n\
                    \            ) AS predicted_on_{{$.inputs.parameters['pipelinechannel--time_column']}},\n\
                    \            JSON_QUERY_ARRAY(\n              prediction,\n  \
                    \            '$.predicted_{{$.inputs.parameters['pipelinechannel--target_column']}}'\n\
                    \            ) AS predicted_{{$.inputs.parameters['pipelinechannel--target_column']}},\n\
                    \          FROM `{{$.inputs.parameters['pipelinechannel--table-to-uri-2-uri']}}`\n\
                    \        )\n        SELECT\n          {{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}},\n\
                    \          PARSE_TIMESTAMP(\n            '\\\"%Y-%m-%dT%H:%M:%SZ\\\
                    \"',\n            predicted_on_{{$.inputs.parameters['pipelinechannel--time_column']}}\n\
                    \          ) AS predicted_on_{{$.inputs.parameters['pipelinechannel--time_column']}},\n\
                    \          PARSE_TIMESTAMP(\n            '\\\"%Y-%m-%dT%H:%M:%SZ\\\
                    \"',\n            {{$.inputs.parameters['pipelinechannel--time_column']}}[SAFE_OFFSET(index)]\n\
                    \          ) AS {{$.inputs.parameters['pipelinechannel--time_column']}},\n\
                    \          STRUCT(\n            CAST(predicted_{{$.inputs.parameters['pipelinechannel--target_column']}}[SAFE_OFFSET(index)]\
                    \ AS FLOAT64)\n              AS value\n          ) AS predicted_{{$.inputs.parameters['pipelinechannel--target_column']}}\n\
                    \        FROM predictions\n        CROSS JOIN\n          UNNEST(GENERATE_ARRAY(0,\
                    \ ARRAY_LENGTH({{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ - 1)) AS index\n  "
          taskInfo:
            name: create-predictions-table
        build-job-configuration-query:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-build-job-configuration-query
          dependentTasks:
          - bigquery-create-dataset
          inputs:
            parameters:
              dataset_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-dataset_id'']}}'
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              project_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-project_id'']}}'
              table_id:
                runtimeValue:
                  constant: data
              write_disposition:
                runtimeValue:
                  constant: WRITE_EMPTY
          taskInfo:
            name: build-job-configuration-query
        build-job-configuration-query-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-build-job-configuration-query-2
          dependentTasks:
          - table-to-uri-2
          inputs:
            parameters:
              dataset_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--table-to-uri-2-dataset_id'']}}'
              pipelinechannel--table-to-uri-2-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: table-to-uri-2
              pipelinechannel--table-to-uri-2-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: table-to-uri-2
              pipelinechannel--table-to-uri-2-table_id:
                taskOutputParameter:
                  outputParameterKey: table_id
                  producerTask: table-to-uri-2
              project_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--table-to-uri-2-project_id'']}}'
              table_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--table-to-uri-2-table_id'']}}'
              write_disposition:
                runtimeValue:
                  constant: WRITE_TRUNCATE
          taskInfo:
            name: build-job-configuration-query-2
        get-first-valid:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-first-valid
          dependentTasks:
          - load-table-from-uri
          inputs:
            parameters:
              pipelinechannel--data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
              pipelinechannel--load-table-from-uri-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: load-table-from-uri
              values:
                runtimeValue:
                  constant: '["{{$.inputs.parameters[''pipelinechannel--data_source_bigquery_table_path'']}}",
                    "{{$.inputs.parameters[''pipelinechannel--load-table-from-uri-Output'']}}"]'
          taskInfo:
            name: get-first-valid
        get-table-location:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-table-location
          inputs:
            parameters:
              default_location:
                componentInputParameter: pipelinechannel--location
              project:
                componentInputParameter: pipelinechannel--project
              table:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
          taskInfo:
            name: get-table-location
        get-table-location-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-table-location-2
          dependentTasks:
          - table-to-uri-2
          inputs:
            parameters:
              project:
                componentInputParameter: pipelinechannel--project
              table:
                taskOutputParameter:
                  outputParameterKey: uri
                  producerTask: table-to-uri-2
          taskInfo:
            name: get-table-location-2
        load-table-from-uri:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-load-table-from-uri
          dependentTasks:
          - bigquery-create-dataset
          - get-table-location
          inputs:
            parameters:
              destination:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-project_id'']}}.{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-dataset_id'']}}.csv_export'
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              project:
                componentInputParameter: pipelinechannel--project
              source_format:
                runtimeValue:
                  constant: CSV
              source_uris:
                componentInputParameter: pipelinechannel--data_source_csv_filenames
          taskInfo:
            name: load-table-from-uri
        make-vertex-model-artifact:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-make-vertex-model-artifact
          inputs:
            parameters:
              location:
                componentInputParameter: pipelinechannel--location
              model_resource_name:
                componentInputParameter: pipelinechannel--model_name
          taskInfo:
            name: make-vertex-model-artifact
        maybe-replace-with-default:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-maybe-replace-with-default
          inputs:
            parameters:
              default:
                componentInputParameter: pipelinechannel--project
              value:
                componentInputParameter: pipelinechannel--bigquery_destination_uri
          taskInfo:
            name: maybe-replace-with-default
        model-batch-predict:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-batch-predict
          dependentTasks:
          - make-vertex-model-artifact
          - maybe-replace-with-default
          - table-to-uri
          inputs:
            artifacts:
              model:
                taskOutputArtifact:
                  outputArtifactKey: vertex_model
                  producerTask: make-vertex-model-artifact
            parameters:
              bigquery_destination_output_uri:
                runtimeValue:
                  constant: bq://{{$.inputs.parameters['pipelinechannel--maybe-replace-with-default-Output']}}
              bigquery_source_input_uri:
                runtimeValue:
                  constant: bq://{{$.inputs.parameters['pipelinechannel--table-to-uri-uri']}}
              instances_format:
                runtimeValue:
                  constant: bigquery
              job_display_name:
                runtimeValue:
                  constant: batch-predict-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
              location:
                componentInputParameter: pipelinechannel--location
              machine_type:
                componentInputParameter: pipelinechannel--machine_type
              max_replica_count:
                componentInputParameter: pipelinechannel--max_num_workers
              pipelinechannel--maybe-replace-with-default-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: maybe-replace-with-default
              pipelinechannel--table-to-uri-uri:
                taskOutputParameter:
                  outputParameterKey: uri
                  producerTask: table-to-uri
              predictions_format:
                runtimeValue:
                  constant: bigquery
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: model-batch-predict
        table-to-uri:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-table-to-uri
          dependentTasks:
          - bigquery-query-job
          inputs:
            artifacts:
              table:
                taskOutputArtifact:
                  outputArtifactKey: destination_table
                  producerTask: bigquery-query-job
          taskInfo:
            name: table-to-uri
        table-to-uri-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-table-to-uri-2
          dependentTasks:
          - model-batch-predict
          inputs:
            artifacts:
              table:
                taskOutputArtifact:
                  outputArtifactKey: bigquery_output_table
                  producerTask: model-batch-predict
          taskInfo:
            name: table-to-uri-2
        validate-inputs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-validate-inputs
          inputs:
            parameters:
              bigquery_destination_uri:
                componentInputParameter: pipelinechannel--bigquery_destination_uri
              data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
              data_source_csv_filenames:
                componentInputParameter: pipelinechannel--data_source_csv_filenames
          taskInfo:
            name: validate-inputs
    inputDefinitions:
      parameters:
        pipelinechannel--bigquery_destination_uri:
          parameterType: STRING
        pipelinechannel--data_source_bigquery_table_path:
          parameterType: STRING
        pipelinechannel--data_source_csv_filenames:
          parameterType: STRING
        pipelinechannel--encryption_spec_key_name:
          parameterType: STRING
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--machine_type:
          parameterType: STRING
        pipelinechannel--max_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
        pipelinechannel--time_column:
          parameterType: STRING
        pipelinechannel--time_series_identifier_column:
          parameterType: STRING
  comp-get-first-valid:
    executorLabel: exec-get-first-valid
    inputDefinitions:
      parameters:
        values:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-table-location:
    executorLabel: exec-get-table-location
    inputDefinitions:
      parameters:
        default_location:
          defaultValue: ''
          description: Location to return if no table was given.
          isOptional: true
          parameterType: STRING
        project:
          description: The GCP project.
          parameterType: STRING
        table:
          description: The BigQuery table to get a location for.
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-table-location-2:
    executorLabel: exec-get-table-location-2
    inputDefinitions:
      parameters:
        default_location:
          defaultValue: ''
          description: Location to return if no table was given.
          isOptional: true
          parameterType: STRING
        project:
          description: The GCP project.
          parameterType: STRING
        table:
          description: The BigQuery table to get a location for.
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-load-table-from-uri:
    executorLabel: exec-load-table-from-uri
    inputDefinitions:
      parameters:
        destination:
          description: Table into which data is to be loaded.
          parameterType: STRING
        location:
          description: The GCP region.
          parameterType: STRING
        project:
          description: The GCP project.
          parameterType: STRING
        source_format:
          defaultValue: CSV
          description: 'The file format for the files being imported. Only CSV is

            supported.'
          isOptional: true
          parameterType: STRING
        source_uris:
          description: 'URIs of data files to be loaded; in format

            gs://<bucket_name>/<object_name_or_glob>.'
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-make-vertex-model-artifact:
    executorLabel: exec-make-vertex-model-artifact
    inputDefinitions:
      parameters:
        location:
          parameterType: STRING
        model_resource_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        vertex_model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-maybe-replace-with-default:
    executorLabel: exec-maybe-replace-with-default
    inputDefinitions:
      parameters:
        default:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        value:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-model-batch-predict:
    executorLabel: exec-model-batch-predict
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: 'The Model used to get predictions via this job. Must share
            the same

            ancestor Location. Starting this job has no impact on any existing

            deployments of the Model and their resources. Either this or

            `unmanaged_container_model` must be specified.'
          isOptional: true
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          description: 'The unmanaged container model used to get predictions via
            this job.

            This should be used for models that are not uploaded to Vertex. Either

            this or model must be specified.'
          isOptional: true
      parameters:
        accelerator_count:
          defaultValue: 0.0
          description: 'The number of accelerators to attach

            to the `machine_type`. Only used if `machine_type` is set.  For more

            details about the machine spec, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec'
          isOptional: true
          parameterType: NUMBER_INTEGER
        accelerator_type:
          defaultValue: ''
          description: 'The type of accelerator(s) that may be

            attached to the machine as per `accelerator_count`. Only used if

            `machine_type` is set.  For more details about the machine spec, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec'
          isOptional: true
          parameterType: STRING
        bigquery_destination_output_uri:
          defaultValue: ''
          description: 'The BigQuery project location where the output is to be written
            to. In

            the given project a new dataset is created with name

            `prediction_<model-display-name>_<job-create-time>` where is made

            BigQuery-dataset-name compatible (for example, most special characters

            become underscores), and timestamp is in YYYY_MM_DDThh_mm_ss_sssZ

            "based on ISO-8601" format. In the dataset two tables will be created,

            `predictions`, and `errors`. If the Model has both `instance`

            and `prediction` schemata defined then the tables have columns as

            follows: The `predictions` table contains instances for which the

            prediction succeeded, it has columns as per a concatenation of the

            Model''s instance and prediction schemata. The `errors` table

            contains rows for which the prediction has failed, it has instance

            columns, as per the instance schema, followed by a single "errors"

            column, which as values has [google.rpc.Status](Status)

            represented as a STRUCT, and containing only `code` and

            `message`. For more details about this output config, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.'
          isOptional: true
          parameterType: STRING
        bigquery_source_input_uri:
          defaultValue: ''
          description: 'BigQuery URI to a table, up to 2000 characters long. For example:

            `projectId.bqDatasetId.bqTableId`  For more details about this input

            config, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.'
          isOptional: true
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Customer-managed encryption

            key options for a BatchPredictionJob. If this is set, then all

            resources created by the BatchPredictionJob will be encrypted with the

            provided encryption key.  Has the form:

            `projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key`.

            The key needs to be in the same region as where the compute resource

            is created.'
          isOptional: true
          parameterType: STRING
        excluded_fields:
          defaultValue: []
          description: 'Fields that will be excluded in the prediction instance that
            is

            sent to the Model.

            Excluded will be attached to the batch prediction output if

            key_field is not specified.

            When `excluded_fields` is populated, `included_fields` must be empty.

            The input must be JSONL with objects at each line, CSV, BigQuery

            or TfRecord.

            may be specified via the Model''s `parameters_schema_uri`.'
          isOptional: true
          parameterType: LIST
        explanation_metadata:
          defaultValue: {}
          description: 'Explanation metadata

            configuration for this BatchPredictionJob. Can be specified only if

            `generate_explanation` is set to `True`.  This value overrides the

            value of `Model.explanation_metadata`. All fields of

            `explanation_metadata` are optional in the request. If a field of the

            `explanation_metadata` object is not populated, the corresponding

            field of the `Model.explanation_metadata` object is inherited.  For

            more details, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata.'
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          description: 'Parameters to configure

            explaining for Model''s predictions. Can be specified only if

            `generate_explanation` is set to `True`.  This value overrides the

            value of `Model.explanation_parameters`. All fields of

            `explanation_parameters` are optional in the request. If a field of

            the `explanation_parameters` object is not populated, the

            corresponding field of the `Model.explanation_parameters` object is

            inherited.  For more details, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters.'
          isOptional: true
          parameterType: STRUCT
        gcs_destination_output_uri_prefix:
          defaultValue: ''
          description: 'The Google Cloud

            Storage location of the directory where the output is to be written

            to. In the given directory a new directory is created. Its name is

            `prediction-<model-display-name>-<job-create-time>`, where timestamp

            is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files

            `predictions_0001.<extension>`, `predictions_0002.<extension>`,

            ..., `predictions_N.<extension>` are created where `<extension>`

            depends on chosen `predictions_format`, and N may equal 0001 and

            depends on the total number of successfully predicted instances. If

            the Model has both `instance` and `prediction` schemata defined

            then each such file contains predictions as per the

            `predictions_format`. If prediction for any instance failed

            (partially or completely), then an additional

            `errors_0001.<extension>`, `errors_0002.<extension>`,...,

            `errors_N.<extension>` files are created (N depends on total number

            of failed predictions). These files contain the failed instances, as

            per their schema, followed by an additional `error` field which as

            value has `google.rpc.Status` containing only `code` and

            `message` fields.  For more details about this output config, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.'
          isOptional: true
          parameterType: STRING
        gcs_source_uris:
          defaultValue: []
          description: 'Google Cloud Storage URI(-s) to your instances to run batch
            prediction

            on. They must match `instances_format`. May contain wildcards. For more

            information on wildcards, see [WildcardNames](https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames).

            For more details about this input config, see [InputConfig](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig).'
          isOptional: true
          parameterType: LIST
        generate_explanation:
          defaultValue: false
          description: 'Generate explanation along with

            the batch prediction results. This will cause the batch prediction

            output to include explanations based on the `prediction_format`: -

            `bigquery`: output includes a column named `explanation`. The value is

            a struct that conforms to the [aiplatform.gapic.Explanation] object. -

            `jsonl`: The JSON objects on each line include an additional entry

            keyed `explanation`. The value of the entry is a JSON object that

            conforms to the [aiplatform.gapic.Explanation] object. - `csv`:

            Generating explanations for CSV format is not supported.  If this

            field is set to true, either the Model.explanation_spec or

            explanation_metadata and explanation_parameters must be populated.'
          isOptional: true
          parameterType: BOOLEAN
        included_fields:
          defaultValue: []
          description: 'Fields that will be included in the prediction instance that
            is

            sent to the Model.

            If `instance_type` is `array`, the order of field names in

            `included_fields` also determines the order of the values in the array.

            When `included_fields` is populated, `excluded_fields` must be empty.

            The input must be JSONL with objects at each line, CSV, BigQuery

            or TfRecord.'
          isOptional: true
          parameterType: LIST
        instance_type:
          defaultValue: ''
          description: "The format of the instance that the Model\naccepts. Vertex\
            \ AI will convert compatible\n[InstancesFormat](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig)\n\
            to the specified format. Supported values are:\n`object`: Each input is\
            \ converted to JSON object format.\n  * For `bigquery`, each row is converted\
            \ to an object.\n  * For `jsonl`, each line of the JSONL input must be\
            \ an object.\n  * Does not apply to `csv`, `file-list`, `tf-record`, or\
            \ `tf-record-gzip`.\n`array`: Each input is converted to JSON array format.\n\
            \  * For `bigquery`, each row is converted to an array. The order\n  \
            \  of columns is determined by the BigQuery column order, unless\n   \
            \ [included_fields](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig)\
            \ is populated.\n    `included_fields` must be populated for specifying\
            \ field orders.\n  * For `jsonl`, if each line of the JSONL input is an\
            \ object,\n    `included_fields` must be populated for specifying field\
            \ orders.\n  * Does not apply to `csv`, `file-list`, `tf-record`, or\n\
            \    `tf-record-gzip`.\nIf not specified, Vertex AI converts the batch\
            \ prediction input as\nfollows:\n * For `bigquery` and `csv`, the behavior\
            \ is the same as `array`. The\n   order of columns is the same as defined\
            \ in the file or table, unless\n   included_fields is populated.\n * For\
            \ `jsonl`, the prediction instance format is determined by\n   each line\
            \ of the input.\n * For `tf-record`/`tf-record-gzip`, each record will\
            \ be converted to\n   an object in the format of `{\"b64\": <value>}`,\
            \ where `<value>` is\n   the Base64-encoded string of the content of the\
            \ record.\n * For `file-list`, each file in the list will be converted\
            \ to an\n   object in the format of `{\"b64\": <value>}`, where `<value>`\
            \ is\n   the Base64-encoded string of the content of the file."
          isOptional: true
          parameterType: STRING
        instances_format:
          defaultValue: jsonl
          description: 'The format in which instances are

            given, must be one of the [Model](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models)''s
            supportedInputStorageFormats.

            For more details about this input config, see

            [InputConfig](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.)'
          isOptional: true
          parameterType: STRING
        job_display_name:
          description: The user-defined name of this BatchPredictionJob.
          parameterType: STRING
        key_field:
          defaultValue: ''
          description: "The name of the field that is considered as a key.\nThe values\
            \ identified by the key field is not included in the\ntransformed instances\
            \ that is sent to the Model. This is similar to\nspecifying this name\
            \ of the field in [excluded_fields](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig).\
            \ In addition,\nthe batch prediction output will not include the instances.\
            \ Instead the\noutput will only include the value of the key field, in\
            \ a field named\n`key` in the output:\n * For `jsonl` output format, the\
            \ output will have a `key` field\n   instead of the `instance` field.\n\
            \ * For `csv`/`bigquery` output format, the output will have have a `key`\n\
            \   column instead of the instance feature columns.\nThe input must be\
            \ JSONL with objects at each line, CSV, BigQuery\nor TfRecord."
          isOptional: true
          parameterType: STRING
        labels:
          defaultValue: {}
          description: 'The labels with user-defined metadata to

            organize your BatchPredictionJobs.  Label keys and values can be no

            longer than 64 characters (Unicode codepoints), can only contain

            lowercase letters, numeric characters, underscores and dashes.

            International characters are allowed.  See https://goo.gl/xmQnxf for

            more information and examples of labels.'
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: Location for creating the BatchPredictionJob.
          isOptional: true
          parameterType: STRING
        machine_type:
          defaultValue: ''
          description: 'The type of machine for running batch

            prediction on dedicated resources. If the Model supports

            DEDICATED_RESOURCES this config may be provided (and the job will use

            these resources). If the Model doesn''t support AUTOMATIC_RESOURCES,

            this config must be provided.  For more details about the

            BatchDedicatedResources, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.

            For more details about the machine spec, see

            https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec'
          isOptional: true
          parameterType: STRING
        manual_batch_tuning_parameters_batch_size:
          defaultValue: 0.0
          description: 'The number of

            the records (e.g. instances) of the operation given in each batch to a

            machine replica. Machine type, and size of a single record should be

            considered when setting this parameter, higher value speeds up the

            batch operation''s execution, but too high value will result in a whole

            batch not fitting in a machine''s memory, and the whole operation will

            fail.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_replica_count:
          defaultValue: 0.0
          description: 'The maximum number of machine replicas the batch operation
            may be scaled

            to. Only used if `machine_type` is set.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_parameters:
          defaultValue: {}
          description: The parameters that govern the predictions. The schema of the
            parameters
          isOptional: true
          parameterType: STRUCT
        predictions_format:
          defaultValue: jsonl
          description: 'The format in which Vertex AI gives the predictions. Must
            be one of the

            Model''s supportedOutputStorageFormats.

            For more details about this output config, see [OutputConfig](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig).'
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to create the BatchPredictionJob. Defaults to the project
            in which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        starting_replica_count:
          defaultValue: 0.0
          description: 'The number of machine replicas

            used at the start of the batch operation. If not set, Vertex AI

            decides starting number, not greater than `max_replica_count`. Only

            used if `machine_type` is set.'
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        batchpredictionjob:
          artifactType:
            schemaTitle: google.VertexBatchPredictionJob
            schemaVersion: 0.0.1
          description: '[**Deprecated. Use gcs_output_directory and bigquery_output_table

            instead.**] Artifact

            representation of the created batch prediction job.'
        bigquery_output_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
          description: 'Artifact tracking the batch prediction job output. This is
            only

            available if

            bigquery_output_table is specified.'
        gcs_output_directory:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Artifact tracking the batch prediction job output. This is
            only

            available if

            gcs_destination_output_uri_prefix is specified.'
      parameters:
        gcp_resources:
          description: 'Serialized gcp_resources proto tracking the batch prediction
            job.

            For more details, see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
  comp-table-to-uri:
    executorLabel: exec-table-to-uri
    inputDefinitions:
      artifacts:
        table:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        use_bq_prefix:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
        table_id:
          parameterType: STRING
        uri:
          parameterType: STRING
  comp-table-to-uri-2:
    executorLabel: exec-table-to-uri-2
    inputDefinitions:
      artifacts:
        table:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        use_bq_prefix:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
        table_id:
          parameterType: STRING
        uri:
          parameterType: STRING
  comp-validate-inputs:
    executorLabel: exec-validate-inputs
    inputDefinitions:
      parameters:
        bigquery_destination_uri:
          isOptional: true
          parameterType: STRING
        data_granularity_unit:
          isOptional: true
          parameterType: STRING
        data_source_bigquery_table_path:
          isOptional: true
          parameterType: STRING
        data_source_csv_filenames:
          isOptional: true
          parameterType: STRING
        optimization_objective:
          isOptional: true
          parameterType: STRING
        predefined_split_key:
          isOptional: true
          parameterType: STRING
        source_model_uri:
          isOptional: true
          parameterType: STRING
        target_column:
          isOptional: true
          parameterType: STRING
        test_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        time_column:
          isOptional: true
          parameterType: STRING
        time_series_identifier_column:
          isOptional: true
          parameterType: STRING
        timestamp_split_key:
          isOptional: true
          parameterType: STRING
        training_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        validation_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        window_column:
          isOptional: true
          parameterType: STRING
        window_max_count:
          isOptional: true
          parameterType: NUMBER_INTEGER
        window_stride_length:
          isOptional: true
          parameterType: NUMBER_INTEGER
deploymentSpec:
  executors:
    exec-bigquery-create-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_create_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_create_dataset(\n    project: str,\n    location: str,\n\
          \    dataset: str,\n    exists_ok: bool = False,\n) -> NamedTuple('Outputs',\
          \ [('project_id', str), ('dataset_id', str)]):\n  \"\"\"Creates a BigQuery\
          \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
          \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project, location=location)\n  ref\
          \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
          \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
          \   ref.project, ref.dataset_id)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-bigquery-delete-dataset-with-prefix:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_delete_dataset_with_prefix
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_delete_dataset_with_prefix(\n    project: str,\n   \
          \ dataset_prefix: str,\n    delete_contents: bool = False,\n) -> None:\n\
          \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
          \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
          \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
          \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-bigquery-query-job:
      container:
        args:
        - --type
        - BigqueryQueryJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --payload
        - '{"Concat": ["{", "\"configuration\": {", "\"query\": ", "{{$.inputs.parameters[''job_configuration_query'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}", "}"]}'
        - --job_configuration_query_override
        - '{"Concat": ["{", "\"query\": \"", "{{$.inputs.parameters[''query'']}}",
          "\"", ", \"query_parameters\": ", "{{$.inputs.parameters[''query_parameters'']}}",
          ", \"destination_encryption_configuration\": {", "\"kmsKeyName\": \"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.bigquery.query_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1
    exec-bigquery-query-job-2:
      container:
        args:
        - --type
        - BigqueryQueryJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --payload
        - '{"Concat": ["{", "\"configuration\": {", "\"query\": ", "{{$.inputs.parameters[''job_configuration_query'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}", "}"]}'
        - --job_configuration_query_override
        - '{"Concat": ["{", "\"query\": \"", "{{$.inputs.parameters[''query'']}}",
          "\"", ", \"query_parameters\": ", "{{$.inputs.parameters[''query_parameters'']}}",
          ", \"destination_encryption_configuration\": {", "\"kmsKeyName\": \"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.bigquery.query_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1
    exec-build-job-configuration-query:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - build_job_configuration_query
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef build_job_configuration_query(\n    project_id: str = '',\n \
          \   dataset_id: str = '',\n    table_id: str = '',\n    write_disposition:\
          \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
          \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
          \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
          \    config['destinationTable'] = {\n        'projectId': project_id,\n\
          \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
          \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
          \  return config\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-build-job-configuration-query-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - build_job_configuration_query
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef build_job_configuration_query(\n    project_id: str = '',\n \
          \   dataset_id: str = '',\n    table_id: str = '',\n    write_disposition:\
          \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
          \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
          \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
          \    config['destinationTable'] = {\n        'projectId': project_id,\n\
          \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
          \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
          \  return config\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-get-first-valid:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_first_valid
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_first_valid(values: str) -> str:\n  \"\"\"Returns the first\
          \ truthy value from the given serialized JSON list.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import json\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  for value in json.loads(values):\n    if value:\n      return value\n\
          \  raise ValueError('No valid values.')\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-get-table-location:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_table_location
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_table_location(\n    project: str,\n    table: Optional[str],\n\
          \    default_location: str = '',\n) -> str:\n  \"\"\"Returns the region\
          \ the given table belongs to.\n\n  Args:\n    project: The GCP project.\n\
          \    table: The BigQuery table to get a location for.\n    default_location:\
          \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
          \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
          \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
          \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
          \  return client.get_table(table).location\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-get-table-location-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_table_location
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_table_location(\n    project: str,\n    table: Optional[str],\n\
          \    default_location: str = '',\n) -> str:\n  \"\"\"Returns the region\
          \ the given table belongs to.\n\n  Args:\n    project: The GCP project.\n\
          \    table: The BigQuery table to get a location for.\n    default_location:\
          \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
          \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
          \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
          \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
          \  return client.get_table(table).location\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-load-table-from-uri:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_table_from_uri
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-rc.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_table_from_uri(\n    project: str,\n    location: str,\n\
          \    source_uris: str,\n    destination: str,\n    source_format: str =\
          \ 'CSV',\n) -> str:\n  \"\"\"Creates a table from a list of URIs.\n\n  Args:\n\
          \    project: The GCP project.\n    location: The GCP region.\n    source_uris:\
          \ URIs of data files to be loaded; in format\n      gs://<bucket_name>/<object_name_or_glob>.\n\
          \    destination: Table into which data is to be loaded.\n    source_format:\
          \ The file format for the files being imported. Only CSV is\n      supported.\n\
          \n  Returns:\n    The destination table containing imported data.\n  \"\"\
          \"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  if not source_uris:\n    return ''\n\n  csv_list = [filename.strip()\
          \ for filename in source_uris.split(',')]\n  client = bigquery.Client(project=project,\
          \ location=location)\n  job_config = bigquery.LoadJobConfig(\n      autodetect=True,\
          \ source_format=source_format)\n  client.load_table_from_uri(\n      source_uris=csv_list,\n\
          \      destination=destination,\n      project=project,\n      location=location,\n\
          \      job_config=job_config).result()\n  return destination\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-make-vertex-model-artifact:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - make_vertex_model_artifact
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef make_vertex_model_artifact(\n    location: str,\n    model_resource_name:\
          \ str,\n    vertex_model: dsl.Output[dsl.Artifact],\n) -> None:\n  \"\"\"\
          Creates a google.VertexModel artifact.\"\"\"\n  vertex_model.metadata =\
          \ {'resourceName': model_resource_name}\n  vertex_model.uri = (f'https://{location}-aiplatform.googleapis.com'\n\
          \                      f'/v1/{model_resource_name}')\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-maybe-replace-with-default:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - maybe_replace_with_default
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef maybe_replace_with_default(value: str, default: str = '') ->\
          \ str:\n  \"\"\"Replaces string with another value if it is a dash.\"\"\"\
          \n  return default if not value else value\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-model-batch-predict:
      container:
        args:
        - --type
        - BatchPredictionJob
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''job_display_name'']}}",
          "\", ", {"IfPresent": {"InputName": "model", "Then": {"Concat": ["\"model\":
          \"", "{{$.inputs.artifacts[''model''].metadata[''resourceName'']}}", "\","]}}},
          " \"input_config\": {", "\"instances_format\": \"", "{{$.inputs.parameters[''instances_format'']}}",
          "\"", ", \"gcs_source\": {", "\"uris\":", "{{$.inputs.parameters[''gcs_source_uris'']}}",
          "}", ", \"bigquery_source\": {", "\"input_uri\": \"", "{{$.inputs.parameters[''bigquery_source_input_uri'']}}",
          "\"", "}", "}", ", \"instance_config\": {", "\"instance_type\": \"", "{{$.inputs.parameters[''instance_type'']}}",
          "\"", ", \"key_field\": \"", "{{$.inputs.parameters[''key_field'']}}", "\"
          ", {"IfPresent": {"InputName": "included_fields", "Then": {"Concat": [",
          \"included_fields\": ", "{{$.inputs.parameters[''included_fields'']}}"]}}},
          {"IfPresent": {"InputName": "excluded_fields", "Then": {"Concat": [", \"excluded_fields\":
          ", "{{$.inputs.parameters[''excluded_fields'']}}"]}}}, "}", ", \"model_parameters\":
          ", "{{$.inputs.parameters[''model_parameters'']}}", ", \"output_config\":
          {", "\"predictions_format\": \"", "{{$.inputs.parameters[''predictions_format'']}}",
          "\"", ", \"gcs_destination\": {", "\"output_uri_prefix\": \"", "{{$.inputs.parameters[''gcs_destination_output_uri_prefix'']}}",
          "\"", "}", ", \"bigquery_destination\": {", "\"output_uri\": \"", "{{$.inputs.parameters[''bigquery_destination_output_uri'']}}",
          "\"", "}", "}", ", \"dedicated_resources\": {", "\"machine_spec\": {", "\"machine_type\":
          \"", "{{$.inputs.parameters[''machine_type'']}}", "\"", ", \"accelerator_type\":
          \"", "{{$.inputs.parameters[''accelerator_type'']}}", "\"", ", \"accelerator_count\":
          ", "{{$.inputs.parameters[''accelerator_count'']}}", "}", ", \"starting_replica_count\":
          ", "{{$.inputs.parameters[''starting_replica_count'']}}", ", \"max_replica_count\":
          ", "{{$.inputs.parameters[''max_replica_count'']}}", "}", ", \"manual_batch_tuning_parameters\":
          {", "\"batch_size\": ", "{{$.inputs.parameters[''manual_batch_tuning_parameters_batch_size'']}}",
          "}", ", \"generate_explanation\": ", "{{$.inputs.parameters[''generate_explanation'']}}",
          ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", ", \"encryption_spec\":
          {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.batch_prediction_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1
    exec-table-to-uri:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - table_to_uri
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef table_to_uri(\n    table: dsl.Input[dsl.Artifact],\n    use_bq_prefix:\
          \ bool = False,\n) -> NamedTuple(\n    'Outputs',\n    [\n        ('project_id',\
          \ str),\n        ('dataset_id', str),\n        ('table_id', str),\n    \
          \    ('uri', str),\n    ],\n):\n  \"\"\"Converts a google.BQTable to a URI.\"\
          \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
          \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
          \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
          \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
          \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
          \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
          \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-table-to-uri-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - table_to_uri
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef table_to_uri(\n    table: dsl.Input[dsl.Artifact],\n    use_bq_prefix:\
          \ bool = False,\n) -> NamedTuple(\n    'Outputs',\n    [\n        ('project_id',\
          \ str),\n        ('dataset_id', str),\n        ('table_id', str),\n    \
          \    ('uri', str),\n    ],\n):\n  \"\"\"Converts a google.BQTable to a URI.\"\
          \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
          \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
          \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
          \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
          \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
          \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
          \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
    exec-validate-inputs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_inputs
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef validate_inputs(\n    time_column: Optional[str] = None,\n  \
          \  time_series_identifier_column: Optional[str] = None,\n    target_column:\
          \ Optional[str] = None,\n    data_source_bigquery_table_path: Optional[str]\
          \ = None,\n    training_fraction: Optional[float] = None,\n    validation_fraction:\
          \ Optional[float] = None,\n    test_fraction: Optional[float] = None,\n\
          \    predefined_split_key: Optional[str] = None,\n    timestamp_split_key:\
          \ Optional[str] = None,\n    data_source_csv_filenames: Optional[str] =\
          \ None,\n    source_model_uri: Optional[str] = None,\n    bigquery_destination_uri:\
          \ Optional[str] = None,\n    window_column: Optional[str] = None,\n    window_stride_length:\
          \ Optional[int] = None,\n    window_max_count: Optional[int] = None,\n \
          \   optimization_objective: Optional[str] = None,\n    data_granularity_unit:\
          \ Optional[str] = None,\n) -> None:\n  \"\"\"Checks training pipeline input\
          \ parameters are valid.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
          \  import re\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
          \n  project_pattern = r'([a-z0-9.-]+:)?[a-z][a-z0-9-_]{4,28}[a-z0-9]'\n\
          \  dataset_pattern = r'[a-zA-Z0-9_]+'\n  table_pattern = r'[^\\.\\:`]+'\n\
          \  dataset_uri_pattern = re.compile(\n      f'(bq://)?{project_pattern}[.:]{dataset_pattern}')\n\
          \  table_uri_pattern = re.compile(\n      f'(bq://)?{project_pattern}[.:]{dataset_pattern}[.:]{table_pattern}')\n\
          \n  # Validate BigQuery column and dataset names.\n  bigquery_column_parameters\
          \ = [\n      time_column,\n      time_series_identifier_column,\n      target_column,\n\
          \  ]\n  column_pattern = re.compile(r'[a-zA-Z_][a-zA-Z0-9_]{1,300}')\n \
          \ for column in bigquery_column_parameters:\n    if column and not column_pattern.fullmatch(column):\n\
          \      raise ValueError(f'Invalid column name: {column}.')\n  if (bigquery_destination_uri\
          \ and\n      not dataset_uri_pattern.fullmatch(bigquery_destination_uri)):\n\
          \    raise ValueError(\n        f'Invalid BigQuery dataset URI: {bigquery_destination_uri}.')\n\
          \  if (source_model_uri and not table_uri_pattern.fullmatch(source_model_uri)):\n\
          \    raise ValueError(f'Invalid BigQuery table URI: {source_model_uri}.')\n\
          \n  # Validate data source.\n  data_source_count = sum([bool(source) for\
          \ source in [\n      data_source_bigquery_table_path, data_source_csv_filenames]])\n\
          \  if data_source_count > 1:\n    raise ValueError(f'Expected 1 data source,\
          \ found {data_source_count}.')\n  if (data_source_bigquery_table_path\n\
          \      and not table_uri_pattern.fullmatch(data_source_bigquery_table_path)):\n\
          \    raise ValueError(\n        f'Invalid BigQuery table URI: {data_source_bigquery_table_path}.')\n\
          \  gcs_path_pattern = re.compile(r'gs:\\/\\/(.+)\\/([^\\/]+)')\n  if data_source_csv_filenames:\n\
          \    csv_list = [filename.strip()\n                for filename in data_source_csv_filenames.split(',')]\n\
          \    for gcs_path in csv_list:\n      if not gcs_path_pattern.fullmatch(gcs_path):\n\
          \        raise ValueError(f'Invalid path to CSV stored in GCS: {gcs_path}.')\n\
          \n  # Validate split spec.\n  fraction_splits = [\n      training_fraction,\n\
          \      validation_fraction,\n      test_fraction,\n  ]\n  fraction_splits\
          \ = [None if fraction == -1 else fraction\n                     for fraction\
          \ in fraction_splits]\n  split_count = sum([\n      bool(source)\n     \
          \ for source in [predefined_split_key,\n                     any(fraction_splits)]\n\
          \  ])\n  if split_count > 1:\n    raise ValueError(f'Expected 1 split type,\
          \ found {split_count}.')\n  if (predefined_split_key and\n      not column_pattern.fullmatch(predefined_split_key)):\n\
          \    raise ValueError(f'Invalid column name: {predefined_split_key}.')\n\
          \  if any(fraction_splits):\n    if not all(fraction_splits):\n      raise\
          \ ValueError(\n          f'All fractions must be non-zero. Got: {fraction_splits}.')\n\
          \    if sum(fraction_splits) != 1:\n      raise ValueError(\n          f'Fraction\
          \ splits must sum to 1. Got: {sum(fraction_splits)}.')\n  if (timestamp_split_key\
          \ and\n      not column_pattern.fullmatch(timestamp_split_key)):\n    raise\
          \ ValueError(f'Invalid column name: {timestamp_split_key}.')\n  if timestamp_split_key\
          \ and not all(fraction_splits):\n    raise ValueError('All fractions must\
          \ be non-zero for timestamp split.')\n\n  # Validate window config.\n  if\
          \ window_stride_length == -1:\n    window_stride_length = None\n  if window_max_count\
          \ == -1:\n    window_max_count = None\n  window_configs = [window_column,\
          \ window_stride_length, window_max_count]\n  window_config_count = sum([bool(config)\
          \ for config in window_configs])\n  if window_config_count > 1:\n    raise\
          \ ValueError(f'Expected 1 window config, found {window_config_count}.')\n\
          \  if window_column and not column_pattern.fullmatch(window_column):\n \
          \   raise ValueError(f'Invalid column name: {window_column}.')\n  if window_stride_length\
          \ and (window_stride_length < 1 or\n                               window_stride_length\
          \ > 1000):\n    raise ValueError('Stride must be between 1 and 1000. Got:\
          \ '\n                     f'{window_stride_length}.')\n  if window_max_count\
          \ and (window_max_count < 1000 or\n                           window_max_count\
          \ > int(1e8)):\n    raise ValueError('Max count must be between 1000 and\
          \ 100000000. Got: '\n                     f'{window_max_count}.')\n\n  #\
          \ Validate eval metric.\n  valid_optimization_objectives = ['rmse', 'mae',\
          \ 'rmsle']\n  if optimization_objective:\n    if optimization_objective\
          \ not in valid_optimization_objectives:\n      raise ValueError(\n     \
          \     'Optimization objective should be one of the following: '\n      \
          \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
          \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
          \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
          \    if data_granularity_unit not in valid_data_granularity_units:\n   \
          \   raise ValueError(\n          'Granularity unit should be one of the\
          \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240710_0625
pipelineInfo:
  description: Creates a batch prediction using a Prophet model.
  name: prophet-predict
root:
  dag:
    tasks:
      bigquery-delete-dataset-with-prefix:
        cachingOptions: {}
        componentRef:
          name: comp-bigquery-delete-dataset-with-prefix
        dependentTasks:
        - exit-handler-1
        inputs:
          parameters:
            dataset_prefix:
              runtimeValue:
                constant: tmp_{{$.pipeline_job_uuid}}
            delete_contents:
              runtimeValue:
                constant: true
            project:
              componentInputParameter: project
        taskInfo:
          name: delete-tmp-dataset
        triggerPolicy:
          strategy: ALL_UPSTREAM_TASKS_COMPLETED
      exit-handler-1:
        componentRef:
          name: comp-exit-handler-1
        inputs:
          parameters:
            pipelinechannel--bigquery_destination_uri:
              componentInputParameter: bigquery_destination_uri
            pipelinechannel--data_source_bigquery_table_path:
              componentInputParameter: data_source_bigquery_table_path
            pipelinechannel--data_source_csv_filenames:
              componentInputParameter: data_source_csv_filenames
            pipelinechannel--encryption_spec_key_name:
              componentInputParameter: encryption_spec_key_name
            pipelinechannel--location:
              componentInputParameter: location
            pipelinechannel--machine_type:
              componentInputParameter: machine_type
            pipelinechannel--max_num_workers:
              componentInputParameter: max_num_workers
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--project:
              componentInputParameter: project
            pipelinechannel--target_column:
              componentInputParameter: target_column
            pipelinechannel--time_column:
              componentInputParameter: time_column
            pipelinechannel--time_series_identifier_column:
              componentInputParameter: time_series_identifier_column
        taskInfo:
          name: exit-handler-1
  inputDefinitions:
    parameters:
      bigquery_destination_uri:
        defaultValue: ''
        description: 'URI of the desired destination dataset. If not

          specified, resources will be created under a new dataset in the project.

          Unlike in Vertex Forecasting, all resources will be given hardcoded names

          under this dataset, and the model artifact will also be exported here.'
        isOptional: true
        parameterType: STRING
      data_source_bigquery_table_path:
        defaultValue: ''
        description: 'The BigQuery table path of format

          bq://bq_project.bq_dataset.bq_table'
        isOptional: true
        parameterType: STRING
      data_source_csv_filenames:
        defaultValue: ''
        description: 'A string that represents a list of comma

          separated CSV filenames.'
        isOptional: true
        parameterType: STRING
      encryption_spec_key_name:
        defaultValue: ''
        description: The KMS key name.
        isOptional: true
        parameterType: STRING
      location:
        description: The GCP region for Vertex AI.
        parameterType: STRING
      machine_type:
        defaultValue: n1-standard-2
        description: The machine type used for batch prediction.
        isOptional: true
        parameterType: STRING
      max_num_workers:
        defaultValue: 10.0
        description: The max number of workers used for batch prediction.
        isOptional: true
        parameterType: NUMBER_INTEGER
      model_name:
        description: 'The name of the Model resource, in a form of

          projects/{project}/locations/{location}/models/{model}.'
        parameterType: STRING
      project:
        description: The GCP project that runs the pipeline components.
        parameterType: STRING
      target_column:
        description: Name of the column that the model is to predict values for.
        parameterType: STRING
      time_column:
        description: 'Name of the column that identifies time order in the time

          series.'
        parameterType: STRING
      time_series_identifier_column:
        description: 'Name of the column that identifies the time

          series.'
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0-rc.2
