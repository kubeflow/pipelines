name: Model-batch_predict
description: |
    Retrieves a BatchPredictionJob resource and instantiates its representation.

    Args:
        project (str):
            Project to create the BatchPredictionJob.
        location (Optional[str]):
            Optional location for creating the BatchPredictionJob. If not set,
            default to us-central1.
        job_display_name (str):
            Required. The user-defined name of the BatchPredictionJob.
        model (Model):
            Required. The model to be used for the BatchPredictionJob.
        gcs_source (Optional[Sequence[str]]):
            Google Cloud Storage URI(-s) to your instances to run
            batch prediction on. They must match `instances_format`.
            May contain wildcards. For more information on wildcards, see
            https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames.
        bigquery_source (Optional[str]):
            BigQuery URI to a table, up to 2000 characters long. For example:
            `projectId.bqDatasetId.bqTableId`
        instances_format (str):
            Required. The format in which instances are given, must be one
            of "jsonl", "csv", "bigquery", "tf-record", "tf-record-gzip",
            or "file-list". Default is "jsonl" when using `gcs_source`. If a
            `bigquery_source` is provided, this is overriden to "bigquery".
        gcs_destination_prefix (Optional[str]):
            The Google Cloud Storage location of the directory where the
            output is to be written to. In the given directory a new
            directory is created. Its name is
            ``prediction-<model-display-name>-<job-create-time>``, where
            timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
            Inside of it files ``predictions_0001.<extension>``,
            ``predictions_0002.<extension>``, ...,
            ``predictions_N.<extension>`` are created where
            ``<extension>`` depends on chosen ``predictions_format``,
            and N may equal 0001 and depends on the total number of
            successfully predicted instances. If the Model has both
            ``instance`` and ``prediction`` schemata defined then each such
            file contains predictions as per the ``predictions_format``.
            If prediction for any instance failed (partially or
            completely), then an additional ``errors_0001.<extension>``,
            ``errors_0002.<extension>``,..., ``errors_N.<extension>``
            files are created (N depends on total number of failed
            predictions). These files contain the failed instances, as
            per their schema, followed by an additional ``error`` field
            which as value has ```google.rpc.Status`` <Status>`__
            containing only ``code`` and ``message`` fields.
        bigquery_destination_prefix (Optional[str]):
            The BigQuery project location where the output is to be
            written to. In the given project a new dataset is created
            with name
            ``prediction_<model-display-name>_<job-create-time>`` where
            is made BigQuery-dataset-name compatible (for example, most
            special characters become underscores), and timestamp is in
            YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the
            dataset two tables will be created, ``predictions``, and
            ``errors``. If the Model has both ``instance`` and ``prediction``
            schemata defined then the tables have columns as follows:
            The ``predictions`` table contains instances for which the
            prediction succeeded, it has columns as per a concatenation
            of the Model's instance and prediction schemata. The
            ``errors`` table contains rows for which the prediction has
            failed, it has instance columns, as per the instance schema,
            followed by a single "errors" column, which as values has
            ```google.rpc.Status`` <Status>`__ represented as a STRUCT,
            and containing only ``code`` and ``message``.
        predictions_format (str="jsonl"):
            Required. The format in which Vertex AI gives the
            predictions, must be one of "jsonl", "csv", or "bigquery".
            Default is "jsonl" when using `gcs_destination_prefix`. If a
            `bigquery_destination_prefix` is provided, this is overriden to
            "bigquery".
        model_parameters (Optional[Dict]):
            The parameters that govern the predictions. The schema of
            the parameters may be specified via the Model's `parameters_schema_uri`.
        machine_type (Optional[str]):
            The type of machine for running batch prediction on
            dedicated resources. Not specifying machine type will result in
            batch prediction job being run with automatic resources.
        accelerator_type (Optional[str]):
            The type of accelerator(s) that may be attached
            to the machine as per `accelerator_count`. Only used if
            `machine_type` is set.
        accelerator_count (Optional[int]):
            The number of accelerators to attach to the
            `machine_type`. Only used if `machine_type` is set.
        starting_replica_count (Optional[int]):
            The number of machine replicas used at the start of the batch
            operation. If not set, Vertex AI decides starting number, not
            greater than `max_replica_count`. Only used if `machine_type` is
            set.
        max_replica_count (Optional[int]):
            The maximum number of machine replicas the batch operation may
            be scaled to. Only used if `machine_type` is set.
            Default is 10.
        generate_explanation (bool):
            Optional. Generate explanation along with the batch prediction
            results. This will cause the batch prediction output to include
            explanations based on the `prediction_format`:
                - `bigquery`: output includes a column named `explanation`. The value
                    is a struct that conforms to the [aiplatform.gapic.Explanation] object.
                - `jsonl`: The JSON objects on each line include an additional entry
                    keyed `explanation`. The value of the entry is a JSON object that
                    conforms to the [aiplatform.gapic.Explanation] object.
                - `csv`: Generating explanations for CSV format is not supported.
        explanation_metadata (aiplatform.explain.ExplanationMetadata):
            Optional. Explanation metadata configuration for this BatchPredictionJob.
            Can be specified only if `generate_explanation` is set to `True`.

            This value overrides the value of `Model.explanation_metadata`.
            All fields of `explanation_metadata` are optional in the request. If
            a field of the `explanation_metadata` object is not populated, the
            corresponding field of the `Model.explanation_metadata` object is inherited.
            For more details, see `Ref docs <http://tinyurl.com/1igh60kt>`
        explanation_parameters (aiplatform.explain.ExplanationParameters):
            Optional. Parameters to configure explaining for Model's predictions.
            Can be specified only if `generate_explanation` is set to `True`.

            This value overrides the value of `Model.explanation_parameters`.
            All fields of `explanation_parameters` are optional in the request. If
            a field of the `explanation_parameters` object is not populated, the
            corresponding field of the `Model.explanation_parameters` object is inherited.
            For more details, see `Ref docs <http://tinyurl.com/1an4zake>`
        labels (Optional[dict]):
            The labels with user-defined metadata to organize your
            BatchPredictionJobs. Label keys and values can be no longer than
            64 characters (Unicode codepoints), can only contain lowercase
            letters, numeric characters, underscores and dashes.
            International characters are allowed. See https://goo.gl/xmQnxf
            for more information and examples of labels.
        encryption_spec_key_name (Optional[str]):
            Optional. The Cloud KMS resource identifier of the customer
            managed encryption key used to protect the job. Has the
            form:
            ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
            The key needs to be in the same region as where the compute
            resource is created.

            If this is set, then all
            resources created by the BatchPredictionJob will
            be encrypted with the provided encryption key.

            Overrides encryption_spec_key_name set in aiplatform.init.

    Returns:
        Instantiated representation of the created batch prediction job.
inputs:
- {name: project, type: String}
- {name: location, type: String, default: "us-central1"}
- {name: job_display_name, type: String}
- {name: model, type: Model}
- {name: gcs_source, type: JsonArray, optional: true}
- {name: bigquery_source, type: String, optional: true}
- {name: instances_format, type: String, default: "jsonl"}
- {name: gcs_destination_prefix, type: String, optional: true}
- {name: bigquery_destination_prefix, type: String, optional: true}
- {name: predictions_format, type: String, default: "jsonl"}
- {name: model_parameters, type: JsonArray, optional: true}
- {name: machine_type, type: String, optional: true}
- {name: accelerator_type, type: String, optional: true}
- {name: accelerator_count, type: Integer, optional: true}
- {name: starting_replica_count, type: Integer, optional: true}
- {name: max_replica_count, type: Integer, optional: true}
- {name: generate_explanation, type: Boolean, optional: true}
- {name: explanation_metadata, type: JsonObject, optional: true}
- {name: explanation_parameters, type: JsonObject, optional: true}
- {name: labels, type: JsonArray, optional: true}
- {name: encryption_spec_key_name, type: String, optional: true}
outputs:
- {name: batchpredictionjob, type: Artifact}
implementation:
  container:
    image: gcr.io/ml-pipeline/google-cloud-pipeline-components:latest
    command: [python3, -m, google_cloud_pipeline_components.remote.aiplatform.remote_runner,
      --cls_name, Model, --method_name, batch_predict]
    args:
    - --init.project
    - {inputValue: project}
    - --init.location
    - {inputValue: location}
    - --init.model_name
    - {inputUri: model}
    - --method.job_display_name
    - {inputValue: job_display_name}
    - if:
        cond: {isPresent: gcs_source}
        then:
        - --method.gcs_source
        - {inputValue: gcs_source}
    - if:
        cond: {isPresent: bigquery_source}
        then:
        - --method.bigquery_source
        - {inputValue: bigquery_source}
    - --method.instances_format
    - {inputValue: instances_format}
    - if:
        cond: {isPresent: gcs_destination_prefix}
        then:
        - --method.gcs_destination_prefix
        - {inputValue: gcs_destination_prefix}
    - if:
        cond: {isPresent: bigquery_destination_prefix}
        then:
        - --method.bigquery_destination_prefix
        - {inputValue: bigquery_destination_prefix}
    - --method.predictions_format
    - {inputValue: predictions_format}
    - if:
        cond: {isPresent: model_parameters}
        then:
        - --method.model_parameters
        - {inputValue: model_parameters}
    - if:
        cond: {isPresent: machine_type}
        then:
        - --method.machine_type
        - {inputValue: machine_type}
    - if:
        cond: {isPresent: accelerator_type}
        then:
        - --method.accelerator_type
        - {inputValue: accelerator_type}
    - if:
        cond: {isPresent: accelerator_count}
        then:
        - --method.accelerator_count
        - {inputValue: accelerator_count}
    - if:
        cond: {isPresent: starting_replica_count}
        then:
        - --method.starting_replica_count
        - {inputValue: starting_replica_count}
    - if:
        cond: {isPresent: max_replica_count}
        then:
        - --method.max_replica_count
        - {inputValue: max_replica_count}
    - if:
        cond: {isPresent: generate_explanation}
        then:
        - --method.generate_explanation
        - {inputValue: generate_explanation}
    - if:
        cond: {isPresent: explanation_metadata}
        then:
        - --method.explanation_metadata
        - {inputValue: explanation_metadata}
    - if:
        cond: {isPresent: explanation_parameters}
        then:
        - --method.explanation_parameters
        - {inputValue: explanation_parameters}
    - if:
        cond: {isPresent: labels}
        then:
        - --method.labels
        - {inputValue: labels}
    - if:
        cond: {isPresent: encryption_spec_key_name}
        then:
        - --method.encryption_spec_key_name
        - {inputValue: encryption_spec_key_name}
    - --executor_input
    - '{{$}}'
    - --resource_name_output_artifact_uri
    - {outputUri: batchpredictionjob}