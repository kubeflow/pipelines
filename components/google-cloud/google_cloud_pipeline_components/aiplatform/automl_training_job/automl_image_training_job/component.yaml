name: automl_image_training_job
description: |
    Runs the AutoML Image training job and returns a model.
    If training on a Vertex AI dataset, you can use one of the following split configurations:
        Data fraction splits:
        Any of ``training_fraction_split``, ``validation_fraction_split`` and
        ``test_fraction_split`` may optionally be provided, they must sum to up to 1. If
        the provided ones sum to less than 1, the remainder is assigned to sets as
        decided by Vertex AI. If none of the fractions are set, by default roughly 80%
        of data will be used for training, 10% for validation, and 10% for test.
        Data filter splits:
        Assigns input data to training, validation, and test sets
        based on the given filters, data pieces not matched by any
        filter are ignored. Currently only supported for Datasets
        containing DataItems.
        If any of the filters in this message are to match nothing, then
        they can be set as '-' (the minus sign).
        Supported only for unstructured Datasets.
    Args:
        dataset (datasets.ImageDataset):
            Required. The dataset within the same Project from which data will be used to train the Model. The
            Dataset must use schema compatible with Model being trained,
            and what is compatible should be described in the used
            TrainingPipeline's [training_task_definition]
            [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
            For tabular Datasets, all their data is exported to
            training, to pick and choose from.
        training_fraction_split (Float):
            Optional. The fraction of the input data that is to be used to train
            the Model. This is ignored if Dataset is not provided.
        validation_fraction_split (Float):
            Optional. The fraction of the input data that is to be used to validate
            the Model. This is ignored if Dataset is not provided.
        test_fraction_split (Float):
            Optional. The fraction of the input data that is to be used to evaluate
            the Model. This is ignored if Dataset is not provided.
        budget_milli_node_hours (Integer):
            Optional. The train budget of creating this Model, expressed in milli node
            hours i.e. 1,000 value in this field means 1 node hour.
            Defaults by `prediction_type`:
                `classification` - For Cloud models the budget must be: 8,000 - 800,000
                milli node hours (inclusive). The default value is 192,000 which
                represents one day in wall time, assuming 8 nodes are used.
                `object_detection` - For Cloud models the budget must be: 20,000 - 900,000
                milli node hours (inclusive). The default value is 216,000 which represents
                one day in wall time, assuming 9 nodes are used.
            The training cost of the model will not exceed this budget. The final
            cost will be attempted to be close to the budget, though may end up
            being (even) noticeably smaller - at the backend's discretion. This
            especially may happen when further model training ceases to provide
            any improvements. If the budget is set to a value known to be insufficient to
            train a Model for the given training set, the training won't be attempted and
            will error.
        model_display_name (String):
            Optional. The display name of the managed Vertex AI Model. The name
            can be up to 128 characters long and can be consist of any UTF-8
            characters. If not provided upon creation, the job's display_name is used.
        model_labels (JsonObject):
            Optional. The labels with user-defined metadata to
            organize your Models.
            Label keys and values can be no longer than 64
            characters (Unicode codepoints), can only
            contain lowercase letters, numeric characters,
            underscores and dashes. International characters
            are allowed.
            See https://goo.gl/xmQnxf for more information
            and examples of labels.
        disable_early_stopping: bool = False
            Required. If true, the entire budget is used. This disables the early stopping
            feature. By default, the early stopping feature is enabled, which means
            that training might stop before the entire training budget has been
            used, if further training does no longer brings significant improvement
            to the model.
        display_name (String):
            Required. The user-defined name of this TrainingPipeline.
        prediction_type (String):
            The type of prediction the Model is to produce, one of:
                "classification" - Predict one out of multiple target values is
                    picked for each row.
                "object_detection" - Predict a value based on its relation to other values.
                    This type is available only to columns that contain
                    semantically numeric values, i.e. integers or floating
                    point number, even if stored as e.g. strings.
        multi_label: bool = False
            Required. Default is False.
            If false, a single-label (multi-class) Model will be trained
            (i.e. assuming that for each image just up to one annotation may be
            applicable). If true, a multi-label Model will be trained (i.e.
            assuming that for each image multiple annotations may be applicable).
            This is only applicable for the "classification" prediction_type and
            will be ignored otherwise.
        model_type: str = "CLOUD"
            Required. One of the following:
                "CLOUD" - Default for Image Classification.
                    A Model best tailored to be used within Google Cloud, and
                    which cannot be exported.
                "CLOUD_HIGH_ACCURACY_1" - Default for Image Object Detection.
                    A model best tailored to be used within Google Cloud, and
                    which cannot be exported. Expected to have a higher latency,
                    but should also have a higher prediction quality than other
                    cloud models.
                "CLOUD_LOW_LATENCY_1" - A model best tailored to be used within
                    Google Cloud, and which cannot be exported. Expected to have a
                    low latency, but may have lower prediction quality than other
                    cloud models.
                "MOBILE_TF_LOW_LATENCY_1" - A model that, in addition to being
                    available within Google Cloud, can also be exported as TensorFlow
                    or Core ML model and used on a mobile or edge device afterwards.
                    Expected to have low latency, but may have lower prediction
                    quality than other mobile models.
                "MOBILE_TF_VERSATILE_1" - A model that, in addition to being
                    available within Google Cloud, can also be exported as TensorFlow
                    or Core ML model and used on a mobile or edge device with afterwards.
                "MOBILE_TF_HIGH_ACCURACY_1" - A model that, in addition to being
                    available within Google Cloud, can also be exported as TensorFlow
                    or Core ML model and used on a mobile or edge device afterwards.
                    Expected to have a higher latency, but should also have a higher
                    prediction quality than other mobile models.
        base_model: Optional[models.Model] = None
            Optional. Only permitted for Image Classification models.
            If it is specified, the new model will be trained based on the `base` model.
            Otherwise, the new model will be trained from scratch. The `base` model
            must be in the same Project and Location as the new Model to train,
            and have the same model_type.
        project (String):
            Required. project to retrieve dataset from.
        location (String):
            Optional location to retrieve dataset from.
        labels (JsonObject):
            Optional. The labels with user-defined metadata to
            organize TrainingPipelines.
            Label keys and values can be no longer than 64
            characters (Unicode codepoints), can only
            contain lowercase letters, numeric characters,
            underscores and dashes. International characters
            are allowed.
            See https://goo.gl/xmQnxf for more information
            and examples of labels.
        training_encryption_spec_key_name (Optional[String]):
            Optional. The Cloud KMS resource identifier of the customer
            managed encryption key used to protect the training pipeline. Has the
            form:
            ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
            The key needs to be in the same region as where the compute
            resource is created.
            If set, this TrainingPipeline will be secured by this key.
            Note: Model trained by this TrainingPipeline is also secured
            by this key if ``model_to_upload`` is not set separately.
            Overrides encryption_spec_key_name set in aiplatform.init.
        model_encryption_spec_key_name (Optional[String]):
            Optional. The Cloud KMS resource identifier of the customer
            managed encryption key used to protect the model. Has the
            form:
            ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
            The key needs to be in the same region as where the compute
            resource is created.
            If set, the trained Model will be secured by this key.
            Overrides encryption_spec_key_name set in aiplatform.init.
    Returns:
        model: The trained Vertex AI Model resource or None if training did not
            produce a Vertex AI Model.
inputs:
- {name: project, type: String}
- {name: location, type: String, default: "us-central1"}
- {name: display_name, type: String}
- {name: dataset, type: google.VertexDataset}
- {name: prediction_type, type: String, optional: true, default: 'classification'}
- {name: multi_label, type: Boolean, optional: true, default: False}
- {name: model_type, type: String, optional: true, default: 'CLOUD'}
- {name: base_model, type: google.VertexModel, optional: true}
- {name: labels, type: JsonObject, optional: true, default: '{}'}
- {name: training_encryption_spec_key_name, type: String, optional: true}
- {name: model_encryption_spec_key_name, type: String, optional: true}
- {name: training_fraction_split, type: Float, optional: true}
- {name: validation_fraction_split, type: Float, optional: true}
- {name: test_fraction_split, type: Float, optional: true}
- {name: budget_milli_node_hours, type: Integer, optional: true}
- {name: model_display_name, type: String, optional: true}
- {name: model_labels, type: JsonObject, optional: true}
- {name: disable_early_stopping, type: Boolean, optional: true, default: False}
outputs:
- {name: model, type: google.VertexModel}
implementation:
  container:
    image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.28
    command: [python3, -m, google_cloud_pipeline_components.container.aiplatform.remote_runner,
      --cls_name, AutoMLImageTrainingJob, --method_name, run]
    args:
    - --init.project
    - {inputValue: project}
    - --init.location
    - {inputValue: location}
    - --init.display_name
    - {inputValue: display_name}
    - --init.prediction_type
    - {inputValue: prediction_type}
    - --init.multi_label
    - {inputValue: multi_label}
    - --init.model_type
    - {inputValue: model_type}
    - --init.labels
    - {inputValue: labels}
    - --method.dataset
    - "{{$.inputs.artifacts['dataset'].metadata['resourceName']}}"
    - --method.disable_early_stopping
    - {inputValue: disable_early_stopping}
    - if:
        cond: {isPresent: training_encryption_spec_key_name}
        then:
        - --init.training_encryption_spec_key_name
        - {inputValue: training_encryption_spec_key_name}
    - if:
        cond: {isPresent: model_encryption_spec_key_name}
        then:
        - --init.model_encryption_spec_key_name
        - {inputValue: model_encryption_spec_key_name}
    - if:
        cond: {isPresent: model_display_name}
        then:
        - --method.model_display_name
        - {inputValue: model_display_name}
    - if:
        cond: {isPresent: training_fraction_split}
        then:
        - --method.training_fraction_split
        - {inputValue: training_fraction_split}
    - if:
        cond: {isPresent: validation_fraction_split}
        then:
        - --method.validation_fraction_split
        - {inputValue: validation_fraction_split}
    - if:
        cond: {isPresent: test_fraction_split}
        then:
        - --method.test_fraction_split
        - {inputValue: test_fraction_split}
    - if:
        cond: {isPresent: budget_milli_node_hours}
        then:
        - --method.budget_milli_node_hours
        - {inputValue: budget_milli_node_hours}
    - if:
        cond: {isPresent: base_model}
        then:
        - --init.base_model
        - "{{$.inputs.artifacts['base_model'].metadata['resourceName']}}"
    - if:
        cond: {isPresent: model_labels}
        then:
        - --method.model_labels
        - {inputValue: model_labels}
    - --executor_input
    - '{{$}}'
    - --resource_name_output_artifact_uri
    - {outputUri: model}
