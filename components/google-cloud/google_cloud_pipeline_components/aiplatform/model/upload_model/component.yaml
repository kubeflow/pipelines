name: Model-upload
description: |
    Uploads a model and returns a Model representing the uploaded Model resource.

    Args:
        project (str):
            Project to upload this model to.
        location (Optional[str]):
            Optional location to upload this model to. If not set,
            default to us-central1.
        display_name (str):
            Required. The display name of the Model. The name can be up to 128
            characters long and can be consist of any UTF-8 characters.
        description (str):
            Optional. The description of the model.
        serving_container_image_uri (str):
            Required. The URI of the Model serving container.
        serving_container_command (Optional[Sequence[str]]=None):
            The command with which the container is run. Not executed within a
            shell. The Docker image's ENTRYPOINT is used if this is not provided.
            Variable references $(VAR_NAME) are expanded using the container's
            environment. If a variable cannot be resolved, the reference in the
            input string will be unchanged. The $(VAR_NAME) syntax can be escaped
            with a double $$, ie: $$(VAR_NAME). Escaped references will never be
            expanded, regardless of whether the variable exists or not.
        serving_container_args (Optional[Sequence[str]]=None):
            The arguments to the command. The Docker image's CMD is used if this is
            not provided. Variable references $(VAR_NAME) are expanded using the
            container's environment. If a variable cannot be resolved, the reference
            in the input string will be unchanged. The $(VAR_NAME) syntax can be
            escaped with a double $$, ie: $$(VAR_NAME). Escaped references will
            never be expanded, regardless of whether the variable exists or not.
        serving_container_environment_variables (Optional[Dict[str, str]]=None):
            The environment variables that are to be present in the container.
            Should be a dictionary where keys are environment variable names
            and values are environment variable values for those names.
        serving_container_ports (Optional[Sequence[int]]=None):
            Declaration of ports that are exposed by the container. This field is
            primarily informational, it gives Vertex AI information about the
            network connections the container uses. Listing or not a port here has
            no impact on whether the port is actually exposed, any port listening on
            the default "0.0.0.0" address inside a container will be accessible from
            the network.
        serving_container_predict_route (str):
            Optional. An HTTP path to send prediction requests to the container, and
            which must be supported by it. If not specified a default HTTP path will
            be used by Vertex AI.
        serving_container_health_route (str):
            Optional. An HTTP path to send health check requests to the container, and which
            must be supported by it. If not specified a standard HTTP path will be
            used by Vertex AI.
        instance_schema_uri (str):
            Optional. Points to a YAML file stored on Google Cloud
            Storage describing the format of a single instance, which
            are used in
            ``PredictRequest.instances``,
            ``ExplainRequest.instances``
            and
            ``BatchPredictionJob.input_config``.
            The schema is defined as an OpenAPI 3.0.2 `Schema
            Object <https://tinyurl.com/y538mdwt#schema-object>`__.
            AutoML Models always have this field populated by AI
            Platform. Note: The URI given on output will be immutable
            and probably different, including the URI scheme, than the
            one given on input. The output URI will point to a location
            where the user only has a read access.
        parameters_schema_uri (str):
            Optional. Points to a YAML file stored on Google Cloud
            Storage describing the parameters of prediction and
            explanation via
            ``PredictRequest.parameters``,
            ``ExplainRequest.parameters``
            and
            ``BatchPredictionJob.model_parameters``.
            The schema is defined as an OpenAPI 3.0.2 `Schema
            Object <https://tinyurl.com/y538mdwt#schema-object>`__.
            AutoML Models always have this field populated by AI
            Platform, if no parameters are supported it is set to an
            empty string. Note: The URI given on output will be
            immutable and probably different, including the URI scheme,
            than the one given on input. The output URI will point to a
            location where the user only has a read access.
        prediction_schema_uri (str):
            Optional. Points to a YAML file stored on Google Cloud
            Storage describing the format of a single prediction
            produced by this Model, which are returned via
            ``PredictResponse.predictions``,
            ``ExplainResponse.explanations``,
            and
            ``BatchPredictionJob.output_config``.
            The schema is defined as an OpenAPI 3.0.2 `Schema
            Object <https://tinyurl.com/y538mdwt#schema-object>`__.
            AutoML Models always have this field populated by AI
            Platform. Note: The URI given on output will be immutable
            and probably different, including the URI scheme, than the
            one given on input. The output URI will point to a location
            where the user only has a read access.
        artifact_uri (str):
            Optional. The path to the directory containing the Model artifact and
            any of its supporting files. Leave blank for custom container prediction.
            Not present for AutoML Models.
        explanation_metadata (explain.ExplanationMetadata):
            Optional. Metadata describing the Model's input and output for explanation.
            Both `explanation_metadata` and `explanation_parameters` must be
            passed together when used. For more details, see
            `Ref docs <http://tinyurl.com/1igh60kt>`
        explanation_parameters (explain.ExplanationParameters):
            Optional. Parameters to configure explaining for Model's predictions.
            For more details, see `Ref docs <http://tinyurl.com/1an4zake>`
        encryption_spec_key_name (Optional[str]):
            Optional. The Cloud KMS resource identifier of the customer
            managed encryption key used to protect the model. Has the
            form:
            ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
            The key needs to be in the same region as where the compute
            resource is created.

            If set, this Model and all sub-resources of this Model will be secured by this key.

            Overrides encryption_spec_key_name set in aiplatform.init.
    Returns:
        model: Instantiated representation of the uploaded model resource.
inputs:
- {name: project, type: String} 
- {name: location, type: String, default: "us-central1"} 
- {name: display_name, type: String}
- {name: description, type: String, optional: true}
- {name: serving_container_image_uri, type: String}
- {name: serving_container_command, type: JsonArray, optional: true}
- {name: serving_container_args, type: JsonArray, optional: true}
- {name: serving_container_environment_variables, type: JsonArray, optional: true}
- {name: serving_container_ports, type: JsonArray, optional: true}
- {name: serving_container_predict_route, type: String, optional: true}
- {name: serving_container_health_route, type: String, optional: true}
- {name: instance_schema_uri, type: String, optional: true}
- {name: parameters_schema_uri, type: String, optional: true}
- {name: prediction_schema_uri, type: String, optional: true}
- {name: artifact_uri, type: String, optional: true}
- {name: explanation_metadata, type: JsonObject, optional: true}
- {name: explanation_parameters, type: JsonObject, optional: true}
- {name: encryption_spec_key_name, type: String, optional: true}
outputs:
- {name: model, type: Model}
implementation:
  container:
    image: gcr.io/ml-pipeline/google-cloud-pipeline-components:latest
    command: [python3, -m, google_cloud_pipeline_components.remote.aiplatform.remote_runner,
      --cls_name, Model, --method_name, upload]
    args:
    - --method.project
    - {inputValue: project}
    - --method.location
    - {inputValue: location}
    - --method.display_name
    - {inputValue: display_name}
    - if:
        cond: {isPresent: description}
        then:
        - --method.description
        - {inputValue: description}
    - --method.serving_container_image_uri
    - {inputValue: serving_container_image_uri}
    - if:
        cond: {isPresent: serving_container_predict_route}
        then:
        - --method.serving_container_predict_route
        - {inputValue: serving_container_predict_route}
    - if:
        cond: {isPresent: serving_container_health_route}
        then:
        - --method.serving_container_health_route
        - {inputValue: serving_container_health_route}
    - if:
        cond: {isPresent: instance_schema_uri}
        then:
        - --method.instance_schema_uri
        - {inputValue: instance_schema_uri}
    - if:
        cond: {isPresent: parameters_schema_uri}
        then:
        - --method.parameters_schema_uri
        - {inputValue: parameters_schema_uri}
    - if:
        cond: {isPresent: prediction_schema_uri}
        then:
        - --method.prediction_schema_uri
        - {inputValue: prediction_schema_uri}
    - if:
        cond: {isPresent: artifact_uri}
        then:
        - --method.artifact_uri
        - {inputValue: artifact_uri}
    - if:
        cond: {isPresent: explanation_metadata}
        then:
        - --method.explanation_metadata
        - {inputValue: explanation_metadata}
    - if:
        cond: {isPresent: explanation_parameters}
        then:
        - --method.explanation_parameters
        - {inputValue: explanation_parameters}
    - if:
        cond: {isPresent: encryption_spec_key_name}
        then:
        - --method.encryption_spec_key_name
        - {inputValue: encryption_spec_key_name}
    - if:
        cond: {isPresent: serving_container_command}
        then:
        - --method.serving_container_command
        - {inputValue: serving_container_command}
    - if:
        cond: {isPresent: serving_container_args}
        then:
        - --method.serving_container_args
        - {inputValue: serving_container_args}
    - if:
        cond: {isPresent: serving_container_environment_variables}
        then:
        - --method.serving_container_environment_variables
        - {inputValue: serving_container_environment_variables}
    - if:
        cond: {isPresent: serving_container_ports}
        then:
        - --method.serving_container_ports
        - {inputValue: serving_container_ports}
    - --executor_input
    - '{{$}}'
    - --resource_name_output_artifact_uri
    - {outputUri: model}
