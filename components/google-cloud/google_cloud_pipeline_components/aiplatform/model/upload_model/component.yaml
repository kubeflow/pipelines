name: Model-upload
description: |
    Uploads a model and returns a Model representing the uploaded Model resource.

    Args:
        project (str):
            Project to upload this model to.
        location (Optional[str]):
            Optional location to upload this model to. If not set,
            default to us-central1.
        display_name (str):
            Required. The display name of the Model. The name can be up to 128
            characters long and can be consist of any UTF-8 characters.
        description (str):
            Optional. The description of the model.
        serving_container_image_uri (str):
            Required. The URI of the Model serving container.
        serving_container_command (Optional[Sequence[str]]=None):
            The command with which the container is run. Not executed within a
            shell. The Docker image's ENTRYPOINT is used if this is not provided.
            Variable references $(VAR_NAME) are expanded using the container's
            environment. If a variable cannot be resolved, the reference in the
            input string will be unchanged. The $(VAR_NAME) syntax can be escaped
            with a double $$, ie: $$(VAR_NAME). Escaped references will never be
            expanded, regardless of whether the variable exists or not.
        serving_container_args (Optional[Sequence[str]]=None):
            The arguments to the command. The Docker image's CMD is used if this is
            not provided. Variable references $(VAR_NAME) are expanded using the
            container's environment. If a variable cannot be resolved, the reference
            in the input string will be unchanged. The $(VAR_NAME) syntax can be
            escaped with a double $$, ie: $$(VAR_NAME). Escaped references will
            never be expanded, regardless of whether the variable exists or not.
        serving_container_environment_variables (Optional[Dict[str, str]]=None):
            The environment variables that are to be present in the container.
            Should be a dictionary where keys are environment variable names
            and values are environment variable values for those names.
        serving_container_ports (Optional[Sequence[int]]=None):
            Declaration of ports that are exposed by the container. This field is
            primarily informational, it gives Vertex AI information about the
            network connections the container uses. Listing or not a port here has
            no impact on whether the port is actually exposed, any port listening on
            the default "0.0.0.0" address inside a container will be accessible from
            the network.
        serving_container_predict_route (str):
            Optional. An HTTP path to send prediction requests to the container, and
            which must be supported by it. If not specified a default HTTP path will
            be used by Vertex AI.
        serving_container_health_route (str):
            Optional. An HTTP path to send health check requests to the container, and which
            must be supported by it. If not specified a standard HTTP path will be
            used by Vertex AI.
        instance_schema_uri (str):
            Optional. Points to a YAML file stored on Google Cloud
            Storage describing the format of a single instance, which
            are used in
            ``PredictRequest.instances``,
            ``ExplainRequest.instances``
            and
            ``BatchPredictionJob.input_config``.
            The schema is defined as an OpenAPI 3.0.2 `Schema
            Object <https://tinyurl.com/y538mdwt#schema-object>`__.
            AutoML Models always have this field populated by AI
            Platform. Note: The URI given on output will be immutable
            and probably different, including the URI scheme, than the
            one given on input. The output URI will point to a location
            where the user only has a read access.
        parameters_schema_uri (str):
            Optional. Points to a YAML file stored on Google Cloud
            Storage describing the parameters of prediction and
            explanation via
            ``PredictRequest.parameters``,
            ``ExplainRequest.parameters``
            and
            ``BatchPredictionJob.model_parameters``.
            The schema is defined as an OpenAPI 3.0.2 `Schema
            Object <https://tinyurl.com/y538mdwt#schema-object>`__.
            AutoML Models always have this field populated by AI
            Platform, if no parameters are supported it is set to an
            empty string. Note: The URI given on output will be
            immutable and probably different, including the URI scheme,
            than the one given on input. The output URI will point to a
            location where the user only has a read access.
        prediction_schema_uri (str):
            Optional. Points to a YAML file stored on Google Cloud
            Storage describing the format of a single prediction
            produced by this Model, which are returned via
            ``PredictResponse.predictions``,
            ``ExplainResponse.explanations``,
            and
            ``BatchPredictionJob.output_config``.
            The schema is defined as an OpenAPI 3.0.2 `Schema
            Object <https://tinyurl.com/y538mdwt#schema-object>`__.
            AutoML Models always have this field populated by AI
            Platform. Note: The URI given on output will be immutable
            and probably different, including the URI scheme, than the
            one given on input. The output URI will point to a location
            where the user only has a read access.
        artifact_uri (str):
            Optional. The path to the directory containing the Model artifact and
            any of its supporting files. Leave blank for custom container prediction.
            Not present for AutoML Models.
        explanation_metadata (explain.ExplanationMetadata):
            Optional. Metadata describing the Model's input and output for explanation.
            Both `explanation_metadata` and `explanation_parameters` must be
            passed together when used. For more details, see
            `Ref docs <http://tinyurl.com/1igh60kt>`
        explanation_parameters (explain.ExplanationParameters):
            Optional. Parameters to configure explaining for Model's predictions.
            For more details, see `Ref docs <http://tinyurl.com/1an4zake>`
        encryption_spec_key_name (Optional[str]):
            Optional. The Cloud KMS resource identifier of the customer
            managed encryption key used to protect the model. Has the
            form:
            ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
            The key needs to be in the same region as where the compute
            resource is created.

            If set, this Model and all sub-resources of this Model will be secured by this key.

            Overrides encryption_spec_key_name set in aiplatform.init.
    Returns:
        model: Instantiated representation of the uploaded model resource.
inputs:
- {name: project, type: String} 
- {name: location, type: String, default: "us-central1"} 
- {name: display_name, type: String}
- {name: description, type: String, optional: true}
- {name: serving_container_image_uri, type: String}
- {name: serving_container_command, type: JsonArray, optional: true, default: '{}'}
- {name: serving_container_args, type: JsonArray, optional: true, default: '{}'}
- {name: serving_container_environment_variables, type: JsonArray, optional: true, default: '{}'}
- {name: serving_container_ports, type: JsonArray, optional: true, default: '{}'}
- {name: serving_container_predict_route, type: String, optional: true}
- {name: serving_container_health_route, type: String, optional: true}
- {name: instance_schema_uri, type: String, optional: true}
- {name: parameters_schema_uri, type: String, optional: true}
- {name: prediction_schema_uri, type: String, optional: true}
- {name: artifact_uri, type: String, optional: true}
- {name: explanation_metadata, type: JsonObject, optional: true, default: '{}'}
- {name: explanation_parameters, type: JsonObject, optional: true, default: '{}'}
- {name: encryption_spec_key_name, type: String, optional: true}
- {name: labels, type: JsonArray, optional: true, default: '{}'}
outputs:
- {name: model, type: Model}
- {name: gcp_resources, type: String}
implementation:
  container:
    image: gcr.io/ml-pipeline/google-cloud-pipeline-components:latest
    command: [python3, -u, -m, google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher]
    args: [
      --type, UploadModel,
      --payload, 
      concat: [
          '{',
          '"display_name": "', {inputValue: display_name}, '"',
          ', "description": "', {inputValue: description}, '"',
          ', "predict_schemata": {',
          '"instance_schema_uri": "',{inputValue: instance_schema_uri}, '"',
          ', "parameters_schema_uri": "', {inputValue: parameters_schema_uri}, '"',
          ', "prediction_schema_uri": "', {inputValue: prediction_schema_uri}, '"',
          '}',
          ', "container_spec": {',
          '"image_uri": "', {inputValue: serving_container_image_uri}, '"',
          ', "command": ', {inputValue: serving_container_command},
          ', "args": ', {inputValue: serving_container_args},
          ', "env": ', {inputValue: serving_container_environment_variables},
          ', "ports": ', {inputValue: serving_container_ports},
          ', "predict_route": "', {inputValue: serving_container_predict_route}, '"',
          ', "health_route": "', {inputValue: serving_container_health_route}, '"',
          '}',
          ', "artifact_uri": "', {inputValue: artifact_uri}, '"',
          ', "explanation_spec": {',
          '"parameters": ', {inputValue: explanation_parameters},
          ', "metadata": ', {inputValue: explanation_metadata},
          '}',
          ', "encryption_spec": {"kms_key_name":"', {inputValue: encryption_spec_key_name}, '"}',
          ', "labels": ', {inputValue: labels},
          '}'
      ],
      --project, {inputValue: project},
      --location, {inputValue: location},
      --gcp_resources, {outputPath: gcp_resources},
      --executor_input, "{{$}}",
    ]
