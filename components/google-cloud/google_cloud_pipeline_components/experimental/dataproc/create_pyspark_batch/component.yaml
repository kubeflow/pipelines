# Copyright 2021 The Kubeflow Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: dataproc_create_pyspark_batch
description: |
  Create a Dataproc PySpark batch workload and wait for it to finish.

    Args:
        project (str):
          Required: Project to create the Dataproc Spark batch workload in.
        location (str):
          Location to create the Spark batch workload. If not set,
          default to `us-central1`.
        batch_id (str):
          The ID to use for the batch, which will become the final component of the batch's resource name.
        labels (Optional[str]):
          The labels to associate with this batch.
        runtime_config_properties (Optional[str]):
          Runtime configuration for a workload.
        service_account (Optional[str]):
          Service account that used to execute workload.
        service_account_scopes (List[Optional[str]]):
          Scopes for the workload service account.
        network_tags (Optional[str]):
          Tags used for network traffic control.
        kms_key (Optional[str]):
          The Cloud KMS key to use for encryption.
        network_uri (Optional[str]]):
          Network URI to connect workload to.
        subnetwork_uri (Optional[str]]):
          Subnetwork URI to connect workload to.
        metastore_service (Optional[str]]):
          Resource name of an existing Dataproc Metastore service.
        spark_history_dataproc_cluster (Optional[str]]):
          The Spark History Server configuration for the workload.
        main_python_file_uri (Optional[str]]):
          The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
        python_file_uris (List[Optional[str]]):
          HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        file_uris (List[Optional[str]]):
          HCFS URIs of files to be placed in the working directory of each executor.
        archive_uris (List[Optional[str]]):
          HCFS URIs of archives to be extracted into the working directory of each executor.
        args (List[Optional[str]]):
          The arguments to pass to the driver.

    Returns:
        gcp_resources (str):
          Serialized gcp_resources proto tracking the PySpark batch workload.
          For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
inputs:
- {name: project, type: String}
- {name: location, type: String, default: "us-central1"}
- {name: batch_id, type: String}
- {name: labels, type: JsonObject, optional: true, default: '{}'}
- {name: runtime_config_properties, type: JsonObject, optional: true, default: '{}'}
- {name: service_account, type: String, optional: true, default: ''}
- {name: service_account_scopes, type: JsonArray, optional: true, default: '[]'}
- {name: network_tags, type: JsonArray, optional: true, default: '[]'}
- {name: kms_key, type: String, optional: true, default: ''}
- {name: network_uri, type: String, optional: true, default: ''}
- {name: subnetwork_uri, type: String, optional: true, default: ''}
- {name: metastore_service, type: String, optional: true, default: ''}
- {name: spark_history_dataproc_cluster, type: String, optional: true, default: ''}
- {name: main_python_file_uri, type: String, optional: true, default: ''}
- {name: python_file_uris,type: JsonArray, optional: true, default: '[]'}
- {name: jar_file_uris, type: JsonArray, optional: true, default: '[]'}
- {name: file_uris, type: JsonArray, optional: true, default: '[]'}
- {name: archive_uris, type: JsonArray, optional: true, default: '[]'}
- {name: args, type: JsonArray, optional: true, default: '[]'}
outputs:
- {name: gcp_resources, type: String}
implementation:
  container:
    image: gcr.io/ml-pipeline/google-cloud-pipeline-components:latest
    command: [python3, -u, -m, google_cloud_pipeline_components.container.v1.gcp_launcher.launcher]
    args: [
      --type, DataprocPySparkBatch,
      --payload,
      concat: [
        '{',
          '"labels": ', {inputValue: labels},
          ', "runtime_config": {',
            '"properties": ', {inputValue: runtime_config_properties},
          '}',
          ', "environment_config": {',
            '"execution_config": {',
              '"service_account": "', {inputValue: service_account}, '"',
              ', "service_account_scopes": ', {inputValue: service_account_scopes},
              ', "network_tags": ', {inputValue: network_tags},
              ', "kms_key": "', {inputValue: kms_key}, '"',
              ', "network_uri": "', {inputValue: network_uri}, '"',
              ', "subnetwork_uri": "', {inputValue: subnetwork_uri}, '"',
            '}',
            ', "peripherals_config": {',
              '"metastore_service": "', {inputValue: metastore_service}, '"',
              ', "spark_history_server_config": { ',
                '"dataproc_cluster": "', {inputValue: spark_history_dataproc_cluster}, '"',
              '}',
            '}',
          '}',
          ', "pyspark_batch": {',
            '"main_python_file_uri": "', {inputValue: main_python_file_uri}, '"',
            ', "python_file_uris": ', {inputValue: python_file_uris},
            ', "jar_file_uris": ', {inputValue: jar_file_uris},
            ', "file_uris": ', {inputValue: file_uris},
            ', "archive_uris": ', {inputValue: archive_uris},
            ', "args": ', {inputValue: args},
          '}',
        '}'
      ],
      --project, {inputValue: project},
      --location, {inputValue: location},
      --batch_id, {inputValue: batch_id},
      --gcp_resources, {outputPath: gcp_resources}
    ]
