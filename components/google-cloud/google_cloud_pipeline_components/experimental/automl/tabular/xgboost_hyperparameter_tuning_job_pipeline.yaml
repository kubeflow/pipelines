# PIPELINE DEFINITION
# Name: automl-tabular-xgboost-hyperparameter-tuning-job
# Description: The XGBoost HyperparameterTuningJob pipeline.
# Inputs:
#    bigquery_staging_full_dataset_id: str [Default: '']
#    data_source_bigquery_table_path: str [Default: '']
#    data_source_csv_filenames: str [Default: '']
#    dataflow_service_account: str [Default: '']
#    dataflow_subnetwork: str [Default: '']
#    dataflow_use_public_ips: bool [Default: True]
#    dataset_level_custom_transformation_definitions: list
#    dataset_level_transformations: list
#    disable_default_eval_metric: int [Default: 0.0]
#    encryption_spec_key_name: str [Default: '']
#    eval_metric: str [Default: '']
#    evaluation_batch_predict_machine_type: str [Default: 'n1-highmem-8']
#    evaluation_batch_predict_max_replica_count: int [Default: 20.0]
#    evaluation_batch_predict_starting_replica_count: int [Default: 20.0]
#    evaluation_dataflow_disk_size_gb: int [Default: 50.0]
#    evaluation_dataflow_machine_type: str [Default: 'n1-standard-4']
#    evaluation_dataflow_max_num_workers: int [Default: 100.0]
#    evaluation_dataflow_starting_num_workers: int [Default: 10.0]
#    feature_selection_algorithm: str [Default: 'AMI']
#    location: str
#    max_failed_trial_count: int [Default: 0.0]
#    max_selected_features: int [Default: -1.0]
#    max_trial_count: int
#    model_description: str [Default: '']
#    model_display_name: str [Default: '']
#    objective: str
#    parallel_trial_count: int
#    predefined_split_key: str [Default: '']
#    project: str
#    root_dir: str
#    run_evaluation: bool [Default: False]
#    run_feature_selection: bool [Default: False]
#    seed: int [Default: 0.0]
#    seed_per_iteration: bool [Default: False]
#    stratified_split_key: str [Default: '']
#    study_spec_algorithm: str [Default: 'ALGORITHM_UNSPECIFIED']
#    study_spec_measurement_selection_type: str [Default: 'BEST_MEASUREMENT']
#    study_spec_metric_goal: str
#    study_spec_metric_id: str
#    study_spec_parameters_override: list
#    target_column: str
#    test_fraction: float [Default: -1.0]
#    tf_auto_transform_features: dict
#    tf_custom_transformation_definitions: list
#    tf_transformations_path: str [Default: '']
#    training_accelerator_count: int [Default: 0.0]
#    training_accelerator_type: str [Default: '']
#    training_fraction: float [Default: -1.0]
#    training_machine_type: str [Default: 'c2-standard-16']
#    training_total_replica_count: int [Default: 1.0]
#    transform_dataflow_disk_size_gb: int [Default: 40.0]
#    transform_dataflow_machine_type: str [Default: 'n1-standard-16']
#    transform_dataflow_max_num_workers: int [Default: 25.0]
#    validation_fraction: float [Default: -1.0]
#    vertex_dataset: system.Artifact
#    weight_column: str [Default: '']
# Outputs:
#    model-evaluation-evaluation_metrics: system.Metrics
components:
  comp-automl-tabular-finalizer:
    executorLabel: exec-automl-tabular-finalizer
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          defaultValue: ''
          description: Customer-managed encryption key.
          isOptional: true
          parameterType: STRING
        location:
          description: Location for running the Cross-validation trainer.
          parameterType: STRING
        project:
          description: Required. Project to run Cross-validation trainer.
          parameterType: STRING
        root_dir:
          description: The Cloud Storage location to store the output.
          parameterType: STRING
    outputDefinitions:
      parameters:
        gcp_resources:
          description: 'GCP resources created by this component.

            For more details, see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
  comp-bool-identity:
    executorLabel: exec-bool-identity
    inputDefinitions:
      parameters:
        value:
          description: Boolean value to return
          parameterType: BOOLEAN
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-condition-2:
    dag:
      outputs:
        artifacts:
          model-evaluation-evaluation_metrics:
            artifactSelectors:
            - outputArtifactKey: evaluation_metrics
              producerSubtask: model-evaluation
      tasks:
        model-batch-predict:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-batch-predict
          inputs:
            artifacts:
              unmanaged_container_model:
                componentInputArtifact: pipelinechannel--get-best-hyperparameter-tuning-job-trial-unmanaged_container_model
            parameters:
              bigquery_source_input_uri:
                componentInputParameter: pipelinechannel--feature-transform-engine-bigquery_test_split_uri
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              gcs_destination_output_uri_prefix:
                componentInputParameter: pipelinechannel--root_dir
              instances_format:
                runtimeValue:
                  constant: bigquery
              job_display_name:
                runtimeValue:
                  constant: batch-predict-evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
              location:
                componentInputParameter: pipelinechannel--location
              machine_type:
                componentInputParameter: pipelinechannel--evaluation_batch_predict_machine_type
              max_replica_count:
                componentInputParameter: pipelinechannel--evaluation_batch_predict_max_replica_count
              predictions_format:
                runtimeValue:
                  constant: jsonl
              project:
                componentInputParameter: pipelinechannel--project
              starting_replica_count:
                componentInputParameter: pipelinechannel--evaluation_batch_predict_starting_replica_count
          taskInfo:
            name: model-batch-predict
        model-evaluation:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-evaluation
          dependentTasks:
          - model-batch-predict
          inputs:
            artifacts:
              batch_prediction_job:
                taskOutputArtifact:
                  outputArtifactKey: batchpredictionjob
                  producerTask: model-batch-predict
            parameters:
              dataflow_disk_size:
                componentInputParameter: pipelinechannel--evaluation_dataflow_disk_size_gb
              dataflow_machine_type:
                componentInputParameter: pipelinechannel--evaluation_dataflow_machine_type
              dataflow_max_workers_num:
                componentInputParameter: pipelinechannel--evaluation_dataflow_max_num_workers
              dataflow_service_account:
                componentInputParameter: pipelinechannel--dataflow_service_account
              dataflow_subnetwork:
                componentInputParameter: pipelinechannel--dataflow_subnetwork
              dataflow_use_public_ips:
                componentInputParameter: pipelinechannel--dataflow_use_public_ips
              dataflow_workers_num:
                componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              ground_truth_column:
                componentInputParameter: pipelinechannel--target_column
              ground_truth_format:
                runtimeValue:
                  constant: jsonl
              location:
                componentInputParameter: pipelinechannel--location
              prediction_label_column:
                runtimeValue:
                  constant: ''
              prediction_score_column:
                runtimeValue:
                  constant: ''
              predictions_format:
                runtimeValue:
                  constant: jsonl
              problem_type:
                componentInputParameter: pipelinechannel--get-prediction-type-for-xgboost-Output
              project:
                componentInputParameter: pipelinechannel--project
              root_dir:
                componentInputParameter: pipelinechannel--root_dir
          taskInfo:
            name: model-evaluation
    inputDefinitions:
      artifacts:
        pipelinechannel--get-best-hyperparameter-tuning-job-trial-unmanaged_container_model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--bool-identity-Output:
          parameterType: STRING
        pipelinechannel--dataflow_service_account:
          parameterType: STRING
        pipelinechannel--dataflow_subnetwork:
          parameterType: STRING
        pipelinechannel--dataflow_use_public_ips:
          parameterType: BOOLEAN
        pipelinechannel--encryption_spec_key_name:
          parameterType: STRING
        pipelinechannel--evaluation_batch_predict_machine_type:
          parameterType: STRING
        pipelinechannel--evaluation_batch_predict_max_replica_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_batch_predict_starting_replica_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_disk_size_gb:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_machine_type:
          parameterType: STRING
        pipelinechannel--evaluation_dataflow_max_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_starting_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--feature-transform-engine-bigquery_test_split_uri:
          parameterType: STRING
        pipelinechannel--get-prediction-type-for-xgboost-Output:
          parameterType: STRING
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--root_dir:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model-evaluation-evaluation_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-exit-handler-1:
    dag:
      outputs:
        artifacts:
          model-evaluation-evaluation_metrics:
            artifactSelectors:
            - outputArtifactKey: model-evaluation-evaluation_metrics
              producerSubtask: condition-2
      tasks:
        bool-identity:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bool-identity
          inputs:
            parameters:
              value:
                componentInputParameter: pipelinechannel--run_evaluation
          taskInfo:
            name: bool-identity
        condition-2:
          componentRef:
            name: comp-condition-2
          dependentTasks:
          - bool-identity
          - feature-transform-engine
          - get-best-hyperparameter-tuning-job-trial
          - get-prediction-type-for-xgboost
          inputs:
            artifacts:
              pipelinechannel--get-best-hyperparameter-tuning-job-trial-unmanaged_container_model:
                taskOutputArtifact:
                  outputArtifactKey: unmanaged_container_model
                  producerTask: get-best-hyperparameter-tuning-job-trial
            parameters:
              pipelinechannel--bool-identity-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: bool-identity
              pipelinechannel--dataflow_service_account:
                componentInputParameter: pipelinechannel--dataflow_service_account
              pipelinechannel--dataflow_subnetwork:
                componentInputParameter: pipelinechannel--dataflow_subnetwork
              pipelinechannel--dataflow_use_public_ips:
                componentInputParameter: pipelinechannel--dataflow_use_public_ips
              pipelinechannel--encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              pipelinechannel--evaluation_batch_predict_machine_type:
                componentInputParameter: pipelinechannel--evaluation_batch_predict_machine_type
              pipelinechannel--evaluation_batch_predict_max_replica_count:
                componentInputParameter: pipelinechannel--evaluation_batch_predict_max_replica_count
              pipelinechannel--evaluation_batch_predict_starting_replica_count:
                componentInputParameter: pipelinechannel--evaluation_batch_predict_starting_replica_count
              pipelinechannel--evaluation_dataflow_disk_size_gb:
                componentInputParameter: pipelinechannel--evaluation_dataflow_disk_size_gb
              pipelinechannel--evaluation_dataflow_machine_type:
                componentInputParameter: pipelinechannel--evaluation_dataflow_machine_type
              pipelinechannel--evaluation_dataflow_max_num_workers:
                componentInputParameter: pipelinechannel--evaluation_dataflow_max_num_workers
              pipelinechannel--evaluation_dataflow_starting_num_workers:
                componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
              pipelinechannel--feature-transform-engine-bigquery_test_split_uri:
                taskOutputParameter:
                  outputParameterKey: bigquery_test_split_uri
                  producerTask: feature-transform-engine
              pipelinechannel--get-prediction-type-for-xgboost-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-prediction-type-for-xgboost
              pipelinechannel--location:
                componentInputParameter: pipelinechannel--location
              pipelinechannel--project:
                componentInputParameter: pipelinechannel--project
              pipelinechannel--root_dir:
                componentInputParameter: pipelinechannel--root_dir
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
          taskInfo:
            name: run-evaluation
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--bool-identity-Output']
              == 'true'
        feature-transform-engine:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-feature-transform-engine
          dependentTasks:
          - get-prediction-type-for-xgboost
          inputs:
            parameters:
              bigquery_staging_full_dataset_id:
                componentInputParameter: pipelinechannel--bigquery_staging_full_dataset_id
              data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--set-optional-inputs-data_source_bigquery_table_path
              data_source_csv_filenames:
                componentInputParameter: pipelinechannel--set-optional-inputs-data_source_csv_filenames
              dataflow_disk_size_gb:
                componentInputParameter: pipelinechannel--transform_dataflow_disk_size_gb
              dataflow_machine_type:
                componentInputParameter: pipelinechannel--transform_dataflow_machine_type
              dataflow_max_num_workers:
                componentInputParameter: pipelinechannel--transform_dataflow_max_num_workers
              dataflow_service_account:
                componentInputParameter: pipelinechannel--dataflow_service_account
              dataflow_subnetwork:
                componentInputParameter: pipelinechannel--dataflow_subnetwork
              dataflow_use_public_ips:
                componentInputParameter: pipelinechannel--dataflow_use_public_ips
              dataset_level_custom_transformation_definitions:
                componentInputParameter: pipelinechannel--dataset_level_custom_transformation_definitions
              dataset_level_transformations:
                componentInputParameter: pipelinechannel--dataset_level_transformations
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              feature_selection_algorithm:
                componentInputParameter: pipelinechannel--feature_selection_algorithm
              location:
                componentInputParameter: pipelinechannel--location
              max_selected_features:
                componentInputParameter: pipelinechannel--max_selected_features
              predefined_split_key:
                componentInputParameter: pipelinechannel--predefined_split_key
              prediction_type:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-prediction-type-for-xgboost
              project:
                componentInputParameter: pipelinechannel--project
              root_dir:
                componentInputParameter: pipelinechannel--root_dir
              run_feature_selection:
                componentInputParameter: pipelinechannel--run_feature_selection
              stratified_split_key:
                componentInputParameter: pipelinechannel--stratified_split_key
              target_column:
                componentInputParameter: pipelinechannel--target_column
              test_fraction:
                componentInputParameter: pipelinechannel--test_fraction
              tf_auto_transform_features:
                componentInputParameter: pipelinechannel--tf_auto_transform_features
              tf_custom_transformation_definitions:
                componentInputParameter: pipelinechannel--tf_custom_transformation_definitions
              tf_transformations_path:
                componentInputParameter: pipelinechannel--tf_transformations_path
              training_fraction:
                componentInputParameter: pipelinechannel--training_fraction
              validation_fraction:
                componentInputParameter: pipelinechannel--validation_fraction
              weight_column:
                componentInputParameter: pipelinechannel--weight_column
          taskInfo:
            name: feature-transform-engine
        generate-xgboost-hyperparameter-tuning-worker-pool-specs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-generate-xgboost-hyperparameter-tuning-worker-pool-specs
          dependentTasks:
          - feature-transform-engine
          - split-materialized-data
          - training-configurator-and-validator
          inputs:
            artifacts:
              instance_baseline:
                taskOutputArtifact:
                  outputArtifactKey: instance_baseline
                  producerTask: training-configurator-and-validator
              materialized_eval_split:
                taskOutputArtifact:
                  outputArtifactKey: materialized_eval_split
                  producerTask: split-materialized-data
              materialized_train_split:
                taskOutputArtifact:
                  outputArtifactKey: materialized_train_split
                  producerTask: split-materialized-data
              training_schema_uri:
                taskOutputArtifact:
                  outputArtifactKey: training_schema
                  producerTask: feature-transform-engine
              transform_output:
                taskOutputArtifact:
                  outputArtifactKey: transform_output
                  producerTask: feature-transform-engine
            parameters:
              accelerator_count:
                componentInputParameter: pipelinechannel--training_accelerator_count
              accelerator_type:
                componentInputParameter: pipelinechannel--training_accelerator_type
              disable_default_eval_metric:
                componentInputParameter: pipelinechannel--disable_default_eval_metric
              eval_metric:
                componentInputParameter: pipelinechannel--eval_metric
              machine_type:
                componentInputParameter: pipelinechannel--training_machine_type
              objective:
                componentInputParameter: pipelinechannel--objective
              seed:
                componentInputParameter: pipelinechannel--seed
              seed_per_iteration:
                componentInputParameter: pipelinechannel--seed_per_iteration
              target_column:
                componentInputParameter: pipelinechannel--target_column
              total_replica_count:
                componentInputParameter: pipelinechannel--training_total_replica_count
              weight_column:
                componentInputParameter: pipelinechannel--weight_column
          taskInfo:
            name: generate-xgboost-hyperparameter-tuning-worker-pool-specs
        get-best-hyperparameter-tuning-job-trial:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-best-hyperparameter-tuning-job-trial
          dependentTasks:
          - generate-xgboost-hyperparameter-tuning-worker-pool-specs
          - xgboost-hyperparameter-tuning-job
          inputs:
            parameters:
              gcp_resources:
                taskOutputParameter:
                  outputParameterKey: gcp_resources
                  producerTask: xgboost-hyperparameter-tuning-job
              instance_schema_uri:
                taskOutputParameter:
                  outputParameterKey: instance_schema_path
                  producerTask: generate-xgboost-hyperparameter-tuning-worker-pool-specs
              prediction_docker_uri:
                taskOutputParameter:
                  outputParameterKey: prediction_docker_uri_artifact_path
                  producerTask: generate-xgboost-hyperparameter-tuning-worker-pool-specs
              prediction_schema_uri:
                taskOutputParameter:
                  outputParameterKey: prediction_schema_path
                  producerTask: generate-xgboost-hyperparameter-tuning-worker-pool-specs
              read_value_from_file:
                runtimeValue:
                  constant: 1.0
              study_spec_metric_goal:
                componentInputParameter: pipelinechannel--study_spec_metric_goal
              trials_dir:
                taskOutputParameter:
                  outputParameterKey: trials_path
                  producerTask: generate-xgboost-hyperparameter-tuning-worker-pool-specs
          taskInfo:
            name: get-best-hyperparameter-tuning-job-trial
        get-prediction-type-for-xgboost:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-prediction-type-for-xgboost
          inputs:
            parameters:
              objective:
                componentInputParameter: pipelinechannel--objective
          taskInfo:
            name: get-prediction-type-for-xgboost
        get-xgboost-study-spec-parameters:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-xgboost-study-spec-parameters
          inputs:
            parameters:
              study_spec_parameters_override:
                componentInputParameter: pipelinechannel--study_spec_parameters_override
          taskInfo:
            name: get-xgboost-study-spec-parameters
        model-upload:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-upload
          dependentTasks:
          - get-best-hyperparameter-tuning-job-trial
          inputs:
            artifacts:
              unmanaged_container_model:
                taskOutputArtifact:
                  outputArtifactKey: unmanaged_container_model
                  producerTask: get-best-hyperparameter-tuning-job-trial
            parameters:
              description:
                componentInputParameter: pipelinechannel--model_description
              display_name:
                componentInputParameter: pipelinechannel--set-optional-inputs-model_display_name
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              location:
                componentInputParameter: pipelinechannel--location
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: model-upload
        split-materialized-data:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-split-materialized-data
          dependentTasks:
          - feature-transform-engine
          inputs:
            artifacts:
              materialized_data:
                taskOutputArtifact:
                  outputArtifactKey: materialized_data
                  producerTask: feature-transform-engine
          taskInfo:
            name: split-materialized-data
        training-configurator-and-validator:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-training-configurator-and-validator
          dependentTasks:
          - feature-transform-engine
          - get-prediction-type-for-xgboost
          inputs:
            artifacts:
              dataset_stats:
                taskOutputArtifact:
                  outputArtifactKey: dataset_stats
                  producerTask: feature-transform-engine
              instance_schema:
                taskOutputArtifact:
                  outputArtifactKey: instance_schema
                  producerTask: feature-transform-engine
              training_schema:
                taskOutputArtifact:
                  outputArtifactKey: training_schema
                  producerTask: feature-transform-engine
            parameters:
              prediction_type:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-prediction-type-for-xgboost
              run_evaluation:
                componentInputParameter: pipelinechannel--run_evaluation
              split_example_counts:
                taskOutputParameter:
                  outputParameterKey: split_example_counts
                  producerTask: feature-transform-engine
              target_column:
                componentInputParameter: pipelinechannel--target_column
              weight_column:
                componentInputParameter: pipelinechannel--weight_column
          taskInfo:
            name: training-configurator-and-validator
        xgboost-hyperparameter-tuning-job:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-xgboost-hyperparameter-tuning-job
          dependentTasks:
          - generate-xgboost-hyperparameter-tuning-worker-pool-specs
          - get-xgboost-study-spec-parameters
          inputs:
            parameters:
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              location:
                componentInputParameter: pipelinechannel--location
              max_failed_trial_count:
                componentInputParameter: pipelinechannel--max_failed_trial_count
              max_trial_count:
                componentInputParameter: pipelinechannel--max_trial_count
              parallel_trial_count:
                componentInputParameter: pipelinechannel--parallel_trial_count
              project:
                componentInputParameter: pipelinechannel--project
              study_spec_algorithm:
                componentInputParameter: pipelinechannel--study_spec_algorithm
              study_spec_measurement_selection_type:
                componentInputParameter: pipelinechannel--study_spec_measurement_selection_type
              study_spec_metric_goal:
                componentInputParameter: pipelinechannel--study_spec_metric_goal
              study_spec_metric_id:
                componentInputParameter: pipelinechannel--study_spec_metric_id
              study_spec_parameters_override:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-xgboost-study-spec-parameters
              worker_pool_specs:
                taskOutputParameter:
                  outputParameterKey: worker_pool_specs
                  producerTask: generate-xgboost-hyperparameter-tuning-worker-pool-specs
          taskInfo:
            name: xgboost-hyperparameter-tuning-job
    inputDefinitions:
      parameters:
        pipelinechannel--bigquery_staging_full_dataset_id:
          parameterType: STRING
        pipelinechannel--dataflow_service_account:
          parameterType: STRING
        pipelinechannel--dataflow_subnetwork:
          parameterType: STRING
        pipelinechannel--dataflow_use_public_ips:
          parameterType: BOOLEAN
        pipelinechannel--dataset_level_custom_transformation_definitions:
          parameterType: LIST
        pipelinechannel--dataset_level_transformations:
          parameterType: LIST
        pipelinechannel--disable_default_eval_metric:
          parameterType: NUMBER_INTEGER
        pipelinechannel--encryption_spec_key_name:
          parameterType: STRING
        pipelinechannel--eval_metric:
          parameterType: STRING
        pipelinechannel--evaluation_batch_predict_machine_type:
          parameterType: STRING
        pipelinechannel--evaluation_batch_predict_max_replica_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_batch_predict_starting_replica_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_disk_size_gb:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_machine_type:
          parameterType: STRING
        pipelinechannel--evaluation_dataflow_max_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_starting_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--feature_selection_algorithm:
          parameterType: STRING
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--max_failed_trial_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--max_selected_features:
          parameterType: NUMBER_INTEGER
        pipelinechannel--max_trial_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--model_description:
          parameterType: STRING
        pipelinechannel--objective:
          parameterType: STRING
        pipelinechannel--parallel_trial_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--predefined_split_key:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--root_dir:
          parameterType: STRING
        pipelinechannel--run_evaluation:
          parameterType: BOOLEAN
        pipelinechannel--run_feature_selection:
          parameterType: BOOLEAN
        pipelinechannel--seed:
          parameterType: NUMBER_INTEGER
        pipelinechannel--seed_per_iteration:
          parameterType: BOOLEAN
        pipelinechannel--set-optional-inputs-data_source_bigquery_table_path:
          parameterType: STRING
        pipelinechannel--set-optional-inputs-data_source_csv_filenames:
          parameterType: STRING
        pipelinechannel--set-optional-inputs-model_display_name:
          parameterType: STRING
        pipelinechannel--stratified_split_key:
          parameterType: STRING
        pipelinechannel--study_spec_algorithm:
          parameterType: STRING
        pipelinechannel--study_spec_measurement_selection_type:
          parameterType: STRING
        pipelinechannel--study_spec_metric_goal:
          parameterType: STRING
        pipelinechannel--study_spec_metric_id:
          parameterType: STRING
        pipelinechannel--study_spec_parameters_override:
          parameterType: LIST
        pipelinechannel--target_column:
          parameterType: STRING
        pipelinechannel--test_fraction:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--tf_auto_transform_features:
          parameterType: STRUCT
        pipelinechannel--tf_custom_transformation_definitions:
          parameterType: LIST
        pipelinechannel--tf_transformations_path:
          parameterType: STRING
        pipelinechannel--training_accelerator_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--training_accelerator_type:
          parameterType: STRING
        pipelinechannel--training_fraction:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--training_machine_type:
          parameterType: STRING
        pipelinechannel--training_total_replica_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--transform_dataflow_disk_size_gb:
          parameterType: NUMBER_INTEGER
        pipelinechannel--transform_dataflow_machine_type:
          parameterType: STRING
        pipelinechannel--transform_dataflow_max_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--validation_fraction:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--weight_column:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model-evaluation-evaluation_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-feature-transform-engine:
    executorLabel: exec-feature-transform-engine
    inputDefinitions:
      parameters:
        autodetect_csv_schema:
          defaultValue: false
          description: 'If True, infers the column types

            when importing CSVs into BigQuery.'
          isOptional: true
          parameterType: BOOLEAN
        bigquery_staging_full_dataset_id:
          defaultValue: ''
          description: 'Dataset in

            "projectId.datasetId" format for storing intermediate-FTE BigQuery

            tables.  If the specified dataset does not exist in BigQuery, FTE will

            create the dataset. If no bigquery_staging_full_dataset_id is specified,

            all intermediate tables will be stored in a dataset created under the

            provided project in the input data source''s location during FTE

            execution called

            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".

            All tables generated by FTE will have a 30 day TTL.'
          isOptional: true
          parameterType: STRING
        data_source_bigquery_table_path:
          defaultValue: ''
          description: 'BigQuery input data

            source to run feature transform on.'
          isOptional: true
          parameterType: STRING
        data_source_csv_filenames:
          defaultValue: ''
          description: 'CSV input data source to run

            feature transform on.'
          isOptional: true
          parameterType: STRING
        dataflow_disk_size_gb:
          defaultValue: 40.0
          description: 'The disk size, in gigabytes, to use

            on each Dataflow worker instance. If not set, default to 40.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_machine_type:
          defaultValue: n1-standard-16
          description: 'The machine type used for dataflow

            jobs. If not set, default to n1-standard-16.'
          isOptional: true
          parameterType: STRING
        dataflow_max_num_workers:
          defaultValue: 25.0
          description: 'The number of workers to run the

            dataflow job. If not set, default to 25.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_service_account:
          defaultValue: ''
          description: 'Custom service account to run

            Dataflow jobs.'
          isOptional: true
          parameterType: STRING
        dataflow_subnetwork:
          defaultValue: ''
          description: 'Dataflow''s fully qualified subnetwork

            name, when empty the default subnetwork will be used. More details:

            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
          isOptional: true
          parameterType: STRING
        dataflow_use_public_ips:
          defaultValue: true
          description: 'Specifies whether Dataflow

            workers use public IP addresses.'
          isOptional: true
          parameterType: BOOLEAN
        dataset_level_custom_transformation_definitions:
          defaultValue: []
          description: "List of dataset-level custom transformation definitions. \
            \ Custom,\nbring-your-own dataset-level transform functions, where users\
            \ can define\nand import their own transform function and use it with\
            \ FTE's built-in\ntransformations. Using custom transformations is an\
            \ experimental feature\nand it is currently not supported during batch\
            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
            \ } ]"
          isOptional: true
          parameterType: LIST
        dataset_level_transformations:
          defaultValue: []
          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
            \ supported built-in\n  transformations:\n    Join: Joins features from\
            \ right_table_uri. For each join key, the\n      left table keys will\
            \ be included and the right table keys will\n      be dropped.\n     \
            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
            \        Arguments:\n            right_table_uri (str): Right table BigQuery\
            \ uri to join\n              with input_full_table_id.\n            join_keys\
            \ (List[List[str]]): Features to join on. For each\n              nested\
            \ list, the first element is a left table column\n              and the\
            \ second is its corresponding right table column.\n    TimeAggregate:\
            \ Creates a new feature composed of values of an\n      existing feature\
            \ from a fixed time period ago or in the future.\n      Ex: A feature\
            \ for sales by store 1 year ago.\n        Example:  .. code-block:: python\
            \  { \"transformation\":\n          \"TimeAggregate\", \"time_difference\"\
            : 40,\n          \"time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
            \     Arguments:\n            time_difference (int): Number of time_difference_units\
            \ to\n              look back or into the future on our\n            \
            \  time_difference_target_column.\n            time_difference_units (str):\
            \ Units of time_difference to\n              look back or into the future\
            \ on our\n              time_difference_target_column. Must be one of\
            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
            \ *\n              'YEAR'\n            time_series_identifier_columns\
            \ (List[str]): Names of the\n              time series identifier columns.\n\
            \            time_column (str): Name of the time column.\n           \
            \ time_difference_target_column (str): Column we wish to get\n       \
            \       the value of time_difference time_difference_units in\n      \
            \        the past or future.\n            output_column (str): Name of\
            \ our new time aggregate\n              feature.\n            is_future\
            \ (Optional[bool]): Whether we wish to look\n              forward in\
            \ time. Defaults to False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
            \              Performs a partition by reduce operation (one of max,\n\
            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
            \              Getting avg sales (the reduce column) for each store\n\
            \              (partition_by_column) over the previous 5 days\n      \
            \        (time_column, time_ago_units, and time_ago).\n        Example:\
            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
            \   reduce_column (str): Column to apply the reduce operation\n      \
            \        on. Reduce operations include the\n                following:\
            \ Max, Min, Avg, Sum.\n            partition_by_columns (List[str]): List\
            \ of columns to\n              partition by.\n            time_column\
            \ (str): Time column for the partition by\n              operation's window\
            \ function.\n            time_ago (int): Number of time_ago_units to look\
            \ back on\n              our target_column, starting from time_column\n\
            \              (inclusive).\n            time_ago_units (str): Units of\
            \ time_ago to look back on\n              our target_column. Must be one\
            \ of * 'DAY' * 'WEEK'\n            output_column (str): Name of our output\
            \ feature."
          isOptional: true
          parameterType: LIST
        encryption_spec_key_name:
          defaultValue: ''
          description: Customer-managed encryption key.
          isOptional: true
          parameterType: STRING
        feature_selection_algorithm:
          defaultValue: AMI
          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
            \       Mutual Information Maximization): Reference paper: Mohamed\n \
            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
            \         and Chris Ding. \"Feature selection based on mutual information\n\
            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
            \         27, no.\n       8 (2005): 1226-1238."
          isOptional: true
          parameterType: STRING
        forecasting_apply_windowing:
          defaultValue: true
          description: 'Whether to apply window

            strategy.'
          isOptional: true
          parameterType: BOOLEAN
        forecasting_available_at_forecast_columns:
          defaultValue: []
          description: Forecasting available at forecast columns.
          isOptional: true
          parameterType: LIST
        forecasting_context_window:
          defaultValue: -1.0
          description: Forecasting context window.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_forecast_horizon:
          defaultValue: -1.0
          description: Forecasting horizon.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_predefined_window_column:
          defaultValue: ''
          description: 'Forecasting

            predefined window column.'
          isOptional: true
          parameterType: STRING
        forecasting_time_column:
          defaultValue: ''
          description: Forecasting time column.
          isOptional: true
          parameterType: STRING
        forecasting_time_series_attribute_columns:
          defaultValue: []
          description: Forecasting time series attribute columns.
          isOptional: true
          parameterType: LIST
        forecasting_time_series_identifier_column:
          defaultValue: ''
          description: 'Forecasting

            time series identifier column.'
          isOptional: true
          parameterType: STRING
        forecasting_unavailable_at_forecast_columns:
          defaultValue: []
          description: Forecasting unavailable at forecast columns.
          isOptional: true
          parameterType: LIST
        forecasting_window_max_count:
          defaultValue: -1.0
          description: 'Forecasting window max

            count.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_window_stride_length:
          defaultValue: -1.0
          description: 'Forecasting window

            stride length.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        group_columns:
          isOptional: true
          parameterType: LIST
        group_temporal_total_weight:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        group_total_weight:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        legacy_transformations_path:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        location:
          description: Location for the created GCP services.
          parameterType: STRING
        materialized_examples_format:
          defaultValue: tfrecords_gzip
          description: 'The format to use for the

            materialized examples. Should be either ''tfrecords_gzip'' (default) or

            ''parquet''.'
          isOptional: true
          parameterType: STRING
        max_selected_features:
          defaultValue: 1000.0
          description: 'Maximum number of features to

            select.  If specified, the transform config will be purged by only using

            the selected features that ranked top in the feature ranking, which has

            the ranking value for all supported features. If the number of input

            features is smaller than max_selected_features specified, we will still

            run the feature selection process and generate the feature ranking, no

            features will be excluded.  The value will be set to 1000 by default if

            run_feature_selection is enabled.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_type:
          description: 'Model type, which we wish to engineer features

            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
            or

            tide. Defaults to the empty value, `None`.'
          isOptional: true
          parameterType: STRING
        predefined_split_key:
          defaultValue: ''
          description: Predefined split key.
          isOptional: true
          parameterType: STRING
        prediction_type:
          defaultValue: ''
          description: 'Model prediction type. One of

            "classification", "regression", "time_series".'
          isOptional: true
          parameterType: STRING
        project:
          description: Project to run feature transform engine.
          parameterType: STRING
        root_dir:
          description: The Cloud Storage location to store the output.
          parameterType: STRING
        run_distill:
          defaultValue: false
          description: 'Whether the distillation should be applied

            to the training.'
          isOptional: true
          parameterType: BOOLEAN
        run_feature_selection:
          defaultValue: false
          description: 'Whether the feature selection

            should be applied to the dataset.'
          isOptional: true
          parameterType: BOOLEAN
        stratified_split_key:
          defaultValue: ''
          description: Stratified split key.
          isOptional: true
          parameterType: STRING
        target_column:
          defaultValue: ''
          description: Target column of input data.
          isOptional: true
          parameterType: STRING
        temporal_total_weight:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        test_fraction:
          defaultValue: -1.0
          description: Fraction of input data for testing.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        tf_auto_transform_features:
          defaultValue: {}
          description: "Dict[str, List[str]]\nmapping auto and/or type-resolutions\
            \ to TF transform features.  FTE will\nautomatically configure a set of\
            \ built-in transformations for each\nfeature based on its data statistics.\
            \  If users do not want auto type\nresolution, but want the set of transformations\
            \ for a given type to be\nautomatically generated, they may specify pre-resolved\
            \ transformations\ntypes.  The following type hint dict keys are supported:\
            \ * 'auto' *\n'categorical' * 'numeric' * 'text' * 'timestamp'\n  Example:\
            \  .. code-block:: python { \"auto\": [\"feature1\"],\n    \"categorical\"\
            : [\"feature2\", \"feature3\"], }  Note that the target and\n    weight\
            \ column may not be included as an auto transformation unless\n    users\
            \ are running forecasting."
          isOptional: true
          parameterType: STRUCT
        tf_custom_transformation_definitions:
          defaultValue: []
          description: "List of\nTensorFlow-based custom transformation definitions.\
            \  Custom,\nbring-your-own transform functions, where users can define\
            \ and import\ntheir own transform function and use it with FTE's built-in\n\
            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
            \    transform function together with FTE's built-in transformations:\
            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
            :\n    [\"feature_1_multiplied_two\"] } ]"
          isOptional: true
          parameterType: LIST
        tf_transform_execution_engine:
          defaultValue: dataflow
          description: 'Execution engine to perform

            row-level TF transformations. Can be one of: "dataflow" (by default) or

            "bigquery". Using "bigquery" as the execution engine is experimental and

            is for allowlisted customers only. In addition, executing on "bigquery"

            only supports auto transformations (i.e., specified by

            tf_auto_transform_features) and will raise an error when

            tf_custom_transformation_definitions or tf_transformations_path is set.'
          isOptional: true
          parameterType: STRING
        tf_transformations_path:
          defaultValue: ''
          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
            \  In the following, we provide\nsome sample transform configurations\
            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
            \ of multiple transformations on a\nsingle column is also supported. For\
            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
            \ Extracts datetime featues from a column containing\n        timestamp\
            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns\
            \ (List[str]): A list with a single column to\n                perform\
            \ the datetime transformation on.\n              output_columns (Optional[List[str]]):\
            \ Names of output\n                columns, one for each datetime_features\
            \ element.\n              time_format (str): Datetime format string. Time\
            \ format is\n                a combination of Date + Time Delimiter (optional)\
            \ + Time\n                (optional) directives. Valid date directives\
            \ are as\n                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'\
            \  #\n                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'\
            \  #\n                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
            \  #\n                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'\
            \  #\n                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'\
            \  #\n                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
            \                are as follows * 'T' * ' ' Valid time directives are\
            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features\
            \ (Optional[List[str]]): List of datetime\n                features to\
            \ be extract. Each entry must be one of *\n                'YEAR' * 'MONTH'\
            \ * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR'\
            \ * 'QUARTER' * 'HOUR' * 'MINUTE' *\n                'SECOND' Defaults\
            \ to ['YEAR', 'MONTH', 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR',\
            \ 'WEEK_OF_YEAR']\n      Log: Performs the natural log on a numeric column.\n\
            \          Example:  .. code-block:: python  { \"transformation\": \"\
            Log\",\n            \"input_columns\": [\"feature_1\"] }\n          Arguments:\n\
            \              input_columns (List[str]): A list with a single column\
            \ to\n                perform the log transformation on.\n           \
            \   output_columns (Optional[List[str]]): A list with a single\n     \
            \           output column name, corresponding to the output of our\n \
            \               transformation.\n      ZScale: Performs Z-scale normalization\
            \ on a numeric column.\n          Example:  .. code-block:: python  {\
            \ \"transformation\":\n            \"ZScale\", \"input_columns\": [\"\
            feature_1\"] }\n          Arguments:\n              input_columns (List[str]):\
            \ A list with a single column to\n                perform the z-scale\
            \ transformation on.\n              output_columns (Optional[List[str]]):\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n      Vocabulary:\
            \ Converts strings to integers, where each unique string\n        gets\
            \ a unique integer representation.\n          Example:  .. code-block::\
            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
            : [\"feature_1\"] }\n          Arguments:\n              input_columns\
            \ (List[str]): A list with a single column to\n                perform\
            \ the vocabulary transformation on.\n              output_columns (Optional[List[str]]):\
            \ A list with a single\n                output column name, corresponding\
            \ to the output of our\n                transformation.\n            \
            \  top_k (Optional[int]): Number of the most frequent words\n        \
            \        in the vocabulary to use for generating dictionary\n        \
            \        lookup indices. If not specified, all words in the\n        \
            \        vocabulary will be used. Defaults to None.\n              frequency_threshold\
            \ (Optional[int]): Limit the vocabulary\n                only to words\
            \ whose number of occurrences in the input\n                exceeds frequency_threshold.\
            \ If not specified, all words\n                in the vocabulary will\
            \ be included. If both top_k and\n                frequency_threshold\
            \ are specified, a word must satisfy\n                both conditions\
            \ to be included. Defaults to None.\n      Categorical: Transforms categorical\
            \ columns to integer columns.\n          Example:  .. code-block:: python\
            \  { \"transformation\":\n            \"Categorical\", \"input_columns\"\
            : [\"feature_1\"], \"top_k\": 10 }\n          Arguments:\n           \
            \   input_columns (List[str]): A list with a single column to\n      \
            \          perform the categorical transformation on.\n              output_columns\
            \ (Optional[List[str]]): A list with a single\n                output\
            \ column name, corresponding to the output of our\n                transformation.\n\
            \              top_k (Optional[int]): Number of the most frequent words\n\
            \                in the vocabulary to use for generating dictionary\n\
            \                lookup indices. If not specified, all words in the\n\
            \                vocabulary will be used.\n              frequency_threshold\
            \ (Optional[int]): Limit the vocabulary\n                only to words\
            \ whose number of occurrences in the input\n                exceeds frequency_threshold.\
            \ If not specified, all words\n                in the vocabulary will\
            \ be included. If both top_k and\n                frequency_threshold\
            \ are specified, a word must satisfy\n                both conditions\
            \ to be included.\n      Reduce: Given a column where each entry is a\
            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
            \          Example:  .. code-block:: python  { \"transformation\":\n \
            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
            \          Arguments:\n              input_columns (List[str]): A list\
            \ with a single column to\n                perform the reduce transformation\
            \ on.\n              output_columns (Optional[List[str]]): A list with\
            \ a single\n                output column name, corresponding to the output\
            \ of our\n                transformation.\n              reduce_mode (Optional[str]):\
            \ One of * 'MAX' * 'MIN' *\n                'MEAN' * 'LAST_K' Defaults\
            \ to 'MEAN'.\n              last_k (Optional[int]): The number of last\
            \ k elements when\n                'LAST_K' reduce mode is used. Defaults\
            \ to 1.\n      SplitString: Given a column of strings, splits strings\
            \ into token\n        arrays.\n          Example:  .. code-block:: python\
            \  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
            \              input_columns (List[str]): A list with a single column\
            \ to\n                perform the split string transformation on.\n  \
            \            output_columns (Optional[List[str]]): A list with a single\n\
            \                output column name, corresponding to the output of our\n\
            \                transformation.\n              separator (Optional[str]):\
            \ Separator to split input string\n                into tokens. Defaults\
            \ to ' '.\n              missing_token (Optional[str]): Missing token\
            \ to use when\n                no string is included. Defaults to ' _MISSING_\
            \ '.\n      NGram: Given a column of strings, splits strings into token\
            \ arrays\n        where each token is an integer.\n          Example:\
            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
            \              input_columns (List[str]): A list with a single column\
            \ to\n                perform the n-gram transformation on.\n        \
            \      output_columns (Optional[List[str]]): A list with a single\n  \
            \              output column name, corresponding to the output of our\n\
            \                transformation.\n              min_ngram_size (Optional[int]):\
            \ Minimum n-gram size. Must\n                be a positive number and\
            \ <= max_ngram_size. Defaults to\n                1.\n              max_ngram_size\
            \ (Optional[int]): Maximum n-gram size. Must\n                be a positive\
            \ number and >= min_ngram_size. Defaults to\n                2.\n    \
            \          top_k (Optional[int]): Number of the most frequent words\n\
            \                in the vocabulary to use for generating dictionary\n\
            \                lookup indices. If not specified, all words in the\n\
            \                vocabulary will be used. Defaults to None.\n        \
            \      frequency_threshold (Optional[int]): Limit the\n              \
            \  dictionary's vocabulary only to words whose number of\n           \
            \     occurrences in the input exceeds frequency_threshold. If\n     \
            \           not specified, all words in the vocabulary will be\n     \
            \           included. If both top_k and frequency_threshold are\n    \
            \            specified, a word must satisfy both conditions to be\n  \
            \              included. Defaults to None.\n              separator (Optional[str]):\
            \ Separator to split input string\n                into tokens. Defaults\
            \ to ' '.\n              missing_token (Optional[str]): Missing token\
            \ to use when\n                no string is included. Defaults to ' _MISSING_\
            \ '.\n      Clip: Given a numeric column, clips elements such that elements\
            \ <\n        min_value are assigned min_value, and elements > max_value\
            \ are\n        assigned max_value.\n          Example:  .. code-block::\
            \ python  { \"transformation\": \"Clip\",\n            \"input_columns\"\
            : [\"col1\"], \"output_columns\":\n            [\"col1_clipped\"], \"\
            min_value\": 1., \"max_value\": 10., }\n          Arguments:\n       \
            \       input_columns (List[str]): A list with a single column to\n  \
            \              perform the n-gram transformation on.\n              output_columns\
            \ (Optional[List[str]]): A list with a single\n                output\
            \ column name, corresponding to the output of our\n                transformation.\n\
            \              min_value (Optional[float]): Number where all values below\n\
            \                min_value are set to min_value. If no min_value is\n\
            \                provided, min clipping will not occur. Defaults to None.\n\
            \              max_value (Optional[float]): Number where all values above\n\
            \                max_value are set to max_value If no max_value is\n \
            \               provided, max clipping will not occur. Defaults to None.\n\
            \      MultiHotEncoding: Performs multi-hot encoding on a categorical\n\
            \        array column.\n          Example:  .. code-block:: python  {\
            \ \"transformation\":\n            \"MultiHotEncoding\", \"input_columns\"\
            : [\"col1\"], }  The number\n            of classes is determened by the\
            \ largest number included in\n            the input if it is numeric or\
            \ the total number of unique\n            values of the input if it is\
            \ type str.  If the input is has\n            type str and an element\
            \ contians separator tokens, the input\n            will be split at separator\
            \ indices, and the each element of\n            the split list will be\
            \ considered a seperate class. For\n            example,\n          Input:\
            \  .. code-block:: python  [ [\"foo bar\"],      # Example\n         \
            \   0 [\"foo\", \"bar\"],   # Example 1 [\"foo\"],          # Example\n\
            \            2 [\"bar\"],          # Example 3 ]\n          Output (with\
            \ default separator=\" \"):  .. code-block:: python [\n            [1,\
            \ 1],          # Example 0 [1, 1],          # Example 1\n            [1,\
            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n          Arguments:\n\
            \              input_columns (List[str]): A list with a single column\
            \ to\n                perform the multi-hot-encoding on.\n           \
            \   output_columns (Optional[List[str]]): A list with a single\n     \
            \           output column name, corresponding to the output of our\n \
            \               transformation.\n              top_k (Optional[int]):\
            \ Number of the most frequent words\n                in the vocabulary\
            \ to use for generating dictionary\n                lookup indices. If\
            \ not specified, all words in the\n                vocabulary will be\
            \ used. Defaults to None.\n              frequency_threshold (Optional[int]):\
            \ Limit the\n                dictionary's vocabulary only to words whose\
            \ number of\n                occurrences in the input exceeds frequency_threshold.\
            \ If\n                not specified, all words in the vocabulary will\
            \ be\n                included. If both top_k and frequency_threshold\
            \ are\n                specified, a word must satisfy both conditions\
            \ to be\n                included. Defaults to None.\n              separator\
            \ (Optional[str]): Separator to split input string\n                into\
            \ tokens. Defaults to ' '.\n      MaxAbsScale: Performs maximum absolute\
            \ scaling on a numeric\n        column.\n          Example:  .. code-block::\
            \ python  { \"transformation\":\n            \"MaxAbsScale\", \"input_columns\"\
            : [\"col1\"], \"output_columns\":\n            [\"col1_max_abs_scaled\"\
            ] }\n          Arguments:\n              input_columns (List[str]): A\
            \ list with a single column to\n                perform max-abs-scale\
            \ on.\n              output_columns (Optional[List[str]]): A list with\
            \ a single\n                output column name, corresponding to the output\
            \ of our\n                transformation.\n      Custom: Transformations\
            \ defined in\n        tf_custom_transformation_definitions are included\
            \ here in the\n        TensorFlow-based transformation configuration.\
            \  For example,\n        given the following tf_custom_transformation_definitions:\
            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
            \ the\n        following transformation:  .. code-block:: python  {\n\
            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
            \ that\n        input_columns must still be included in our arguments\
            \ and\n        output_columns is optional. All other arguments are those\n\
            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
            \ this\n        case. See tf_custom_transformation_definitions above.\n\
            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
            \ and tf_auto_transform_features\n        cannot both be specified."
          isOptional: true
          parameterType: STRING
        timestamp_split_key:
          defaultValue: ''
          description: Timestamp split key.
          isOptional: true
          parameterType: STRING
        training_fraction:
          defaultValue: -1.0
          description: Fraction of input data for training.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        validation_fraction:
          defaultValue: -1.0
          description: 'Fraction of input data for

            validation.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        weight_column:
          defaultValue: ''
          description: Weight column of input data.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_stats:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The stats of the dataset.
        feature_ranking:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'The ranking of features, all features supported in the dataset
            will be

            included.


            for "AMI" algorithm, array features won''t be available in the ranking

            as arrays are not

            supported yet.'
        instance_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        materialized_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: The materialized dataset.
        training_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        transform_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The transform output artifact.
      parameters:
        bigquery_downsampled_test_split_uri:
          description: 'BigQuery URI for the downsampled test split to pass to the
            batch

            prediction component

            during batch explain.'
          parameterType: STRING
        bigquery_test_split_uri:
          description: 'BigQuery URI for the test split to pass to the batch prediction

            component during

            evaluation.'
          parameterType: STRING
        bigquery_train_split_uri:
          description: 'BigQuery URI for the train split to pass to the batch prediction
            component during

            distillation.'
          parameterType: STRING
        bigquery_validation_split_uri:
          description: 'BigQuery URI for the validation split to pass to the batch
            prediction component during

            distillation.'
          parameterType: STRING
        gcp_resources:
          description: 'GCP resources created by this component.

            For more details, see

            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
          parameterType: STRING
        split_example_counts:
          description: 'JSON string of data split example counts for train, validate,
            and test

            splits.'
          parameterType: STRING
  comp-generate-xgboost-hyperparameter-tuning-worker-pool-specs:
    executorLabel: exec-generate-xgboost-hyperparameter-tuning-worker-pool-specs
    inputDefinitions:
      artifacts:
        instance_baseline:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: Path to JSON file for baseline values.
        materialized_eval_split:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Required. The path to the materialized validation

            split.'
        materialized_train_split:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Required. The path to the materialized train

            split.'
        training_schema_uri:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: Required. The path to the training schema.
        transform_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: Required. The path to transform output.
      parameters:
        accelerator_count:
          defaultValue: 0.0
          description: Accelerator count.
          isOptional: true
          parameterType: NUMBER_INTEGER
        accelerator_type:
          defaultValue: ''
          description: Accelerator type.
          isOptional: true
          parameterType: STRING
        disable_default_eval_metric:
          defaultValue: 0.0
          description: 'Flag to disable default metric. Set to >0 to

            disable. Default to 0.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        eval_metric:
          defaultValue: ''
          description: 'Evaluation metrics for validation data represented as a

            comma-separated string.'
          isOptional: true
          parameterType: STRING
        machine_type:
          defaultValue: c2-standard-16
          description: Machine type.
          isOptional: true
          parameterType: STRING
        objective:
          description: Required. Specifies the learning task and the learning objective.
          parameterType: STRING
        seed:
          defaultValue: 0.0
          description: Random seed.
          isOptional: true
          parameterType: NUMBER_INTEGER
        seed_per_iteration:
          defaultValue: false
          description: Seed PRNG determnisticly via iterator number.
          isOptional: true
          parameterType: BOOLEAN
        target_column:
          description: Required. Target column name.
          parameterType: STRING
        total_replica_count:
          description: Number of workers.
          parameterType: NUMBER_INTEGER
        weight_column:
          defaultValue: ''
          description: Weight column name.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        job_dir:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        instance_schema_path:
          parameterType: STRING
        instance_schema_uri:
          parameterType: STRING
        prediction_docker_uri_artifact_path:
          parameterType: STRING
        prediction_docker_uri_output:
          parameterType: STRING
        prediction_schema_path:
          parameterType: STRING
        prediction_schema_uri:
          parameterType: STRING
        trials:
          parameterType: STRING
        trials_path:
          parameterType: STRING
        worker_pool_specs:
          parameterType: LIST
  comp-get-best-hyperparameter-tuning-job-trial:
    executorLabel: exec-get-best-hyperparameter-tuning-job-trial
    inputDefinitions:
      parameters:
        gcp_resources:
          description: Proto tracking the hyperparameter tuning job.
          parameterType: STRING
        instance_schema_uri:
          defaultValue: ''
          description: The instance schema uri.
          isOptional: true
          parameterType: STRING
        prediction_docker_uri:
          defaultValue: ''
          description: The prediction docker container uri.
          isOptional: true
          parameterType: STRING
        prediction_schema_uri:
          defaultValue: ''
          description: The prediction schema_uri.
          isOptional: true
          parameterType: STRING
        read_value_from_file:
          defaultValue: false
          description: If true, read file to get the relevant value.
          isOptional: true
          parameterType: BOOLEAN
        study_spec_metric_goal:
          description: 'Optimization goal of the metric, possible values:

            "MAXIMIZE", "MINIMIZE".'
          parameterType: STRING
        trials_dir:
          defaultValue: ''
          description: The path to the hyperparameter tuning trials.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        unmanaged_container_model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-get-prediction-type-for-xgboost:
    executorLabel: exec-get-prediction-type-for-xgboost
    inputDefinitions:
      parameters:
        objective:
          description: The XGBoost training objective
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-xgboost-study-spec-parameters:
    executorLabel: exec-get-xgboost-study-spec-parameters
    inputDefinitions:
      parameters:
        study_spec_parameters_override:
          description: 'List of dictionaries representing parameters

            to optimize. The dictionary key is the parameter_id, which is passed to

            training job as a command line argument, and the dictionary value is the

            parameter specification of the metric.'
          parameterType: LIST
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-model-batch-predict:
    executorLabel: exec-model-batch-predict
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          isOptional: true
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          isOptional: true
      parameters:
        accelerator_count:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        accelerator_type:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        bigquery_destination_output_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        bigquery_source_input_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        explanation_metadata:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        gcs_destination_output_uri_prefix:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        gcs_source_uris:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        generate_explanation:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        instances_format:
          defaultValue: jsonl
          isOptional: true
          parameterType: STRING
        job_display_name:
          parameterType: STRING
        labels:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        machine_type:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        manual_batch_tuning_parameters_batch_size:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_replica_count:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_parameters:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        predictions_format:
          defaultValue: jsonl
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
        starting_replica_count:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        batchpredictionjob:
          artifactType:
            schemaTitle: google.VertexBatchPredictionJob
            schemaVersion: 0.0.1
        bigquery_output_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
        gcs_output_directory:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-model-evaluation:
    executorLabel: exec-model-evaluation
    inputDefinitions:
      artifacts:
        batch_prediction_job:
          artifactType:
            schemaTitle: google.VertexBatchPredictionJob
            schemaVersion: 0.0.1
      parameters:
        dataflow_disk_size:
          defaultValue: 50.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_machine_type:
          defaultValue: n1-standard-4
          isOptional: true
          parameterType: STRING
        dataflow_max_workers_num:
          defaultValue: 100.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_service_account:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_subnetwork:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_use_public_ips:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        dataflow_workers_num:
          defaultValue: 10.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        example_weight_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        ground_truth_column:
          parameterType: STRING
        ground_truth_format:
          defaultValue: jsonl
          isOptional: true
          parameterType: STRING
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        prediction_id_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        prediction_label_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        prediction_score_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        predictions_format:
          defaultValue: jsonl
          isOptional: true
          parameterType: STRING
        problem_type:
          parameterType: STRING
        project:
          parameterType: STRING
        root_dir:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        evaluation_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-model-upload:
    executorLabel: exec-model-upload
    inputDefinitions:
      artifacts:
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          isOptional: true
      parameters:
        description:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        display_name:
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        explanation_metadata:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-set-optional-inputs:
    executorLabel: exec-set-optional-inputs
    inputDefinitions:
      artifacts:
        vertex_dataset:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The Vertex dataset when data source is Vertex dataset.
      parameters:
        data_source_bigquery_table_path:
          description: The BigQuery table when data source is BQ.
          parameterType: STRING
        data_source_csv_filenames:
          description: The CSV GCS path when data source is CSV.
          parameterType: STRING
        location:
          description: The GCP region that runs the pipeline components.
          parameterType: STRING
        model_display_name:
          description: The uploaded model's display name.
          parameterType: STRING
        project:
          description: The GCP project that runs the pipeline components.
          parameterType: STRING
    outputDefinitions:
      parameters:
        data_source_bigquery_table_path:
          parameterType: STRING
        data_source_csv_filenames:
          parameterType: STRING
        model_display_name:
          parameterType: STRING
  comp-split-materialized-data:
    executorLabel: exec-split-materialized-data
    inputDefinitions:
      artifacts:
        materialized_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        materialized_eval_split:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        materialized_test_split:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        materialized_train_split:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-training-configurator-and-validator:
    executorLabel: exec-training-configurator-and-validator
    inputDefinitions:
      artifacts:
        dataset_stats:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Dataset stats generated by

            feature transform engine.'
        instance_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: 'Schema of input data to the tf_model at

            serving time.'
        training_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        available_at_forecast_columns:
          defaultValue: []
          description: The names of the columns that are available at forecast time.
          isOptional: true
          parameterType: LIST
        context_window:
          defaultValue: -1.0
          description: The length of the context window.
          isOptional: true
          parameterType: NUMBER_INTEGER
        enable_probabilistic_inference:
          defaultValue: false
          description: 'If probabilistic inference is

            enabled, the model will fit a distribution that captures the uncertainty

            of a prediction. At inference time, the predictive distribution is used

            to make a point prediction that minimizes the optimization objective.

            For example, the mean of a predictive distribution is the point

            prediction that minimizes RMSE loss. If quantiles are specified, then

            the quantiles of the distribution are also returned.'
          isOptional: true
          parameterType: BOOLEAN
        forecast_horizon:
          defaultValue: -1.0
          description: The length of the forecast horizon.
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_model_type:
          defaultValue: ''
          description: The model types, e.g. l2l, seq2seq, tft.
          isOptional: true
          parameterType: STRING
        forecasting_transformations:
          defaultValue: {}
          description: 'Dict mapping auto and/or type-resolutions to feature columns.
            The

            supported types are auto, categorical, numeric, text, and timestamp.'
          isOptional: true
          parameterType: STRUCT
        group_columns:
          description: 'A list of time series attribute column

            names that define the time series hierarchy.'
          isOptional: true
          parameterType: LIST
        group_temporal_total_weight:
          defaultValue: 0.0
          description: 'The weight of the loss for

            predictions aggregated over both the horizon and time series in the same

            hierarchy group.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        group_total_weight:
          defaultValue: 0.0
          description: 'The weight of the loss for

            predictions aggregated over time series in the same group.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        optimization_objective:
          defaultValue: ''
          description: "Objective function the model is optimizing\ntowards. The training\
            \ process creates a model that maximizes/minimizes\nthe value of the objective\
            \ function over the validation set. The\nsupported optimization objectives\
            \ depend on the prediction type. If the\nfield is not set, a default objective\
            \ function is used.\n  classification (binary): \"maximize-au-roc\" (default)\
            \ - Maximize the\n    area under the receiver operating characteristic\
            \ (ROC) curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
            \ - Maximize recall for a\n    specified precision value.\n  classification\
            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
          isOptional: true
          parameterType: STRING
        optimization_objective_precision_value:
          defaultValue: -1.0
          description: 'Required when

            optimization_objective is "maximize-recall-at-precision". Must be

            between 0 and 1, inclusive.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        optimization_objective_recall_value:
          defaultValue: -1.0
          description: 'Required when

            optimization_objective is "maximize-precision-at-recall". Must be

            between 0 and 1, inclusive.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        prediction_type:
          defaultValue: ''
          description: 'Model prediction type. One of "classification",

            "regression", "time_series".'
          isOptional: true
          parameterType: STRING
        quantiles:
          defaultValue: []
          description: All quantiles that the model need to predict.
          isOptional: true
          parameterType: LIST
        run_distill:
          defaultValue: false
          description: 'Whether the distillation should be applied to the

            training.'
          isOptional: true
          parameterType: BOOLEAN
        run_evaluation:
          defaultValue: false
          description: 'Whether we are running evaluation in the training

            pipeline.'
          isOptional: true
          parameterType: BOOLEAN
        split_example_counts:
          description: 'JSON string of data split example counts for

            train, validate, and test splits.'
          parameterType: STRING
        stage_1_deadline_hours:
          description: 'Stage 1 training budget in

            hours.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        stage_2_deadline_hours:
          description: 'Stage 2 training budget in

            hours.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        target_column:
          defaultValue: ''
          description: Target column of input data.
          isOptional: true
          parameterType: STRING
        temporal_total_weight:
          defaultValue: 0.0
          description: 'The weight of the loss for

            predictions aggregated over the horizon for a single time series.'
          isOptional: true
          parameterType: NUMBER_DOUBLE
        time_column:
          defaultValue: ''
          description: 'The column that indicates the time. Used by forecasting

            only.'
          isOptional: true
          parameterType: STRING
        time_series_attribute_columns:
          defaultValue: []
          description: The column names of the time series attributes.
          isOptional: true
          parameterType: LIST
        time_series_identifier_column:
          defaultValue: ''
          description: 'Time series idenfier column. Used by

            forecasting only.'
          isOptional: true
          parameterType: STRING
        unavailable_at_forecast_columns:
          defaultValue: []
          description: The names of the columns that are not available at forecast
            time.
          isOptional: true
          parameterType: LIST
        weight_column:
          defaultValue: ''
          description: Weight column of input data.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        instance_baseline:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        metadata:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: The tabular example gen metadata.
  comp-xgboost-hyperparameter-tuning-job:
    executorLabel: exec-xgboost-hyperparameter-tuning-job
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          defaultValue: ''
          description: The KMS key name.
          isOptional: true
          parameterType: STRING
        location:
          description: 'Required. The GCP region that runs the pipeline

            components.'
          parameterType: STRING
        max_failed_trial_count:
          defaultValue: 0.0
          description: 'The number of failed trials that

            need to be seen before failing the HyperparameterTuningJob. If set to
            0,

            Vertex AI decides how many trials must fail before the whole job fails.'
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_trial_count:
          description: Required. The desired total number of trials.
          parameterType: NUMBER_INTEGER
        parallel_trial_count:
          description: 'Required. The desired number of trials to run

            in parallel.'
          parameterType: NUMBER_INTEGER
        project:
          description: 'Required. The GCP project that runs the pipeline

            components.'
          parameterType: STRING
        study_spec_algorithm:
          defaultValue: ALGORITHM_UNSPECIFIED
          description: 'The search algorithm specified for

            the study. One of ''ALGORITHM_UNSPECIFIED'', ''GRID_SEARCH'', or

            ''RANDOM_SEARCH''.'
          isOptional: true
          parameterType: STRING
        study_spec_measurement_selection_type:
          defaultValue: BEST_MEASUREMENT
          description: 'Which measurement

            to use if/when the service automatically selects the final measurement

            from previously reported intermediate measurements. One of

            "BEST_MEASUREMENT" or "LAST_MEASUREMENT".'
          isOptional: true
          parameterType: STRING
        study_spec_metric_goal:
          description: 'Required. Optimization goal of the metric,

            possible values: "MAXIMIZE", "MINIMIZE".'
          parameterType: STRING
        study_spec_metric_id:
          description: 'Required. Metric to optimize. For options,

            please look under ''eval_metric'' at

            https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters.'
          parameterType: STRING
        study_spec_parameters_override:
          description: 'List of dictionaries

            representing parameters to optimize. The dictionary key is the

            parameter_id, which is passed to training job as a command line

            argument, and the dictionary value is the parameter specification of the

            metric.'
          parameterType: LIST
        worker_pool_specs:
          description: The worker pool specs.
          parameterType: LIST
    outputDefinitions:
      parameters:
        gcp_resources:
          description: Serialized gcp_resources proto tracking the custom training
            job.
          parameterType: STRING
deploymentSpec:
  executors:
    exec-automl-tabular-finalizer:
      container:
        args:
        - --type
        - CustomJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --payload
        - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
          \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
          {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20230605_0125", "\",
          \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
          "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.custom_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
    exec-bool-identity:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - _bool_identity
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-beta.17'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef _bool_identity(value: bool) -> str:\n  \"\"\"Returns boolean\
          \ value.\n\n  Args:\n    value: Boolean value to return\n\n  Returns:\n\
          \    Boolean value.\n  \"\"\"\n  return 'true' if value else 'false'\n\n"
        image: python:3.7
    exec-feature-transform-engine:
      container:
        args:
        - feature_transform_engine
        - '{"Concat": ["--project=", "{{$.inputs.parameters[''project'']}}"]}'
        - '{"Concat": ["--location=", "{{$.inputs.parameters[''location'']}}"]}'
        - '{"Concat": ["--dataset_level_custom_transformation_definitions=", "{{$.inputs.parameters[''dataset_level_custom_transformation_definitions'']}}"]}'
        - '{"Concat": ["--dataset_level_transformations=", "{{$.inputs.parameters[''dataset_level_transformations'']}}"]}'
        - '{"Concat": ["--forecasting_time_column=", "{{$.inputs.parameters[''forecasting_time_column'']}}"]}'
        - '{"Concat": ["--forecasting_time_series_identifier_column=", "{{$.inputs.parameters[''forecasting_time_series_identifier_column'']}}"]}'
        - '{"Concat": ["--forecasting_time_series_attribute_columns=", "{{$.inputs.parameters[''forecasting_time_series_attribute_columns'']}}"]}'
        - '{"Concat": ["--forecasting_unavailable_at_forecast_columns=", "{{$.inputs.parameters[''forecasting_unavailable_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--forecasting_available_at_forecast_columns=", "{{$.inputs.parameters[''forecasting_available_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--forecasting_forecast_horizon=", "{{$.inputs.parameters[''forecasting_forecast_horizon'']}}"]}'
        - '{"Concat": ["--forecasting_context_window=", "{{$.inputs.parameters[''forecasting_context_window'']}}"]}'
        - '{"Concat": ["--forecasting_predefined_window_column=", "{{$.inputs.parameters[''forecasting_predefined_window_column'']}}"]}'
        - '{"Concat": ["--forecasting_window_stride_length=", "{{$.inputs.parameters[''forecasting_window_stride_length'']}}"]}'
        - '{"Concat": ["--forecasting_window_max_count=", "{{$.inputs.parameters[''forecasting_window_max_count'']}}"]}'
        - '{"Concat": ["--forecasting_apply_windowing=", "{{$.inputs.parameters[''forecasting_apply_windowing'']}}"]}'
        - '{"Concat": ["--predefined_split_key=", "{{$.inputs.parameters[''predefined_split_key'']}}"]}'
        - '{"Concat": ["--stratified_split_key=", "{{$.inputs.parameters[''stratified_split_key'']}}"]}'
        - '{"Concat": ["--timestamp_split_key=", "{{$.inputs.parameters[''timestamp_split_key'']}}"]}'
        - '{"Concat": ["--training_fraction=", "{{$.inputs.parameters[''training_fraction'']}}"]}'
        - '{"Concat": ["--validation_fraction=", "{{$.inputs.parameters[''validation_fraction'']}}"]}'
        - '{"Concat": ["--test_fraction=", "{{$.inputs.parameters[''test_fraction'']}}"]}'
        - '{"Concat": ["--tf_transform_execution_engine=", "{{$.inputs.parameters[''tf_transform_execution_engine'']}}"]}'
        - '{"IfPresent": {"InputName": "tf_auto_transform_features", "Then": {"Concat":
          ["--tf_auto_transform_features=", "{{$.inputs.parameters[''tf_auto_transform_features'']}}"]}}}'
        - '{"Concat": ["--tf_custom_transformation_definitions=", "{{$.inputs.parameters[''tf_custom_transformation_definitions'']}}"]}'
        - '{"Concat": ["--tf_transformations_path=", "{{$.inputs.parameters[''tf_transformations_path'']}}"]}'
        - '{"Concat": ["--legacy_transformations_path=", "{{$.inputs.parameters[''legacy_transformations_path'']}}"]}'
        - '{"Concat": ["--data_source_csv_filenames=", "{{$.inputs.parameters[''data_source_csv_filenames'']}}"]}'
        - '{"Concat": ["--data_source_bigquery_table_path=", "{{$.inputs.parameters[''data_source_bigquery_table_path'']}}"]}'
        - '{"Concat": ["--bigquery_staging_full_dataset_id=", "{{$.inputs.parameters[''bigquery_staging_full_dataset_id'']}}"]}'
        - '{"Concat": ["--target_column=", "{{$.inputs.parameters[''target_column'']}}"]}'
        - '{"Concat": ["--weight_column=", "{{$.inputs.parameters[''weight_column'']}}"]}'
        - '{"Concat": ["--prediction_type=", "{{$.inputs.parameters[''prediction_type'']}}"]}'
        - '{"IfPresent": {"InputName": "model_type", "Then": {"Concat": ["--model_type=",
          "{{$.inputs.parameters[''model_type'']}}"]}}}'
        - '{"Concat": ["--run_distill=", "{{$.inputs.parameters[''run_distill'']}}"]}'
        - '{"Concat": ["--run_feature_selection=", "{{$.inputs.parameters[''run_feature_selection'']}}"]}'
        - '{"Concat": ["--materialized_examples_format=", "{{$.inputs.parameters[''materialized_examples_format'']}}"]}'
        - '{"Concat": ["--max_selected_features=", "{{$.inputs.parameters[''max_selected_features'']}}"]}'
        - '{"Concat": ["--feature_selection_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/feature_selection_staging_dir"]}'
        - '{"Concat": ["--feature_selection_algorithm=", "{{$.inputs.parameters[''feature_selection_algorithm'']}}"]}'
        - '{"Concat": ["--feature_ranking_path=", "{{$.outputs.artifacts[''feature_ranking''].uri}}"]}'
        - '{"Concat": ["--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.txt"]}'
        - '{"Concat": ["--stats_result_path=", "{{$.outputs.artifacts[''dataset_stats''].uri}}"]}'
        - '{"Concat": ["--transform_output_artifact_path=", "{{$.outputs.artifacts[''transform_output''].uri}}"]}'
        - '{"Concat": ["--transform_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform"]}'
        - '{"Concat": ["--materialized_examples_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/materialized"]}'
        - '{"Concat": ["--export_data_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/export"]}'
        - '{"Concat": ["--materialized_data_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/materialized_data"]}'
        - '{"Concat": ["--materialized_data_artifact_path=", "{{$.outputs.artifacts[''materialized_data''].uri}}"]}'
        - '{"Concat": ["--bigquery_train_split_uri_path=", "{{$.outputs.parameters[''bigquery_train_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_validation_split_uri_path=", "{{$.outputs.parameters[''bigquery_validation_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_test_split_uri_path=", "{{$.outputs.parameters[''bigquery_test_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_downsampled_test_split_uri_path=", "{{$.outputs.parameters[''bigquery_downsampled_test_split_uri''].output_file}}"]}'
        - '{"Concat": ["--split_example_counts_path=", "{{$.outputs.parameters[''split_example_counts''].output_file}}"]}'
        - '{"Concat": ["--instance_schema_path=", "{{$.outputs.artifacts[''instance_schema''].path}}"]}'
        - '{"Concat": ["--training_schema_path=", "{{$.outputs.artifacts[''training_schema''].path}}"]}'
        - --job_name=feature-transform-engine-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
        - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
        - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
        - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
        - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
        - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20230605_0125
        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230605_0125
        - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
        - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
        - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
        - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
        - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
        - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
        - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
        - '{"IfPresent": {"InputName": "group_columns", "Then": {"Concat": ["--group_columns=",
          "{{$.inputs.parameters[''group_columns'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
          "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
          ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
          ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230605_0125
        resources:
          cpuLimit: 8.0
          memoryLimit: 30.0
    exec-generate-xgboost-hyperparameter-tuning-worker-pool-specs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - _generate_xgboost_hyperparameter_tuning_worker_pool_specs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-beta.17'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef _generate_xgboost_hyperparameter_tuning_worker_pool_specs(\n\
          \    total_replica_count: int,\n    target_column: str,\n    objective:\
          \ str,\n    materialized_train_split: dsl.InputPath('MaterializedSplit'),\n\
          \    materialized_eval_split: dsl.InputPath('MaterializedSplit'),\n    transform_output:\
          \ dsl.InputPath('TransformOutput'),\n    training_schema_uri: dsl.InputPath('DatasetSchema'),\n\
          \    instance_baseline: dsl.InputPath('AutoMLTabularInstanceBaseline'),\n\
          \    job_dir: OutputPath('JobDir'),\n    instance_schema_uri: OutputPath(str),\n\
          \    prediction_schema_uri: OutputPath(str),\n    trials: OutputPath(str),\n\
          \    prediction_docker_uri_output: OutputPath(str),\n    machine_type: str\
          \ = 'c2-standard-16',\n    accelerator_type: str = '',\n    accelerator_count:\
          \ int = 0,\n    weight_column: str = '',\n    eval_metric: str = '',\n \
          \   disable_default_eval_metric: int = 0,\n    seed: int = 0,\n    seed_per_iteration:\
          \ bool = False,\n) -> NamedTuple(\n    'Outputs',\n    [\n        ('worker_pool_specs',\
          \ list),  # pylint:disable=g-bare-generic\n        ('instance_schema_path',\
          \ str),\n        ('prediction_schema_path', str),\n        ('trials_path',\
          \ str),\n        ('prediction_docker_uri_artifact_path', str),\n    ],\n\
          ):\n  \"\"\"Generates worker pool specs for XGBoost hyperparameter tuning.\n\
          \n  For single machine XGBoost training, returns one worker pool spec for\
          \ master.\n  For distributed XGBoost training, returns two worker pool specs,\
          \ the first one\n  for master and the second one for the remaining workers.\n\
          \n  Args:\n    total_replica_count: Number of workers.\n    target_column:\
          \ Required. Target column name.\n    objective: Required. Specifies the\
          \ learning task and the learning objective.\n    materialized_train_split:\
          \ Required. The path to the materialized train\n      split.\n    materialized_eval_split:\
          \ Required. The path to the materialized validation\n      split.\n    transform_output:\
          \ Required. The path to transform output.\n    training_schema_uri: Required.\
          \ The path to the training schema.\n    instance_baseline: Path to JSON\
          \ file for baseline values.\n    job_dir: Job dir path.\n    instance_schema_uri:\
          \ The instance schema uri.\n    prediction_schema_uri: The prediction schema_uri.\n\
          \    trials: The trials uri.\n    prediction_docker_uri_output: The prediction\
          \ docker container uri.\n    machine_type: Machine type.\n    accelerator_type:\
          \ Accelerator type.\n    accelerator_count: Accelerator count.\n    weight_column:\
          \ Weight column name.\n    eval_metric: Evaluation metrics for validation\
          \ data represented as a\n      comma-separated string.\n    disable_default_eval_metric:\
          \ Flag to disable default metric. Set to >0 to\n      disable. Default to\
          \ 0.\n    seed: Random seed.\n    seed_per_iteration: Seed PRNG determnisticly\
          \ via iterator number.\n\n  Raises:\n    ValueError: If accelerator_count\
          \ <= 0 and accelerator_type is specified.\n\n  Returns:\n    Output parameters.\n\
          \  \"\"\"\n  import copy\n  import collections\n  import re\n\n  def get_gcs_path(path):\n\
          \    return re.sub(r'^/gcs/', r'gs://', path)\n\n  master_worker_pool_spec\
          \ = {\n      'replica_count': 1,\n      'machine_spec': {\n          'machine_type':\
          \ machine_type,\n      },\n      'container_spec': {\n          'image_uri':\
          \ 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/xgboost-training:20230605_0125',\n\
          \          'args': [\n              f'--job_dir={get_gcs_path(job_dir)}',\n\
          \              f'--instance_schema_path={get_gcs_path(instance_schema_uri)}',\n\
          \              f'--prediction_schema_path={get_gcs_path(prediction_schema_uri)}',\n\
          \              f'--trials_path={get_gcs_path(trials)}',\n              f'--prediction_docker_uri_artifact_path={get_gcs_path(prediction_docker_uri_output)}',\n\
          \              f'--target_column={target_column}',\n              f'--objective={objective}',\n\
          \              f'--training_data_path={get_gcs_path(materialized_train_split)}',\n\
          \              f'--validation_data_path={get_gcs_path(materialized_eval_split)}',\n\
          \              f'--transform_output_path={get_gcs_path(transform_output)}',\n\
          \              f'--training_schema_path={get_gcs_path(training_schema_uri)}',\n\
          \              f'--baseline_path={get_gcs_path(instance_baseline)}',\n \
          \             f'--eval_metric={eval_metric}',\n              f'--disable_default_eval_metric={disable_default_eval_metric}',\n\
          \              f'--seed={seed}',\n              f'--seed_per_iteration={seed_per_iteration}',\n\
          \              '--prediction_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/xgboost-prediction-server:20230605_0125',\n\
          \          ],\n      },\n  }\n\n  # Add optional arguments if set\n  if\
          \ weight_column:\n    master_worker_pool_spec['container_spec']['args'].append(\n\
          \        f'--weight_column={weight_column}'\n    )\n\n  # Add accelerator_type\
          \ and accelerator_count if set.\n  if accelerator_type:\n    if accelerator_count\
          \ <= 0:\n      raise ValueError(\n          'Accelerator count must be greator\
          \ than 0 when type is specified.'\n      )\n    master_worker_pool_spec['machine_spec'][\n\
          \        'accelerator_type'\n    ] = accelerator_type\n    master_worker_pool_spec['machine_spec'][\n\
          \        'accelerator_count'\n    ] = accelerator_count\n\n  worker_pool_specs_lst\
          \ = [master_worker_pool_spec]\n\n  # Add an additional worker pool spec\
          \ for distributed training.\n  if total_replica_count > 1:\n    additional_replica\
          \ = total_replica_count - 1\n    additional_worker_spec = copy.deepcopy(master_worker_pool_spec)\n\
          \    additional_worker_spec['replica_count'] = additional_replica\n    worker_pool_specs_lst.append(additional_worker_spec)\n\
          \n  return collections.namedtuple(\n      'Outputs',\n      [\n        \
          \  'worker_pool_specs',\n          'instance_schema_path',\n          'prediction_schema_path',\n\
          \          'trials_path',\n          'prediction_docker_uri_artifact_path',\n\
          \      ],\n  )(\n      worker_pool_specs_lst,\n      get_gcs_path(instance_schema_uri),\n\
          \      get_gcs_path(prediction_schema_uri),\n      get_gcs_path(trials),\n\
          \      get_gcs_path(prediction_docker_uri_output),\n  )\n\n"
        image: python:3.7
    exec-get-best-hyperparameter-tuning-job-trial:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - _get_best_hyperparameter_tuning_job_trial
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-beta.17'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef _get_best_hyperparameter_tuning_job_trial(\n    gcp_resources:\
          \ str,\n    study_spec_metric_goal: str,\n    unmanaged_container_model:\
          \ dsl.Output[dsl.Artifact],\n    trials_dir: str = '',\n    instance_schema_uri:\
          \ str = '',\n    prediction_schema_uri: str = '',\n    prediction_docker_uri:\
          \ str = '',\n    read_value_from_file: bool = False,\n):\n  \"\"\"Gets best\
          \ HyperparameterTuningJob trial.\n\n  Args:\n    gcp_resources: Proto tracking\
          \ the hyperparameter tuning job.\n    study_spec_metric_goal: Optimization\
          \ goal of the metric, possible values:\n      \"MAXIMIZE\", \"MINIMIZE\"\
          .\n    unmanaged_container_model: The unmanaged model.\n    trials_dir:\
          \ The path to the hyperparameter tuning trials.\n    instance_schema_uri:\
          \ The instance schema uri.\n    prediction_schema_uri: The prediction schema_uri.\n\
          \    prediction_docker_uri: The prediction docker container uri.\n    read_value_from_file:\
          \ If true, read file to get the relevant value.\n\n  Raises:\n    RuntimeError:\
          \ If there are multiple metrics.\n  \"\"\"\n\n  import os\n  import json\n\
          \  from google.api_core.retry import Retry\n  from google.cloud import aiplatform_v1beta1\
          \ as aip\n  import tensorflow as tf\n\n  # If path to file with value is\
          \ provided, read the file before continuing.\n  if read_value_from_file:\n\
          \    with tf.io.gfile.GFile(trials_dir, 'r') as f:\n      trials_dir = f.read()\n\
          \    with tf.io.gfile.GFile(instance_schema_uri, 'r') as f:\n      instance_schema_uri\
          \ = f.read()\n    with tf.io.gfile.GFile(prediction_schema_uri, 'r') as\
          \ f:\n      prediction_schema_uri = f.read()\n    with tf.io.gfile.GFile(prediction_docker_uri,\
          \ 'r') as f:\n      prediction_docker_uri = f.read()\n\n  api_endpoint_suffix\
          \ = '-aiplatform.googleapis.com'\n  gcp_resources_json = json.loads(gcp_resources)\n\
          \  resource = gcp_resources_json['resources'][0]\n\n  uri_key = 'resource_uri'\n\
          \  if uri_key not in resource:\n    uri_key = 'resourceUri'\n\n  gcp_resources_split\
          \ = resource[uri_key].partition('projects')\n  resource_name = gcp_resources_split[1]\
          \ + gcp_resources_split[2]\n  prefix_str = gcp_resources_split[0]\n  prefix_str\
          \ = prefix_str[: prefix_str.find(api_endpoint_suffix)]\n  api_endpoint =\
          \ (\n      prefix_str[(prefix_str.rfind('//') + 2) :] + api_endpoint_suffix\n\
          \  )\n\n  job_client = aip.JobServiceClient(\n      client_options={'api_endpoint':\
          \ api_endpoint}\n  )\n  response = job_client.get_hyperparameter_tuning_job(\n\
          \      name=resource_name,\n      retry=Retry(initial=10.0, maximum=60.0,\
          \ deadline=10.0 * 60.0),\n  )\n\n  # Get best trial\n  trials_list = []\n\
          \  for trial in response.trials:\n    if trial.final_measurement:\n    \
          \  trials_list.append({\n          'id': trial.id,\n          'objective_value':\
          \ trial.final_measurement.metrics[0].value,\n      })\n\n  best_trial =\
          \ None\n  best_fn = None\n  if study_spec_metric_goal == 'MAXIMIZE':\n \
          \   best_fn = max\n  elif study_spec_metric_goal == 'MINIMIZE':\n    best_fn\
          \ = min\n  else:\n    raise ValueError(\n        f'Unexpected study spec\
          \ metric goal: {study_spec_metric_goal}'\n    )\n\n  best_trial = best_fn(trials_list,\
          \ key=lambda trial: trial['objective_value'])\n\n  # Build unmanaged_container_model\n\
          \  unmanaged_container_model.metadata['containerSpec'] = {\n      'imageUri':\
          \ prediction_docker_uri,\n      'healthRoute': '/health',\n      'predictRoute':\
          \ '/predict',\n  }\n  unmanaged_container_model.metadata['predictSchemata']\
          \ = {\n      'instanceSchemaUri': instance_schema_uri,\n      'predictionSchemaUri':\
          \ prediction_schema_uri,\n  }\n  unmanaged_container_model.uri = os.path.join(\n\
          \      trials_dir, 'trial_{}'.format(best_trial['id']), 'model'\n  )\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230605_0125
    exec-get-prediction-type-for-xgboost:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - _get_prediction_type_for_xgboost
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-beta.17'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef _get_prediction_type_for_xgboost(objective: str) -> str:\n  \"\
          \"\"Returns prediction_type given XGBoost training objective..\n\n  Args:\n\
          \    objective: The XGBoost training objective\n\n  Returns:\n    A string.\
          \ One of 'regression' or 'classification'\n  \"\"\"\n  if objective.startswith('binary')\
          \ or objective.startswith('multi'):\n    return 'classification'\n  elif\
          \ objective.startswith('reg'):\n    return 'regression'\n  else:\n    raise\
          \ ValueError(\n        f'Unsupported XGBoost training objective: {objective}.\
          \ Must be one of'\n        ' [reg:squarederror, reg:squaredlogerror, reg:logistic,\
          \ reg:gamma,'\n        ' reg:tweedie, reg:pseudohubererror, binary:logistic,'\n\
          \        ' multi:softprob].'\n    )\n\n"
        image: python:3.7
    exec-get-xgboost-study-spec-parameters:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_xgboost_study_spec_parameters
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-beta.17'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_xgboost_study_spec_parameters(\n    study_spec_parameters_override:\
          \ list,  # Required for KFP validation; pylint:disable=g-bare-generic,unused-argument\n\
          ) -> list:  # Required for KFP validation; pylint:disable=g-bare-generic\n\
          \  \"\"\"Get study_spec_parameters for an XGBoost hyperparameter tuning\
          \ job.\n\n  Args:\n    study_spec_parameters_override: List of dictionaries\
          \ representing parameters\n      to optimize. The dictionary key is the\
          \ parameter_id, which is passed to\n      training job as a command line\
          \ argument, and the dictionary value is the\n      parameter specification\
          \ of the metric.\n\n  Returns:\n    List of final Vizier study_spec_parameters\
          \ of type ParameterSpec.\n  \"\"\"\n  # pylint:disable=g-import-not-at-top,redefined-outer-name,reimported\n\
          \  import functools\n  import math\n  from typing import Any, Dict, List,\
          \ Optional\n  # pylint:enable=g-import-not-at-top,redefined-outer-name,reimported\n\
          \n  # Need to define constants within the component function\n  # pylint:disable=invalid-name\n\
          \  _GBTREE_BOOSTER = 'gbtree'\n  _GBLINEAR_BOOSTER = 'gblinear'\n  _DART_BOOSTER\
          \ = 'dart'\n  _XGBOOST_BOOSTER_PARAMETERS_MAP = {\n      'eta': [_GBTREE_BOOSTER,\
          \ _DART_BOOSTER],\n      'gamma': [_GBTREE_BOOSTER, _DART_BOOSTER],\n  \
          \    'max_depth': [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'min_child_weight':\
          \ [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'max_delta_step': [_GBTREE_BOOSTER,\
          \ _DART_BOOSTER],\n      'subsample': [_GBTREE_BOOSTER, _DART_BOOSTER],\n\
          \      'colsample_bytree': [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'colsample_bylevel':\
          \ [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'colsample_bynode': [_GBTREE_BOOSTER,\
          \ _DART_BOOSTER],\n      'lambda': [_GBTREE_BOOSTER, _DART_BOOSTER, _GBLINEAR_BOOSTER],\n\
          \      'alpha': [_GBTREE_BOOSTER, _DART_BOOSTER, _GBLINEAR_BOOSTER],\n \
          \     'tree_method': [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'scale_pos_weight':\
          \ [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'updater': [_GBTREE_BOOSTER,\
          \ _DART_BOOSTER, _GBLINEAR_BOOSTER],\n      'refresh_leaf': [_GBTREE_BOOSTER,\
          \ _DART_BOOSTER],\n      'process_type': [_GBTREE_BOOSTER, _DART_BOOSTER],\n\
          \      'grow_policy': [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'sampling_method':\
          \ [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'monotone_constraints': [_GBTREE_BOOSTER,\
          \ _DART_BOOSTER],\n      'interaction_constraints': [_GBTREE_BOOSTER, _DART_BOOSTER],\n\
          \      'sample_type': [_DART_BOOSTER],\n      'normalize_type': [_DART_BOOSTER],\n\
          \      'rate_drop': [_DART_BOOSTER],\n      'one_drop': [_DART_BOOSTER],\n\
          \      'skip_drop': [_DART_BOOSTER],\n      'num_parallel_tree': [_GBLINEAR_BOOSTER],\n\
          \      'feature_selector': [_GBLINEAR_BOOSTER],\n      'top_k': [_GBLINEAR_BOOSTER],\n\
          \      'max_leaves': [_GBTREE_BOOSTER, _DART_BOOSTER],\n      'max_bin':\
          \ [_GBTREE_BOOSTER, _DART_BOOSTER],\n  }\n  _XGBOOST_NO_DEFAULT_BOOSTER_PARAMS\
          \ = frozenset(\n      ['updater', 'monotone_constraints', 'interaction_constraints']\n\
          \  )\n\n  def _validate_float_spec(\n      parameter_spec: Dict[str, Any],\
          \ lower_bound: float, upper_bound: float\n  ) -> None:\n    msg = (\n  \
          \      f'Parameter spec for {parameter_spec[\"parameter_id\"]} must contain\
          \ '\n        'double_value_spec or discrete_value_spec with float values\
          \ within '\n        f'the range of {lower_bound} and {upper_bound} (inclusive)'\n\
          \    )\n    if 'double_value_spec' in parameter_spec:\n      float_spec\
          \ = parameter_spec['double_value_spec']\n      if float_spec['min_value']\
          \ < lower_bound:\n        raise ValueError(\n            f'{msg}, but got\
          \ {float_spec[\"min_value\"]} for min_value.'\n        )\n      if float_spec['max_value']\
          \ > upper_bound:\n        raise ValueError(\n            f'{msg}, but got\
          \ {float_spec[\"max_value\"]} for max_value.'\n        )\n    elif 'discrete_value_spec'\
          \ in parameter_spec:\n      float_spec = parameter_spec['discrete_value_spec']\n\
          \      float_values = float_spec['values']\n      for val in float_values:\n\
          \        if val < lower_bound or val > upper_bound:\n          raise ValueError(f'{msg},\
          \ but got {val} in {float_values}.')\n    else:\n      raise ValueError(\n\
          \          f'Unexpected value spec for {parameter_spec[\"parameter_id\"\
          ]}. {msg}.'\n      )\n\n  def _validate_int_spec(\n      parameter_spec:\
          \ Dict[str, Any],\n      lower_bound: Optional[int],\n      upper_bound:\
          \ Optional[int],\n  ) -> None:\n    msg = (\n        f'Parameter spec for\
          \ {parameter_spec[\"parameter_id\"]} must contain '\n        'integer_value_spec\
          \ or discrete_value_spec with integer values within '\n        f'the range\
          \ of {lower_bound} and {upper_bound} (inclusive)'\n    )\n    if 'integer_value_spec'\
          \ in parameter_spec:\n      int_spec = parameter_spec['integer_value_spec']\n\
          \      if lower_bound is not None and int_spec['min_value'] < lower_bound:\n\
          \        raise ValueError(\n            f'{msg}, but got {int_spec[\"min_value\"\
          ]} for min_value.'\n        )\n      if upper_bound is not None and int_spec['max_value']\
          \ > upper_bound:\n        raise ValueError(\n            f'{msg}, but got\
          \ {int_spec[\"max_value\"]} for max_value.'\n        )\n    elif 'discrete_value_spec'\
          \ in parameter_spec:\n      int_values = parameter_spec['discrete_value_spec']['values']\n\
          \      for val in int_values:\n        if not isinstance(val, int):\n  \
          \        raise ValueError(\n              f'{msg}, but got non-integer {val}\
          \ with '\n              f'type {type(val)} in {int_values}.'\n         \
          \ )\n        if (lower_bound is not None and val < lower_bound) or (\n \
          \           upper_bound is not None and val > upper_bound\n        ):\n\
          \          raise ValueError(f'{msg}, but got {val} in {int_values}.')\n\
          \    else:\n      raise ValueError(\n          f'Unexpected value spec for\
          \ {parameter_spec[\"parameter_id\"]}. {msg}.'\n      )\n\n  def _validate_categorical_spec(\n\
          \      parameter_spec: Dict[str, Any], valid_categories: Optional[List[str]]\n\
          \  ) -> None:\n    msg = (\n        f'Parameter spec for {parameter_spec[\"\
          parameter_id\"]} must contain '\n        'categorical_value_spec with unique\
          \ categories from '\n        f'{valid_categories}'\n    )\n    if 'categorical_value_spec'\
          \ in parameter_spec:\n      if valid_categories is None:\n        # Any\
          \ category is valid.\n        return\n      categorical_values = parameter_spec['categorical_value_spec']['values']\n\
          \      valid_categorical_values = set(categorical_values).intersection(\n\
          \          set(valid_categories)\n      )\n      if len(valid_categorical_values)\
          \ != len(categorical_values):\n        raise ValueError(f'{msg}, but got\
          \ {categorical_values}.')\n    else:\n      raise ValueError(\n        \
          \  f'Unexpected value spec for {parameter_spec[\"parameter_id\"]}. {msg}.'\n\
          \      )\n\n  _XGBOOST_PARAM_VALIDATIONS = {\n      'num_boost_round': functools.partial(\n\
          \          _validate_int_spec, lower_bound=1, upper_bound=None\n      ),\n\
          \      'early_stopping_rounds': functools.partial(\n          _validate_int_spec,\
          \ lower_bound=1, upper_bound=None\n      ),\n      'base_score': functools.partial(\n\
          \          _validate_float_spec, lower_bound=0, upper_bound=1\n      ),\n\
          \      'booster': functools.partial(\n          _validate_categorical_spec,\n\
          \          valid_categories=['gbtree', 'gblinear', 'dart'],\n      ),\n\
          \      'eta': functools.partial(\n          _validate_float_spec, lower_bound=0,\
          \ upper_bound=1\n      ),\n      'gamma': functools.partial(\n         \
          \ _validate_float_spec, lower_bound=0, upper_bound=math.inf\n      ),\n\
          \      'max_depth': functools.partial(\n          _validate_int_spec, lower_bound=0,\
          \ upper_bound=None\n      ),\n      'min_child_weight': functools.partial(\n\
          \          _validate_float_spec, lower_bound=0, upper_bound=math.inf\n \
          \     ),\n      'max_delta_step': functools.partial(\n          _validate_float_spec,\
          \ lower_bound=0, upper_bound=math.inf\n      ),\n      'subsample': functools.partial(\n\
          \          _validate_float_spec, lower_bound=0.0001, upper_bound=1\n   \
          \   ),\n      'colsample_bytree': functools.partial(\n          _validate_float_spec,\
          \ lower_bound=0.0001, upper_bound=1\n      ),\n      'colsample_bylevel':\
          \ functools.partial(\n          _validate_float_spec, lower_bound=0.0001,\
          \ upper_bound=1\n      ),\n      'colsample_bynode': functools.partial(\n\
          \          _validate_float_spec, lower_bound=0.0001, upper_bound=1\n   \
          \   ),\n      'lambda': functools.partial(\n          _validate_float_spec,\
          \ lower_bound=0, upper_bound=1\n      ),\n      'alpha': functools.partial(\n\
          \          _validate_float_spec, lower_bound=0, upper_bound=1\n      ),\n\
          \      'tree_method': functools.partial(\n          _validate_categorical_spec,\n\
          \          valid_categories=['auto', 'exact', 'approx', 'hist', 'gpu_hist'],\n\
          \      ),\n      'scale_pos_weight': functools.partial(\n          _validate_float_spec,\
          \ lower_bound=0, upper_bound=math.inf\n      ),\n      'updater': functools.partial(\n\
          \          _validate_categorical_spec, valid_categories=None\n      ),\n\
          \      'refresh_leaf': functools.partial(\n          _validate_int_spec,\
          \ lower_bound=0, upper_bound=1\n      ),\n      'process_type': functools.partial(\n\
          \          _validate_categorical_spec, valid_categories=['default', 'updated']\n\
          \      ),\n      'grow_policy': functools.partial(\n          _validate_categorical_spec,\n\
          \          valid_categories=['depthwise', 'lossguide'],\n      ),\n    \
          \  'sampling_method': functools.partial(\n          _validate_categorical_spec,\n\
          \          valid_categories=['uniform', 'gradient_based'],\n      ),\n \
          \     'monotone_constraints': functools.partial(\n          _validate_categorical_spec,\
          \ valid_categories=None\n      ),\n      'interaction_constraints': functools.partial(\n\
          \          _validate_categorical_spec, valid_categories=None\n      ),\n\
          \      'sample_type': functools.partial(\n          _validate_categorical_spec,\
          \ valid_categories=['uniform', 'weighted']\n      ),\n      'normalize_type':\
          \ functools.partial(\n          _validate_categorical_spec, valid_categories=['tree',\
          \ 'forest']\n      ),\n      'rate_drop': functools.partial(\n         \
          \ _validate_float_spec, lower_bound=0, upper_bound=1\n      ),\n      'one_drop':\
          \ functools.partial(\n          _validate_int_spec, lower_bound=0, upper_bound=1\n\
          \      ),\n      'skip_drop': functools.partial(\n          _validate_float_spec,\
          \ lower_bound=0, upper_bound=1\n      ),\n      'num_parallel_tree': functools.partial(\n\
          \          _validate_int_spec, lower_bound=1, upper_bound=None\n      ),\n\
          \      'feature_selector': functools.partial(\n          _validate_categorical_spec,\n\
          \          valid_categories=['cyclic', 'shuffle', 'random', 'greedy', 'thrifty'],\n\
          \      ),\n      'top_k': functools.partial(\n          _validate_int_spec,\
          \ lower_bound=0, upper_bound=None\n      ),\n      'max_cat_to_onehot':\
          \ functools.partial(\n          _validate_int_spec, lower_bound=0, upper_bound=None\n\
          \      ),\n      'max_leaves': functools.partial(\n          _validate_int_spec,\
          \ lower_bound=0, upper_bound=None\n      ),\n      'max_bin': functools.partial(\n\
          \          _validate_int_spec, lower_bound=0, upper_bound=None\n      ),\n\
          \  }\n\n  def _add_booster_param(\n      override_booster_params: Dict[str,\
          \ Any],\n      param: Dict[str, Any],\n      override_boosters: List[str],\n\
          \  ) -> None:\n    # Validate parameter spec.\n    param_id = param['parameter_spec']['parameter_id']\n\
          \    validation_func = _XGBOOST_PARAM_VALIDATIONS[param_id]\n    validation_func(param['parameter_spec'])\n\
          \    # Add parameter spec for valid boosters.\n    parent_boosters = param['parent_categorical_values']['values']\n\
          \    all_boosters = set(_XGBOOST_BOOSTER_PARAMETERS_MAP[param_id]).intersection(\n\
          \        set(override_boosters)\n    )\n    valid_parent_boosters = set(parent_boosters).intersection(all_boosters)\n\
          \    if valid_parent_boosters:\n      override_booster_params[param_id]\
          \ = {}\n      for booster in valid_parent_boosters:\n        override_booster_params[param_id][booster]\
          \ = param['parameter_spec']\n\n  def _get_booster_param_specs(\n      override_booster_params:\
          \ Dict[str, Any],\n      param_id: str,\n      default_param_spec: Optional[Dict[str,\
          \ Any]],\n  ) -> List[Dict[str, Any]]:\n    if param_id not in override_booster_params:\n\
          \      if default_param_spec is None:\n        return []\n      return [default_param_spec]\n\
          \    override_param_specs = override_booster_params[param_id]\n    if default_param_spec\
          \ is not None:\n      for booster in default_param_spec['parent_categorical_values']['values']:\n\
          \        if booster not in override_param_specs:\n          override_param_specs[booster]\
          \ = default_param_spec['parameter_spec']\n    param_specs = []\n    for\
          \ booster, override_spec in override_param_specs.items():\n      included\
          \ = False\n      for spec in param_specs:\n        if spec['parameter_spec']\
          \ == override_spec:\n          spec['parent_categorical_values']['values'].append(booster)\n\
          \          included = True\n          break\n      if not included:\n  \
          \      param_specs.append({\n            'parameter_spec': override_spec,\n\
          \            'parent_categorical_values': {'values': [booster]},\n     \
          \   })\n    return param_specs\n\n  default_params = [\n      {\n      \
          \    'parameter_id': 'num_boost_round',\n          'discrete_value_spec':\
          \ {'values': [1, 5, 10, 15, 20]},\n      },\n      {\n          'parameter_id':\
          \ 'early_stopping_rounds',\n          'discrete_value_spec': {'values':\
          \ [3, 5, 10]},\n      },\n      {'parameter_id': 'base_score', 'discrete_value_spec':\
          \ {'values': [0.5]}},\n      {\n          'parameter_id': 'booster',\n \
          \         'categorical_value_spec': {'values': ['gbtree', 'gblinear', 'dart']},\n\
          \          'conditional_parameter_specs': [\n              {\n         \
          \         'parameter_spec': {\n                      'parameter_id': 'eta',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0001,\n                          'max_value': 1.0,\n \
          \                     },\n                      'scale_type': 'UNIT_LOG_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'gamma',\n\
          \                      'discrete_value_spec': {\n                      \
          \    'values': [0, 10, 50, 100, 500, 1000]\n                      },\n \
          \                 },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'max_depth',\n\
          \                      'integer_value_spec': {'min_value': 6, 'max_value':\
          \ 10},\n                      'scale_type': 'UNIT_LINEAR_SCALE',\n     \
          \             },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'min_child_weight',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0,\n                          'max_value': 10.0,\n   \
          \                   },\n                      'scale_type': 'UNIT_LINEAR_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'max_delta_step',\n\
          \                      'discrete_value_spec': {\n                      \
          \    'values': [0.0, 1.0, 3.0, 5.0, 7.0, 9.0]\n                      },\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'subsample',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0001,\n                          'max_value': 1.0,\n \
          \                     },\n                      'scale_type': 'UNIT_LINEAR_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'colsample_bytree',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0001,\n                          'max_value': 1.0,\n \
          \                     },\n                      'scale_type': 'UNIT_LINEAR_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'colsample_bylevel',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0001,\n                          'max_value': 1.0,\n \
          \                     },\n                      'scale_type': 'UNIT_LINEAR_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'colsample_bynode',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0001,\n                          'max_value': 1.0,\n \
          \                     },\n                      'scale_type': 'UNIT_LINEAR_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'lambda',\n\
          \                      'double_value_spec': {\n                        \
          \  'min_value': 0.0001,\n                          'max_value': 1.0,\n \
          \                     },\n                      'scale_type': 'UNIT_REVERSE_LOG_SCALE',\n\
          \                  },\n                  'parent_categorical_values': {\n\
          \                      'values': ['gbtree', 'dart', 'gblinear']\n      \
          \            },\n              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'alpha',\n                  \
          \    'double_value_spec': {\n                          'min_value': 0.0001,\n\
          \                          'max_value': 1.0,\n                      },\n\
          \                      'scale_type': 'UNIT_LOG_SCALE',\n               \
          \   },\n                  'parent_categorical_values': {\n             \
          \         'values': ['gbtree', 'dart', 'gblinear']\n                  },\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'tree_method',\n            \
          \          'categorical_value_spec': {'values': ['auto']},\n           \
          \       },\n                  'parent_categorical_values': {'values': ['gbtree',\
          \ 'dart']},\n              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'scale_pos_weight',\n       \
          \               'discrete_value_spec': {'values': [1.0]},\n            \
          \      },\n                  'parent_categorical_values': {'values': ['gbtree',\
          \ 'dart']},\n              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'refresh_leaf',\n           \
          \           'discrete_value_spec': {'values': [1]},\n                  },\n\
          \                  'parent_categorical_values': {'values': ['gbtree', 'dart']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'process_type',\n           \
          \           'categorical_value_spec': {'values': ['default']},\n       \
          \           },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'grow_policy',\n\
          \                      'categorical_value_spec': {'values': ['depthwise']},\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'sampling_method',\n\
          \                      'categorical_value_spec': {'values': ['uniform']},\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['gbtree', 'dart']},\n              },\n              {\n            \
          \      'parameter_spec': {\n                      'parameter_id': 'sample_type',\n\
          \                      'categorical_value_spec': {'values': ['uniform']},\n\
          \                  },\n                  'parent_categorical_values': {'values':\
          \ ['dart']},\n              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'normalize_type',\n         \
          \             'categorical_value_spec': {'values': ['tree']},\n        \
          \          },\n                  'parent_categorical_values': {'values':\
          \ ['dart']},\n              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'rate_drop',\n              \
          \        'discrete_value_spec': {'values': [0.0]},\n                  },\n\
          \                  'parent_categorical_values': {'values': ['dart']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'one_drop',\n               \
          \       'discrete_value_spec': {'values': [0]},\n                  },\n\
          \                  'parent_categorical_values': {'values': ['dart']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'skip_drop',\n              \
          \        'discrete_value_spec': {'values': [0.0]},\n                  },\n\
          \                  'parent_categorical_values': {'values': ['dart']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'num_parallel_tree',\n      \
          \                'discrete_value_spec': {'values': [1]},\n             \
          \     },\n                  'parent_categorical_values': {'values': ['gblinear']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'feature_selector',\n       \
          \               'categorical_value_spec': {'values': ['cyclic']},\n    \
          \              },\n                  'parent_categorical_values': {'values':\
          \ ['gblinear']},\n              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'top_k',\n                  \
          \    'discrete_value_spec': {'values': [0]},\n                  },\n   \
          \               'parent_categorical_values': {'values': ['gblinear']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'max_leaves',\n             \
          \         'discrete_value_spec': {'values': [0]},\n                  },\n\
          \                  'parent_categorical_values': {'values': ['gbtree', 'dart']},\n\
          \              },\n              {\n                  'parameter_spec':\
          \ {\n                      'parameter_id': 'max_bin',\n                \
          \      'discrete_value_spec': {'values': [256]},\n                  },\n\
          \                  'parent_categorical_values': {'values': ['gbtree', 'dart']},\n\
          \              },\n          ],\n      },\n  ]\n\n  # Construct dictionaries\
          \ so that parameter specs are accessible by id.\n  override_params = {}\n\
          \  override_booster_params = {}\n  for param in study_spec_parameters_override:\n\
          \    # Validate a study spec before adding to the override_params dictionary.\n\
          \    validation_func = _XGBOOST_PARAM_VALIDATIONS[param['parameter_id']]\n\
          \    validation_func(param)\n    override_params[param['parameter_id']]\
          \ = param\n\n    # Add any param that does not have a default parameter\
          \ spec.\n    if (\n        param['parameter_id'] == 'max_cat_to_onehot'\n\
          \        and param['parameter_id'] not in default_params\n    ):\n     \
          \ default_params.append(param)\n    if (\n        param['parameter_id']\
          \ == 'booster'\n        and 'conditional_parameter_specs' in param\n   \
          \ ):\n      for booster_param in param['conditional_parameter_specs']:\n\
          \        _add_booster_param(\n            override_booster_params,\n   \
          \         booster_param,\n            override_boosters=param['categorical_value_spec']['values'],\n\
          \        )\n\n  # Validate override params according to XGBoost param dependencies.\n\
          \  tree_method = override_booster_params.get('tree_method', None)\n  if\
          \ tree_method is not None:\n    for booster, tree_method_spec in tree_method.items():\n\
          \      if tree_method_spec['categorical_value_spec']['values'] != ['exact']:\n\
          \        continue\n      # TODO(b/277777886): exact requires non-zero max_depth\
          \ value.\n      # The below code is no longer necessary after raising min_value\
          \ to 6 in\n      # the default spec. In the long run, we need to decide\
          \ the best\n      # approach for max_depth. Keeping the code for now in\
          \ case the approach\n      # involves overriding max_depth for 'exact' tree_method.\n\
          \      max_depth_spec = {\n          'parameter_id': 'max_depth',\n    \
          \      'integer_value_spec': {'min_value': 6, 'max_value': 10},\n      \
          \    'scale_type': 'UNIT_LINEAR_SCALE',\n      }\n      override_booster_params['max_depth']\
          \ = override_booster_params.get(\n          'max_depth', {booster: max_depth_spec}\n\
          \      )\n      override_booster_params['max_depth'][booster] = override_booster_params[\n\
          \          'max_depth'\n      ].get(booster, max_depth_spec)\n      try:\n\
          \        _validate_int_spec(\n            override_booster_params['max_depth'][booster],\n\
          \            lower_bound=1,\n            upper_bound=None,\n        )\n\
          \      except ValueError as e:\n        raise ValueError(\n            'max_depth\
          \ cannot be 0 (or < 0) when tree method is fixed to be '\n            '\"\
          exact\".'\n        ) from e\n\n  # Construct the modified study specs study_spec_parameters.\n\
          \  study_spec_parameters = []\n  for default_param in default_params:\n\
          \    override_param = override_params.get(\n        default_param['parameter_id'],\
          \ default_param\n    )\n    study_spec_parameters.append(override_param)\n\
          \    # Override conditional parameters for booster.\n    if default_param['parameter_id']\
          \ == 'booster':\n      booster_param_specs = []\n      override_booster_vals\
          \ = override_param['categorical_value_spec']['values']\n\n      for booster_param\
          \ in default_param['conditional_parameter_specs']:\n        override_parent_boosters\
          \ = set(\n            booster_param['parent_categorical_values']['values']\n\
          \        ).intersection(override_booster_vals)\n        if not override_parent_boosters:\n\
          \          # No need to include a booster param if no relevant booster will\n\
          \          # be used.\n          continue\n        # Update default booster\
          \ param boosters to exclude irrelevant boosters.\n        booster_param['parent_categorical_values']['values']\
          \ = list(\n            override_parent_boosters\n        )\n        booster_param_specs.extend(\n\
          \            _get_booster_param_specs(\n                override_booster_params,\n\
          \                param_id=booster_param['parameter_spec']['parameter_id'],\n\
          \                default_param_spec=booster_param,\n            )\n    \
          \    )\n\n      for booster_param_name in _XGBOOST_NO_DEFAULT_BOOSTER_PARAMS:\n\
          \        booster_param_specs.extend(\n            _get_booster_param_specs(\n\
          \                override_booster_params,\n                param_id=booster_param_name,\n\
          \                default_param_spec=None,\n            )\n        )\n\n\
          \      # booster_param_specs combines the overriding booster parameter\n\
          \      # specs from user input and the default booster parameter specs.\n\
          \      override_param['conditional_parameter_specs'] = booster_param_specs\n\
          \n  return study_spec_parameters\n\n"
        image: python:3.7
    exec-model-batch-predict:
      container:
        args:
        - --type
        - BatchPredictionJob
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''job_display_name'']}}",
          "\", ", {"IfPresent": {"InputName": "model", "Then": {"Concat": ["\"model\":
          \"", "{{$.inputs.artifacts[''model''].metadata[''resourceName'']}}", "\","]}}},
          " \"input_config\": {", "\"instances_format\": \"", "{{$.inputs.parameters[''instances_format'']}}",
          "\"", ", \"gcs_source\": {", "\"uris\":", "{{$.inputs.parameters[''gcs_source_uris'']}}",
          "}", ", \"bigquery_source\": {", "\"input_uri\": \"", "{{$.inputs.parameters[''bigquery_source_input_uri'']}}",
          "\"", "}", "}", ", \"model_parameters\": ", "{{$.inputs.parameters[''model_parameters'']}}",
          ", \"output_config\": {", "\"predictions_format\": \"", "{{$.inputs.parameters[''predictions_format'']}}",
          "\"", ", \"gcs_destination\": {", "\"output_uri_prefix\": \"", "{{$.inputs.parameters[''gcs_destination_output_uri_prefix'']}}",
          "\"", "}", ", \"bigquery_destination\": {", "\"output_uri\": \"", "{{$.inputs.parameters[''bigquery_destination_output_uri'']}}",
          "\"", "}", "}", ", \"dedicated_resources\": {", "\"machine_spec\": {", "\"machine_type\":
          \"", "{{$.inputs.parameters[''machine_type'']}}", "\"", ", \"accelerator_type\":
          \"", "{{$.inputs.parameters[''accelerator_type'']}}", "\"", ", \"accelerator_count\":
          ", "{{$.inputs.parameters[''accelerator_count'']}}", "}", ", \"starting_replica_count\":
          ", "{{$.inputs.parameters[''starting_replica_count'']}}", ", \"max_replica_count\":
          ", "{{$.inputs.parameters[''max_replica_count'']}}", "}", ", \"manual_batch_tuning_parameters\":
          {", "\"batch_size\": ", "{{$.inputs.parameters[''manual_batch_tuning_parameters_batch_size'']}}",
          "}", ", \"generate_explanation\": ", "{{$.inputs.parameters[''generate_explanation'']}}",
          ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", ", \"encryption_spec\":
          {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.batch_prediction_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
    exec-model-evaluation:
      container:
        args:
        - --setup_file
        - /setup.py
        - --json_mode
        - 'true'
        - --project_id
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --problem_type
        - '{{$.inputs.parameters[''problem_type'']}}'
        - --batch_prediction_format
        - '{{$.inputs.parameters[''predictions_format'']}}'
        - --batch_prediction_gcs_source
        - '{{$.inputs.artifacts[''batch_prediction_job''].metadata[''gcsOutputDirectory'']}}'
        - --ground_truth_format
        - '{{$.inputs.parameters[''ground_truth_format'']}}'
        - --key_prefix_in_prediction_dataset
        - instance
        - --root_dir
        - '{{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}'
        - --classification_type
        - multiclass
        - --ground_truth_column
        - instance.{{$.inputs.parameters['ground_truth_column']}}
        - --prediction_score_column
        - '{{$.inputs.parameters[''prediction_score_column'']}}'
        - --prediction_label_column
        - '{{$.inputs.parameters[''prediction_label_column'']}}'
        - --prediction_id_column
        - ''
        - --example_weight_column
        - ''
        - --generate_feature_attribution
        - 'false'
        - --dataflow_job_prefix
        - evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
        - --dataflow_service_account
        - '{{$.inputs.parameters[''dataflow_service_account'']}}'
        - --dataflow_disk_size
        - '{{$.inputs.parameters[''dataflow_disk_size'']}}'
        - --dataflow_machine_type
        - '{{$.inputs.parameters[''dataflow_machine_type'']}}'
        - --dataflow_workers_num
        - '{{$.inputs.parameters[''dataflow_workers_num'']}}'
        - --dataflow_max_workers_num
        - '{{$.inputs.parameters[''dataflow_max_workers_num'']}}'
        - --dataflow_subnetwork
        - '{{$.inputs.parameters[''dataflow_subnetwork'']}}'
        - --dataflow_use_public_ips
        - '{{$.inputs.parameters[''dataflow_use_public_ips'']}}'
        - --kms_key_name
        - '{{$.inputs.parameters[''encryption_spec_key_name'']}}'
        - --output_metrics_gcs_path
        - '{{$.outputs.artifacts[''evaluation_metrics''].uri}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python
        - /main.py
        image: gcr.io/ml-pipeline/model-evaluation:v0.4
    exec-model-upload:
      container:
        args:
        - --type
        - UploadModel
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
          "\"", ", \"description\": \"", "{{$.inputs.parameters[''description'']}}",
          "\"", ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - launcher
        image: gcr.io/ml-pipeline/automl-tables-private:1.0.13
    exec-set-optional-inputs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - _set_optional_inputs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0-beta.17'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef _set_optional_inputs(\n    project: str,\n    location: str,\n\
          \    data_source_csv_filenames: str,\n    data_source_bigquery_table_path:\
          \ str,\n    vertex_dataset: Input[dsl.Artifact],\n    model_display_name:\
          \ str,\n) -> NamedTuple(\n    'Outputs',\n    [\n        ('data_source_csv_filenames',\
          \ str),\n        ('data_source_bigquery_table_path', str),\n        ('model_display_name',\
          \ str),\n    ],\n):\n  \"\"\"Get the data source URI.\n\n  Args:\n    project:\
          \ The GCP project that runs the pipeline components.\n    location: The\
          \ GCP region that runs the pipeline components.\n    data_source_csv_filenames:\
          \ The CSV GCS path when data source is CSV.\n    data_source_bigquery_table_path:\
          \ The BigQuery table when data source is BQ.\n    vertex_dataset: The Vertex\
          \ dataset when data source is Vertex dataset.\n    model_display_name: The\
          \ uploaded model's display name.\n\n  Returns:\n    A named tuple of CSV\
          \ or BQ URI.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name\n\
          \  import collections\n  from google.cloud import aiplatform\n  from google.cloud\
          \ import aiplatform_v1beta1 as aip\n  import uuid\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name\n\
          \n  if not model_display_name:\n    model_display_name = f'tabular-workflow-model-{uuid.uuid4()}'\n\
          \n  if vertex_dataset is not None:\n    # of format\n    # projects/294348452381/locations/us-central1/datasets/7104764862735056896\n\
          \    dataset_name = vertex_dataset.metadata['resourceName']\n\n    aiplatform.init(project=project,\
          \ location=location)\n    client = aip.DatasetServiceClient(\n        client_options={'api_endpoint':\
          \ f'{location}-aiplatform.googleapis.com'}\n    )\n    dataset = client.get_dataset(name=dataset_name)\n\
          \    input_config = dataset.metadata['inputConfig']\n    print(input_config)\n\
          \    if 'gcsSource' in input_config:\n      data_source_csv_filenames =\
          \ ','.join(input_config['gcsSource']['uri'])\n    elif 'bigquerySource'\
          \ in input_config:\n      data_source_bigquery_table_path = input_config['bigquerySource']['uri']\n\
          \  elif data_source_csv_filenames:\n    pass\n  elif data_source_bigquery_table_path:\n\
          \    pass\n  else:\n    raise ValueError(\n        'One of vertex_dataset,\
          \ data_source_csv_filenames,'\n        ' data_source_bigquery_table_path\
          \ must be specified'\n    )\n\n  return collections.namedtuple(\n      'Outputs',\n\
          \      [\n          'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
          \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
          \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230605_0125
    exec-split-materialized-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - _split_materialized_data
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef _split_materialized_data(\n    materialized_data: Input[Dataset],\n\
          \    materialized_train_split: OutputPath('MaterializedSplit'),\n    materialized_eval_split:\
          \ OutputPath('MaterializedSplit'),\n    materialized_test_split: OutputPath('MaterializedSplit')):\n\
          \  \"\"\"Splits materialized_data into materialized_data test, train, and\
          \ eval splits.\n\n  Necessary adapter between FTE pipeline and trainer.\n\
          \n  Args:\n    materialized_data: materialized_data dataset output by FTE.\n\
          \    materialized_train_split: Path patern to materialized_train_split.\n\
          \    materialized_eval_split: Path patern to materialized_eval_split.\n\
          \    materialized_test_split: Path patern to materialized_test_split.\n\
          \  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import json\n  import tensorflow as tf\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  with tf.io.gfile.GFile(materialized_data.path, 'r') as f:\n    artifact_path\
          \ = f.read()\n\n  # needed to import tf because this is a path in gs://\n\
          \  with tf.io.gfile.GFile(artifact_path, 'r') as f:\n    materialized_data_json\
          \ = json.load(f)\n\n  if 'tf_record_data_source' in materialized_data_json:\n\
          \    file_patterns = materialized_data_json['tf_record_data_source'][\n\
          \        'file_patterns']\n  elif 'avro_data_source' in materialized_data_json:\n\
          \    file_patterns = materialized_data_json['avro_data_source'][\n     \
          \   'file_patterns']\n  elif 'parquet_data_source' in materialized_data_json:\n\
          \    file_patterns = materialized_data_json['parquet_data_source'][\n  \
          \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
          \ data source: {materialized_data_json}')\n\n  # we map indices to file\
          \ patterns based on the ordering of insertion order\n  # in our transform_data\
          \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
          \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
          \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
          \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20230605_0125
    exec-training-configurator-and-validator:
      container:
        args:
        - training_configurator_and_validator
        - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
        - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
        - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
        - '{"Concat": ["--split_example_counts=", "{{$.inputs.parameters[''split_example_counts'']}}"]}'
        - '{"Concat": ["--target_column=", "{{$.inputs.parameters[''target_column'']}}"]}'
        - '{"Concat": ["--weight_column=", "{{$.inputs.parameters[''weight_column'']}}"]}'
        - '{"Concat": ["--prediction_type=", "{{$.inputs.parameters[''prediction_type'']}}"]}'
        - '{"Concat": ["--optimization_objective=", "{{$.inputs.parameters[''optimization_objective'']}}"]}'
        - '{"Concat": ["--optimization_objective_recall_value=", "{{$.inputs.parameters[''optimization_objective_recall_value'']}}"]}'
        - '{"Concat": ["--optimization_objective_precision_value=", "{{$.inputs.parameters[''optimization_objective_precision_value'']}}"]}'
        - '{"Concat": ["--metadata_path=", "{{$.outputs.artifacts[''metadata''].uri}}"]}'
        - '{"Concat": ["--instance_baseline_path=", "{{$.outputs.artifacts[''instance_baseline''].uri}}"]}'
        - '{"Concat": ["--run_evaluation=", "{{$.inputs.parameters[''run_evaluation'']}}"]}'
        - '{"Concat": ["--run_distill=", "{{$.inputs.parameters[''run_distill'']}}"]}'
        - '{"Concat": ["--enable_probabilistic_inference=", "{{$.inputs.parameters[''enable_probabilistic_inference'']}}"]}'
        - '{"Concat": ["--time_series_identifier_column=", "{{$.inputs.parameters[''time_series_identifier_column'']}}"]}'
        - '{"Concat": ["--time_column=", "{{$.inputs.parameters[''time_column'']}}"]}'
        - '{"Concat": ["--time_series_attribute_columns=", "{{$.inputs.parameters[''time_series_attribute_columns'']}}"]}'
        - '{"Concat": ["--available_at_forecast_columns=", "{{$.inputs.parameters[''available_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--unavailable_at_forecast_columns=", "{{$.inputs.parameters[''unavailable_at_forecast_columns'']}}"]}'
        - '{"IfPresent": {"InputName": "quantiles", "Then": {"Concat": ["--quantiles=",
          "{{$.inputs.parameters[''quantiles'']}}"]}}}'
        - '{"Concat": ["--context_window=", "{{$.inputs.parameters[''context_window'']}}"]}'
        - '{"Concat": ["--forecast_horizon=", "{{$.inputs.parameters[''forecast_horizon'']}}"]}'
        - '{"Concat": ["--forecasting_model_type=", "{{$.inputs.parameters[''forecasting_model_type'']}}"]}'
        - '{"Concat": ["--forecasting_transformations=", "{{$.inputs.parameters[''forecasting_transformations'']}}"]}'
        - '{"IfPresent": {"InputName": "stage_1_deadline_hours", "Then": {"Concat":
          ["--stage_1_deadline_hours=", "{{$.inputs.parameters[''stage_1_deadline_hours'']}}"]}}}'
        - '{"IfPresent": {"InputName": "stage_2_deadline_hours", "Then": {"Concat":
          ["--stage_2_deadline_hours=", "{{$.inputs.parameters[''stage_2_deadline_hours'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_columns", "Then": {"Concat": ["--group_columns=",
          "{{$.inputs.parameters[''group_columns'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
          "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
          ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
        - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
          ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230605_0125
    exec-xgboost-hyperparameter-tuning-job:
      container:
        args:
        - --type
        - HyperparameterTuningJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --payload
        - '{"Concat": ["{\"display_name\": \"xgboost-hyperparameter-tuning-job-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
          \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}, \"study_spec\": {\"metrics\": [{\"metric_id\": \"", "{{$.inputs.parameters[''study_spec_metric_id'']}}",
          "\", \"goal\": \"", "{{$.inputs.parameters[''study_spec_metric_goal'']}}",
          "\"}], \"parameters\": ", "{{$.inputs.parameters[''study_spec_parameters_override'']}}",
          ", \"algorithm\": \"", "{{$.inputs.parameters[''study_spec_algorithm'']}}",
          "\", \"measurement_selection_type\": \"", "{{$.inputs.parameters[''study_spec_measurement_selection_type'']}}",
          "\"}, \"max_trial_count\": ", "{{$.inputs.parameters[''max_trial_count'']}}",
          ", \"parallel_trial_count\": ", "{{$.inputs.parameters[''parallel_trial_count'']}}",
          ", \"max_failed_trial_count\": ", "{{$.inputs.parameters[''max_failed_trial_count'']}}",
          ", \"trial_job_spec\": {\"worker_pool_specs\": ", "{{$.inputs.parameters[''worker_pool_specs'']}}",
          "}}"]}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.hyperparameter_tuning_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
pipelineInfo:
  description: The XGBoost HyperparameterTuningJob pipeline.
  name: automl-tabular-xgboost-hyperparameter-tuning-job
root:
  dag:
    outputs:
      artifacts:
        model-evaluation-evaluation_metrics:
          artifactSelectors:
          - outputArtifactKey: model-evaluation-evaluation_metrics
            producerSubtask: exit-handler-1
    tasks:
      automl-tabular-finalizer:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-automl-tabular-finalizer
        dependentTasks:
        - exit-handler-1
        inputs:
          parameters:
            location:
              componentInputParameter: location
            project:
              componentInputParameter: project
            root_dir:
              componentInputParameter: root_dir
        taskInfo:
          name: automl-tabular-finalizer
        triggerPolicy:
          strategy: ALL_UPSTREAM_TASKS_COMPLETED
      exit-handler-1:
        componentRef:
          name: comp-exit-handler-1
        dependentTasks:
        - set-optional-inputs
        inputs:
          parameters:
            pipelinechannel--bigquery_staging_full_dataset_id:
              componentInputParameter: bigquery_staging_full_dataset_id
            pipelinechannel--dataflow_service_account:
              componentInputParameter: dataflow_service_account
            pipelinechannel--dataflow_subnetwork:
              componentInputParameter: dataflow_subnetwork
            pipelinechannel--dataflow_use_public_ips:
              componentInputParameter: dataflow_use_public_ips
            pipelinechannel--dataset_level_custom_transformation_definitions:
              componentInputParameter: dataset_level_custom_transformation_definitions
            pipelinechannel--dataset_level_transformations:
              componentInputParameter: dataset_level_transformations
            pipelinechannel--disable_default_eval_metric:
              componentInputParameter: disable_default_eval_metric
            pipelinechannel--encryption_spec_key_name:
              componentInputParameter: encryption_spec_key_name
            pipelinechannel--eval_metric:
              componentInputParameter: eval_metric
            pipelinechannel--evaluation_batch_predict_machine_type:
              componentInputParameter: evaluation_batch_predict_machine_type
            pipelinechannel--evaluation_batch_predict_max_replica_count:
              componentInputParameter: evaluation_batch_predict_max_replica_count
            pipelinechannel--evaluation_batch_predict_starting_replica_count:
              componentInputParameter: evaluation_batch_predict_starting_replica_count
            pipelinechannel--evaluation_dataflow_disk_size_gb:
              componentInputParameter: evaluation_dataflow_disk_size_gb
            pipelinechannel--evaluation_dataflow_machine_type:
              componentInputParameter: evaluation_dataflow_machine_type
            pipelinechannel--evaluation_dataflow_max_num_workers:
              componentInputParameter: evaluation_dataflow_max_num_workers
            pipelinechannel--evaluation_dataflow_starting_num_workers:
              componentInputParameter: evaluation_dataflow_starting_num_workers
            pipelinechannel--feature_selection_algorithm:
              componentInputParameter: feature_selection_algorithm
            pipelinechannel--location:
              componentInputParameter: location
            pipelinechannel--max_failed_trial_count:
              componentInputParameter: max_failed_trial_count
            pipelinechannel--max_selected_features:
              componentInputParameter: max_selected_features
            pipelinechannel--max_trial_count:
              componentInputParameter: max_trial_count
            pipelinechannel--model_description:
              componentInputParameter: model_description
            pipelinechannel--objective:
              componentInputParameter: objective
            pipelinechannel--parallel_trial_count:
              componentInputParameter: parallel_trial_count
            pipelinechannel--predefined_split_key:
              componentInputParameter: predefined_split_key
            pipelinechannel--project:
              componentInputParameter: project
            pipelinechannel--root_dir:
              componentInputParameter: root_dir
            pipelinechannel--run_evaluation:
              componentInputParameter: run_evaluation
            pipelinechannel--run_feature_selection:
              componentInputParameter: run_feature_selection
            pipelinechannel--seed:
              componentInputParameter: seed
            pipelinechannel--seed_per_iteration:
              componentInputParameter: seed_per_iteration
            pipelinechannel--set-optional-inputs-data_source_bigquery_table_path:
              taskOutputParameter:
                outputParameterKey: data_source_bigquery_table_path
                producerTask: set-optional-inputs
            pipelinechannel--set-optional-inputs-data_source_csv_filenames:
              taskOutputParameter:
                outputParameterKey: data_source_csv_filenames
                producerTask: set-optional-inputs
            pipelinechannel--set-optional-inputs-model_display_name:
              taskOutputParameter:
                outputParameterKey: model_display_name
                producerTask: set-optional-inputs
            pipelinechannel--stratified_split_key:
              componentInputParameter: stratified_split_key
            pipelinechannel--study_spec_algorithm:
              componentInputParameter: study_spec_algorithm
            pipelinechannel--study_spec_measurement_selection_type:
              componentInputParameter: study_spec_measurement_selection_type
            pipelinechannel--study_spec_metric_goal:
              componentInputParameter: study_spec_metric_goal
            pipelinechannel--study_spec_metric_id:
              componentInputParameter: study_spec_metric_id
            pipelinechannel--study_spec_parameters_override:
              componentInputParameter: study_spec_parameters_override
            pipelinechannel--target_column:
              componentInputParameter: target_column
            pipelinechannel--test_fraction:
              componentInputParameter: test_fraction
            pipelinechannel--tf_auto_transform_features:
              componentInputParameter: tf_auto_transform_features
            pipelinechannel--tf_custom_transformation_definitions:
              componentInputParameter: tf_custom_transformation_definitions
            pipelinechannel--tf_transformations_path:
              componentInputParameter: tf_transformations_path
            pipelinechannel--training_accelerator_count:
              componentInputParameter: training_accelerator_count
            pipelinechannel--training_accelerator_type:
              componentInputParameter: training_accelerator_type
            pipelinechannel--training_fraction:
              componentInputParameter: training_fraction
            pipelinechannel--training_machine_type:
              componentInputParameter: training_machine_type
            pipelinechannel--training_total_replica_count:
              componentInputParameter: training_total_replica_count
            pipelinechannel--transform_dataflow_disk_size_gb:
              componentInputParameter: transform_dataflow_disk_size_gb
            pipelinechannel--transform_dataflow_machine_type:
              componentInputParameter: transform_dataflow_machine_type
            pipelinechannel--transform_dataflow_max_num_workers:
              componentInputParameter: transform_dataflow_max_num_workers
            pipelinechannel--validation_fraction:
              componentInputParameter: validation_fraction
            pipelinechannel--weight_column:
              componentInputParameter: weight_column
        taskInfo:
          name: exit-handler-1
      set-optional-inputs:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-set-optional-inputs
        inputs:
          artifacts:
            vertex_dataset:
              componentInputArtifact: vertex_dataset
          parameters:
            data_source_bigquery_table_path:
              componentInputParameter: data_source_bigquery_table_path
            data_source_csv_filenames:
              componentInputParameter: data_source_csv_filenames
            location:
              componentInputParameter: location
            model_display_name:
              componentInputParameter: model_display_name
            project:
              componentInputParameter: project
        taskInfo:
          name: set-optional-inputs
  inputDefinitions:
    artifacts:
      vertex_dataset:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
        description: The Vertex dataset artifact.
    parameters:
      bigquery_staging_full_dataset_id:
        defaultValue: ''
        description: 'The BigQuery staging full dataset id for

          storing intermediate tables.'
        isOptional: true
        parameterType: STRING
      data_source_bigquery_table_path:
        defaultValue: ''
        description: The BigQuery data source.
        isOptional: true
        parameterType: STRING
      data_source_csv_filenames:
        defaultValue: ''
        description: The CSV data source.
        isOptional: true
        parameterType: STRING
      dataflow_service_account:
        defaultValue: ''
        description: Custom service account to run dataflow jobs.
        isOptional: true
        parameterType: STRING
      dataflow_subnetwork:
        defaultValue: ''
        description: 'Dataflow''s fully qualified subnetwork name, when empty

          the default subnetwork will be used. Example:

          https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
        isOptional: true
        parameterType: STRING
      dataflow_use_public_ips:
        defaultValue: true
        description: 'Specifies whether Dataflow workers use public IP

          addresses.'
        isOptional: true
        parameterType: BOOLEAN
      dataset_level_custom_transformation_definitions:
        description: 'Dataset-level custom

          transformation definitions in string format.'
        isOptional: true
        parameterType: LIST
      dataset_level_transformations:
        description: 'Dataset-level transformation configuration in

          string format.'
        isOptional: true
        parameterType: LIST
      disable_default_eval_metric:
        defaultValue: 0.0
        description: 'Flag to disable default metric. Set to >0 to

          disable. Default to 0.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      encryption_spec_key_name:
        defaultValue: ''
        description: The KMS key name.
        isOptional: true
        parameterType: STRING
      eval_metric:
        defaultValue: ''
        description: 'Evaluation metrics for validation data represented as a

          comma-separated string.'
        isOptional: true
        parameterType: STRING
      evaluation_batch_predict_machine_type:
        defaultValue: n1-highmem-8
        description: 'The prediction server machine type

          for batch predict components during evaluation.'
        isOptional: true
        parameterType: STRING
      evaluation_batch_predict_max_replica_count:
        defaultValue: 20.0
        description: 'The max number of prediction

          server for batch predict components during evaluation.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      evaluation_batch_predict_starting_replica_count:
        defaultValue: 20.0
        description: 'The initial number of

          prediction server for batch predict components during evaluation.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      evaluation_dataflow_disk_size_gb:
        defaultValue: 50.0
        description: 'Dataflow worker''s disk size in GB for

          evaluation components.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      evaluation_dataflow_machine_type:
        defaultValue: n1-standard-4
        description: 'The dataflow machine type for evaluation

          components.'
        isOptional: true
        parameterType: STRING
      evaluation_dataflow_max_num_workers:
        defaultValue: 100.0
        description: 'The max number of Dataflow workers for

          evaluation components.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      evaluation_dataflow_starting_num_workers:
        defaultValue: 10.0
        description: 'The initial number of Dataflow

          workers for evaluation components.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      feature_selection_algorithm:
        defaultValue: AMI
        description: Feature selection algorithm.
        isOptional: true
        parameterType: STRING
      location:
        description: The GCP region that runs the pipeline components.
        parameterType: STRING
      max_failed_trial_count:
        defaultValue: 0.0
        description: 'The number of failed trials that need to be seen

          before failing the HyperparameterTuningJob. If set to 0, Vertex AI decides

          how many trials must fail before the whole job fails.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_selected_features:
        defaultValue: -1.0
        description: Maximum number of features to select.
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_trial_count:
        description: The desired total number of trials.
        parameterType: NUMBER_INTEGER
      model_description:
        defaultValue: ''
        description: The description name of the uploaded Vertex model.
        isOptional: true
        parameterType: STRING
      model_display_name:
        defaultValue: ''
        description: The display name of the uploaded Vertex model.
        isOptional: true
        parameterType: STRING
      objective:
        description: 'Specifies the learning task and the learning objective. Must
          be

          one of [reg:squarederror, reg:squaredlogerror,

          reg:logistic, reg:gamma, reg:tweedie, reg:pseudohubererror,

          binary:logistic, multi:softprob].'
        parameterType: STRING
      parallel_trial_count:
        description: The desired number of trials to run in parallel.
        parameterType: NUMBER_INTEGER
      predefined_split_key:
        defaultValue: ''
        description: Predefined split key.
        isOptional: true
        parameterType: STRING
      project:
        description: The GCP project that runs the pipeline components.
        parameterType: STRING
      root_dir:
        description: The root GCS directory for the pipeline components.
        parameterType: STRING
      run_evaluation:
        defaultValue: false
        description: Whether to run evaluation steps during training.
        isOptional: true
        parameterType: BOOLEAN
      run_feature_selection:
        defaultValue: false
        description: Whether to enable feature selection.
        isOptional: true
        parameterType: BOOLEAN
      seed:
        defaultValue: 0.0
        description: Random seed.
        isOptional: true
        parameterType: NUMBER_INTEGER
      seed_per_iteration:
        defaultValue: false
        description: Seed PRNG determnisticly via iterator number.
        isOptional: true
        parameterType: BOOLEAN
      stratified_split_key:
        defaultValue: ''
        description: Stratified split key.
        isOptional: true
        parameterType: STRING
      study_spec_algorithm:
        defaultValue: ALGORITHM_UNSPECIFIED
        description: 'The search algorithm specified for the study. One of

          ''ALGORITHM_UNSPECIFIED'', ''GRID_SEARCH'', or ''RANDOM_SEARCH''.'
        isOptional: true
        parameterType: STRING
      study_spec_measurement_selection_type:
        defaultValue: BEST_MEASUREMENT
        description: ' Which measurement to use if/when the

          service automatically selects the final measurement from previously

          reported intermediate measurements. One of "BEST_MEASUREMENT" or

          "LAST_MEASUREMENT".'
        isOptional: true
        parameterType: STRING
      study_spec_metric_goal:
        description: 'Optimization goal of the metric, possible values:

          "MAXIMIZE", "MINIMIZE".'
        parameterType: STRING
      study_spec_metric_id:
        description: 'Metric to optimize. For options, please look under

          ''eval_metrics'' at

          https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters.'
        parameterType: STRING
      study_spec_parameters_override:
        description: 'List of dictionaries representing parameters

          to optimize. The dictionary key is the parameter_id, which is passed to

          training job as a command line argument, and the dictionary value is the

          parameter specification of the metric.'
        parameterType: LIST
      target_column:
        description: The target column name.
        parameterType: STRING
      test_fraction:
        defaultValue: -1.0
        description: Test fraction.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      tf_auto_transform_features:
        description: 'List of auto transform features in the

          comma-separated string format.'
        isOptional: true
        parameterType: STRUCT
      tf_custom_transformation_definitions:
        description: 'TF custom transformation definitions

          in string format.'
        isOptional: true
        parameterType: LIST
      tf_transformations_path:
        defaultValue: ''
        description: Path to TF transformation configuration.
        isOptional: true
        parameterType: STRING
      training_accelerator_count:
        defaultValue: 0.0
        description: Accelerator count.
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_accelerator_type:
        defaultValue: ''
        description: Accelerator type.
        isOptional: true
        parameterType: STRING
      training_fraction:
        defaultValue: -1.0
        description: Training fraction.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      training_machine_type:
        defaultValue: c2-standard-16
        description: Machine type.
        isOptional: true
        parameterType: STRING
      training_total_replica_count:
        defaultValue: 1.0
        description: Number of workers.
        isOptional: true
        parameterType: NUMBER_INTEGER
      transform_dataflow_disk_size_gb:
        defaultValue: 40.0
        description: 'Dataflow worker''s disk size in GB for

          transform component.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      transform_dataflow_machine_type:
        defaultValue: n1-standard-16
        description: 'The dataflow machine type for transform

          component.'
        isOptional: true
        parameterType: STRING
      transform_dataflow_max_num_workers:
        defaultValue: 25.0
        description: 'The max number of Dataflow workers for

          transform component.'
        isOptional: true
        parameterType: NUMBER_INTEGER
      validation_fraction:
        defaultValue: -1.0
        description: Validation fraction.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      weight_column:
        defaultValue: ''
        description: The weight column name.
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      model-evaluation-evaluation_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0-beta.17
