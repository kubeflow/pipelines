{
  "pipelineSpec": {
    "components": {
      "comp-bigquery-create-dataset": {
        "executorLabel": "exec-bigquery-create-dataset",
        "inputDefinitions": {
          "parameters": {
            "dataset": {
              "type": "STRING"
            },
            "exists_ok": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-bigquery-delete-dataset-with-prefix": {
        "executorLabel": "exec-bigquery-delete-dataset-with-prefix",
        "inputDefinitions": {
          "parameters": {
            "dataset_prefix": {
              "type": "STRING"
            },
            "delete_contents": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-build-jobs-parameters": {
        "executorLabel": "exec-build-jobs-parameters",
        "inputDefinitions": {
          "parameters": {
            "data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "data_source_csv_filenames": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "training_jobs": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-concatenate-data": {
        "executorLabel": "exec-concatenate-data",
        "inputDefinitions": {
          "parameters": {
            "data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "data_source_csv_filenames": {
              "type": "STRING"
            },
            "evaluation_data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "evaluation_data_source_csv_filenames": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "tmp_dataset_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "data_source_csv_filenames": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-exit-handler-1": {
        "dag": {
          "tasks": {
            "bigquery-create-dataset": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-bigquery-create-dataset"
              },
              "dependentTasks": [
                "generate-iso8601-underscore-datetime-format",
                "get-table-location"
              ],
              "inputs": {
                "parameters": {
                  "dataset": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "model_comparison_tmp_{{$.inputs.parameters['pipelineparam--generate-iso8601-underscore-datetime-format-Output']}}"
                      }
                    }
                  },
                  "exists_ok": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "False"
                      }
                    }
                  },
                  "location": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-table-location"
                    }
                  },
                  "pipelineparam--generate-iso8601-underscore-datetime-format-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "generate-iso8601-underscore-datetime-format"
                    }
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  }
                }
              },
              "taskInfo": {
                "name": "create-tmp-dataset"
              }
            },
            "build-jobs-parameters": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-build-jobs-parameters"
              },
              "dependentTasks": [
                "concatenate-data",
                "validate-inputs"
              ],
              "inputs": {
                "parameters": {
                  "data_source_bigquery_table_path": {
                    "taskOutputParameter": {
                      "outputParameterKey": "data_source_bigquery_table_path",
                      "producerTask": "concatenate-data"
                    }
                  },
                  "data_source_csv_filenames": {
                    "taskOutputParameter": {
                      "outputParameterKey": "data_source_csv_filenames",
                      "producerTask": "concatenate-data"
                    }
                  },
                  "location": {
                    "componentInputParameter": "pipelineparam--location"
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  },
                  "training_jobs": {
                    "componentInputParameter": "pipelineparam--training_jobs"
                  }
                }
              },
              "taskInfo": {
                "name": "build-jobs-parameters"
              }
            },
            "concatenate-data": {
              "cachingOptions": {},
              "componentRef": {
                "name": "comp-concatenate-data"
              },
              "dependentTasks": [
                "bigquery-create-dataset",
                "get-table-location",
                "validate-inputs"
              ],
              "inputs": {
                "parameters": {
                  "data_source_bigquery_table_path": {
                    "componentInputParameter": "pipelineparam--data_source_bigquery_table_path"
                  },
                  "data_source_csv_filenames": {
                    "componentInputParameter": "pipelineparam--data_source_csv_filenames"
                  },
                  "evaluation_data_source_bigquery_table_path": {
                    "componentInputParameter": "pipelineparam--evaluation_data_source_bigquery_table_path"
                  },
                  "evaluation_data_source_csv_filenames": {
                    "componentInputParameter": "pipelineparam--evaluation_data_source_csv_filenames"
                  },
                  "location": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-table-location"
                    }
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  },
                  "tmp_dataset_name": {
                    "taskOutputParameter": {
                      "outputParameterKey": "dataset_id",
                      "producerTask": "bigquery-create-dataset"
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "concatenate-data"
              }
            },
            "for-loop-2": {
              "componentRef": {
                "name": "comp-for-loop-2"
              },
              "dependentTasks": [
                "build-jobs-parameters",
                "get-experiment"
              ],
              "inputs": {
                "parameters": {
                  "pipelineparam--build-jobs-parameters-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "build-jobs-parameters"
                    }
                  },
                  "pipelineparam--get-experiment-experiment_name": {
                    "taskOutputParameter": {
                      "outputParameterKey": "experiment_name",
                      "producerTask": "get-experiment"
                    }
                  },
                  "pipelineparam--location": {
                    "componentInputParameter": "pipelineparam--location"
                  },
                  "pipelineparam--network": {
                    "componentInputParameter": "pipelineparam--network"
                  },
                  "pipelineparam--project": {
                    "componentInputParameter": "pipelineparam--project"
                  },
                  "pipelineparam--root_dir": {
                    "componentInputParameter": "pipelineparam--root_dir"
                  },
                  "pipelineparam--service_account": {
                    "componentInputParameter": "pipelineparam--service_account"
                  }
                }
              },
              "parameterIterator": {
                "itemInput": "pipelineparam--build-jobs-parameters-Output-loop-item",
                "items": {
                  "inputParameter": "pipelineparam--build-jobs-parameters-Output"
                }
              },
              "taskInfo": {
                "name": "for-loop-2"
              }
            },
            "generate-iso8601-underscore-datetime-format": {
              "cachingOptions": {},
              "componentRef": {
                "name": "comp-generate-iso8601-underscore-datetime-format"
              },
              "dependentTasks": [
                "validate-inputs"
              ],
              "inputs": {
                "parameters": {
                  "run_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{{$.pipeline_job_uuid}}"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "generate-iso8601-underscore-datetime-format"
              }
            },
            "get-experiment": {
              "cachingOptions": {},
              "componentRef": {
                "name": "comp-get-experiment"
              },
              "dependentTasks": [
                "validate-inputs"
              ],
              "inputs": {
                "parameters": {
                  "experiment_name": {
                    "componentInputParameter": "pipelineparam--experiment"
                  },
                  "location": {
                    "componentInputParameter": "pipelineparam--location"
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  }
                }
              },
              "taskInfo": {
                "name": "get-experiment"
              }
            },
            "get-table-location": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-get-table-location"
              },
              "inputs": {
                "parameters": {
                  "default_location": {
                    "componentInputParameter": "pipelineparam--location"
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  },
                  "table": {
                    "componentInputParameter": "pipelineparam--data_source_bigquery_table_path"
                  }
                }
              },
              "taskInfo": {
                "name": "get-table-location"
              }
            },
            "validate-inputs": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-validate-inputs"
              },
              "inputs": {
                "parameters": {
                  "data_source_bigquery_table_path": {
                    "componentInputParameter": "pipelineparam--data_source_bigquery_table_path"
                  },
                  "data_source_csv_filenames": {
                    "componentInputParameter": "pipelineparam--data_source_csv_filenames"
                  },
                  "evaluation_data_source_bigquery_table_path": {
                    "componentInputParameter": "pipelineparam--evaluation_data_source_bigquery_table_path"
                  },
                  "evaluation_data_source_csv_filenames": {
                    "componentInputParameter": "pipelineparam--evaluation_data_source_csv_filenames"
                  },
                  "location": {
                    "componentInputParameter": "pipelineparam--location"
                  },
                  "prediction_type": {
                    "componentInputParameter": "pipelineparam--prediction_type"
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  },
                  "training_jobs": {
                    "componentInputParameter": "pipelineparam--training_jobs"
                  }
                }
              },
              "taskInfo": {
                "name": "validate-inputs"
              }
            }
          }
        },
        "inputDefinitions": {
          "parameters": {
            "pipelineparam--data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "pipelineparam--data_source_csv_filenames": {
              "type": "STRING"
            },
            "pipelineparam--evaluation_data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "pipelineparam--evaluation_data_source_csv_filenames": {
              "type": "STRING"
            },
            "pipelineparam--experiment": {
              "type": "STRING"
            },
            "pipelineparam--location": {
              "type": "STRING"
            },
            "pipelineparam--network": {
              "type": "STRING"
            },
            "pipelineparam--prediction_type": {
              "type": "STRING"
            },
            "pipelineparam--project": {
              "type": "STRING"
            },
            "pipelineparam--root_dir": {
              "type": "STRING"
            },
            "pipelineparam--service_account": {
              "type": "STRING"
            },
            "pipelineparam--training_jobs": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-for-loop-2": {
        "dag": {
          "tasks": {
            "run-pipeline": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-pipeline"
              },
              "inputs": {
                "parameters": {
                  "experiment_name": {
                    "componentInputParameter": "pipelineparam--get-experiment-experiment_name"
                  },
                  "job_params": {
                    "componentInputParameter": "pipelineparam--build-jobs-parameters-Output-loop-item",
                    "parameterExpressionSelector": "parseJson(string_value)[\"job_params\"]"
                  },
                  "location": {
                    "componentInputParameter": "pipelineparam--location"
                  },
                  "model_comparison_job_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{{$.pipeline_job_name}}"
                      }
                    }
                  },
                  "model_name": {
                    "componentInputParameter": "pipelineparam--build-jobs-parameters-Output-loop-item",
                    "parameterExpressionSelector": "parseJson(string_value)[\"model_name\"]"
                  },
                  "network": {
                    "componentInputParameter": "pipelineparam--network"
                  },
                  "pipeline_root": {
                    "componentInputParameter": "pipelineparam--root_dir"
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project"
                  },
                  "service_account": {
                    "componentInputParameter": "pipelineparam--service_account"
                  }
                }
              },
              "taskInfo": {
                "name": "run-pipeline"
              }
            }
          }
        },
        "inputDefinitions": {
          "parameters": {
            "pipelineparam--build-jobs-parameters-Output": {
              "type": "STRING"
            },
            "pipelineparam--build-jobs-parameters-Output-loop-item": {
              "type": "STRING"
            },
            "pipelineparam--get-experiment-experiment_name": {
              "type": "STRING"
            },
            "pipelineparam--location": {
              "type": "STRING"
            },
            "pipelineparam--network": {
              "type": "STRING"
            },
            "pipelineparam--project": {
              "type": "STRING"
            },
            "pipelineparam--root_dir": {
              "type": "STRING"
            },
            "pipelineparam--service_account": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-generate-iso8601-underscore-datetime-format": {
        "executorLabel": "exec-generate-iso8601-underscore-datetime-format",
        "inputDefinitions": {
          "parameters": {
            "run_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-get-experiment": {
        "executorLabel": "exec-get-experiment",
        "inputDefinitions": {
          "parameters": {
            "experiment_name": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "experiment": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "experiment_name": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-get-table-location": {
        "executorLabel": "exec-get-table-location",
        "inputDefinitions": {
          "parameters": {
            "default_location": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "table": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-run-pipeline": {
        "executorLabel": "exec-run-pipeline",
        "inputDefinitions": {
          "parameters": {
            "experiment_name": {
              "type": "STRING"
            },
            "job_params": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "model_comparison_job_name": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "pipeline_root": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-validate-inputs": {
        "executorLabel": "exec-validate-inputs",
        "inputDefinitions": {
          "parameters": {
            "data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "data_source_csv_filenames": {
              "type": "STRING"
            },
            "evaluation_data_source_bigquery_table_path": {
              "type": "STRING"
            },
            "evaluation_data_source_csv_filenames": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "prediction_type": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "training_jobs": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-bigquery-create-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bigquery_create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1' 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bigquery_create_dataset(\n    project: str,\n    location: str,\n    dataset: str,\n    exists_ok: bool = False,\n) -> NamedTuple('Outputs', [('project_id', str), ('dataset_id', str)]):\n  \"\"\"Creates a BigQuery dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n  import collections\n\n  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\n  client = bigquery.Client(project=project, location=location)\n  ref = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n      ref.project, ref.dataset_id)\n\n"
            ],
            "image": "python:3.7-slim"
          }
        },
        "exec-bigquery-delete-dataset-with-prefix": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bigquery_delete_dataset_with_prefix"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1' 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bigquery_delete_dataset_with_prefix(\n    project: str,\n    dataset_prefix: str,\n    delete_contents: bool = False,\n) -> None:\n  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\n"
            ],
            "image": "python:3.7-slim"
          }
        },
        "exec-build-jobs-parameters": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "build_jobs_parameters"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef build_jobs_parameters(\n    project: str,\n    location: str,\n    training_jobs: Dict[str, Dict[str, Any]],\n    data_source_csv_filenames: str,\n    data_source_bigquery_table_path: str) -> List[Dict[str, Any]]:\n  \"\"\"Prepares a list of dictionaries with name and parameters for each sub-pipeline job.\n\n  Args:\n    project:\n      The GCP project parameter that should be passed to all the jobs.\n    location:\n      The GCP location parameter that should be passed to all the jobs.\n    training_jobs:\n      Dictionary of jobs with their parameters.\n    data_source_csv_filenames:\n      Data source parameter that should be passed to all the jobs\n    data_source_bigquery_table_path:\n      Data source parameter that should be passed to all the jobs\n\n  Returns:\n    A list of dictionaries with two keys. The keys are 'model_name' - the\n    original key from the training_jobs dictionary and 'job_params' the value\n    associated with that key.\n  \"\"\"\n\n  # TODO(b/221603586): Remove these if blocks.\n  # Some pipelines don't interpret dashes as missing values.\n  if data_source_csv_filenames == '-':\n    data_source_csv_filenames = ''\n  if data_source_bigquery_table_path == '-':\n    data_source_bigquery_table_path = ''\n\n  # Tabular Workflows pipelines expect a schema for BigQuery URIs.\n  if (\n      data_source_bigquery_table_path\n      and not data_source_bigquery_table_path.startswith('bq://')\n      and not data_source_bigquery_table_path.startswith('bigquery://')\n  ):\n    data_source_bigquery_table_path = f'bq://{data_source_bigquery_table_path}'\n\n  for job in training_jobs:\n    training_jobs[job]['parameter_values'].update({\n        'project': project,\n        'location': location,\n        'data_source_csv_filenames': data_source_csv_filenames,\n        'data_source_bigquery_table_path': data_source_bigquery_table_path\n    })\n\n  return [{'model_name': k, 'job_params': v} for k, v in training_jobs.items()]\n\n"
            ],
            "image": "python:3.8-slim"
          }
        },
        "exec-concatenate-data": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "concatenate_data"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1' 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef concatenate_data(\n    project: str,\n    location: str,\n    tmp_dataset_name: str,\n    data_source_csv_filenames: Optional[str] = None,\n    data_source_bigquery_table_path: Optional[str] = None,\n    evaluation_data_source_csv_filenames: Optional[str] = None,\n    evaluation_data_source_bigquery_table_path: Optional[str] = None,\n) -> NamedTuple('Outputs', [\n    ('data_source_csv_filenames', str),\n    ('data_source_bigquery_table_path', str),\n]):\n  \"\"\"Concatenate input and evaluation data sources together.\n\n  Args:\n    project: The GCP project that runs the pipeline components.\n    location: The GCP region that runs the BigQuery queries.\n    tmp_dataset_name: The name of the tmp dataset to create the unioned table.\n    data_source_csv_filenames: Comma-separated paths to CSVs stored in GCS to\n      use as the dataset for all training pipelines. This should be None if\n      `data_source_bigquery_table_path` is not None. This should only contain\n      data from the training and validation split and not from the test split.\n    data_source_bigquery_table_path: Path to BigQuery Table to use as the\n      dataset for all training pipelines. This should be None if\n      `data_source_csv_filenames` is not None. This should only contain data\n      from the training and validation split and not from the test split.\n    evaluation_data_source_csv_filenames: Comma-separated paths to CSVs stored\n      in GCS to use as the evaluation dataset for all training pipelines. This\n      should be None if `evaluation_data_source_bigquery_table_path` is not\n      None. This should only contain data from the test split and not from the\n      training and validation split.\n    evaluation_data_source_bigquery_table_path: Path to BigQuery Table to use as\n      the evaluation dataset for all training pipelines. This should be None if\n      `evaluation_data_source_csv_filenames` is not None. This should only\n      contain data from the test split and not from the training and validation\n      split.\n\n  Returns:\n    Concatenated csv and bigquery paths.\n  \"\"\"\n\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n  import collections\n  import uuid\n\n  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n  if data_source_csv_filenames == '-':\n    data_source_csv_filenames = None\n  if data_source_bigquery_table_path == '-':\n    data_source_bigquery_table_path = None\n  if evaluation_data_source_csv_filenames == '-':\n    evaluation_data_source_csv_filenames = None\n  if evaluation_data_source_bigquery_table_path == '-':\n    evaluation_data_source_bigquery_table_path = None\n\n  if data_source_csv_filenames and evaluation_data_source_csv_filenames:\n    return collections.namedtuple('Outputs', [\n        'data_source_csv_filenames',\n        'data_source_bigquery_table_path',\n    ])(\n        data_source_csv_filenames + ',' + evaluation_data_source_csv_filenames,\n        '-',\n    )\n\n  client = bigquery.Client(project=project, location=location)\n  bq_table_names = [\n      data_source_bigquery_table_path,\n      evaluation_data_source_bigquery_table_path\n  ]\n  for i in range(len(bq_table_names)):\n    table_uri = bq_table_names[i]\n    if table_uri.startswith('bq://'):\n      table_uri = table_uri[len('bq://'):]\n    elif table_uri.startswith('bigquery://'):\n      table_uri = table_uri[len('bigquery://'):]\n\n    bq_table_names[i] = table_uri\n\n  data_source_bigquery_table_path = bq_table_names[0]\n  evaluation_data_source_bigquery_table_path = bq_table_names[1]\n\n  final_table_id = (f'{project}.{tmp_dataset_name}'\n                    f'.model_comparison_unioned_table_{uuid.uuid4().hex[-4:]}')\n\n  union_query = f\"\"\"\n      CREATE TABLE `{final_table_id}` AS (\n        SELECT * FROM `{data_source_bigquery_table_path}`\n        UNION ALL\n        SELECT * FROM `{evaluation_data_source_bigquery_table_path}`\n      )\n  \"\"\"\n  client.query(union_query).result()\n\n  return collections.namedtuple('Outputs', [\n      'data_source_csv_filenames',\n      'data_source_bigquery_table_path',\n  ])(\n      '-',\n      final_table_id,\n  )\n\n"
            ],
            "image": "python:3.8-slim"
          }
        },
        "exec-generate-iso8601-underscore-datetime-format": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "generate_iso8601_underscore_datetime_format"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef generate_iso8601_underscore_datetime_format(run_id: str) -> str:\n  \"\"\"Creates a timestamp using the same logic as Vertex Forecasting.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n  import datetime\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\n  timestamp = datetime.datetime.now().strftime('%Y_%m_%dT%H_%M_%S_%f')[:23]\n  return f'{run_id}_{timestamp}Z'\n\n"
            ],
            "image": "python:3.7-slim"
          }
        },
        "exec-get-experiment": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_experiment"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform==1.18.2' 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_experiment(\n    project: str, location: str, experiment_name: Optional[str],\n    experiment: dsl.Output[dsl.Artifact]\n) -> NamedTuple('Outputs', [('experiment_name', str)]):\n  \"\"\"Returns a Vertex AI experiment name under which to run the pipelines.\n\n  Args:\n    project:\n      The GCP project in which the experiment will be created.\n    location:\n      The GCP location in which the experiment will be created.\n    experiment_name:\n      Dictionary of jobs with their parameters.\n    experiment:\n      Output artifact containing experiment name and cloud console URL.\n\n  Returns:\n    A NamedTuple with the name of the created experiment.\n\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n  import datetime\n  import logging\n  import uuid\n  from google.cloud import aiplatform\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\n  if not experiment_name or experiment_name == '-':\n    experiment_datetime = datetime.datetime.now().strftime(\n        '%Y-%m-%d')\n    experiment_name = f'model-comparison-{experiment_datetime}-{uuid.uuid4().hex[-4:]}'\n\n  aiplatform.Experiment.get_or_create(\n      experiment_name=experiment_name, project=project, location=location)\n\n  experiment_ui_url = f'https://console.cloud.google.com/vertex-ai/locations/{location}/experiments/{experiment_name}?project={project}'\n  logging.info(\n      'Using experiment %s. Access it at %s',\n      experiment_name, experiment_ui_url\n  )\n\n  experiment.metadata = {\n      'experiment_name': experiment_name\n  }\n  experiment.uri = experiment_ui_url\n\n  return (experiment_name,)\n\n"
            ],
            "image": "python:3.8-slim"
          }
        },
        "exec-get-table-location": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_table_location"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1' 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_table_location(\n    project: str,\n    table: Optional[str],\n    default_location: str = '',\n) -> str:\n  \"\"\"Returns the region the given table belongs to.\n\n  Args:\n    project: The GCP project.\n    table: The BigQuery table to get a location for.\n    default_location: Location to return if no table was given.\n\n  Returns:\n    A GCP region or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\n  if not table or table == '-':\n    return default_location\n\n  client = bigquery.Client(project=project)\n  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n  return client.get_table(table).location\n\n"
            ],
            "image": "python:3.7-slim"
          }
        },
        "exec-run-pipeline": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_pipeline"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform==1.18.2' 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_pipeline(project: str, location: str, model_name: str,\n                 job_params: Dict[str, Any], experiment_name: str,\n                 model_comparison_job_name: str, pipeline_root: str,\n                 service_account: str, network: str) -> None:\n  \"\"\"Starts a pipeline job.\n\n  Args:\n    project:\n      The GCP project that runs the pipeline components.\n    location:\n      The GCP region that runs the pipeline components.\n    model_name:\n      A user-defined string identyfing the model. Populated from the keys of\n      training_jobs dictionary passed to the model comparison pipeline.\n    job_params:\n      The parameters of the job. Populated from the values of training_jobs\n      dictionary passed to the model comparison pipeline.\n    experiment_name:\n      Name of the Vertex AI Experiment to which this pipeine job should be\n      bound.\n    model_comparison_job_name:\n      Name of the model comparison job running this component.\n    pipeline_root:\n      The root GCS directory for the pipeline components.\n    service_account: Specifies the service account for the pipeline job.\n    network: The full name of the Compute Engine network to which the\n      pipeline job should be peered.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\n  import os\n  import datetime\n  import logging\n  import uuid\n\n  from google.cloud import aiplatform\n  from google.cloud.aiplatform.metadata import constants as metadata_constants\n  from google.cloud.aiplatform.metadata import utils as metadata_utils\n  from google.cloud.aiplatform.compat.types import pipeline_state\n  from google.cloud.aiplatform.compat.types import execution\n  # pylint: enable=g-import-not-at-top,i;mport-outside-toplevel,redefined-outer-name,reimported\n\n  states_map = {\n      pipeline_state.PipelineState.PIPELINE_STATE_CANCELLED:\n          execution.Execution.State.CANCELLED,\n      pipeline_state.PipelineState.PIPELINE_STATE_FAILED:\n          execution.Execution.State.FAILED,\n      pipeline_state.PipelineState.PIPELINE_STATE_PAUSED:\n          execution.Execution.State.RUNNING,\n      pipeline_state.PipelineState.PIPELINE_STATE_RUNNING:\n          execution.Execution.State.RUNNING,\n      pipeline_state.PipelineState.PIPELINE_STATE_SUCCEEDED:\n          execution.Execution.State.COMPLETE\n  }\n\n  aiplatform.init(\n      project=project, location=location, experiment=experiment_name)\n\n  random_string = uuid.uuid4().hex[-4:]\n  job_id = f'{model_comparison_job_name}-{random_string}-{model_name}'\n  job = aiplatform.PipelineJob(\n      job_id=job_id,\n      display_name=job_id,\n      pipeline_root=os.path.join(pipeline_root, job_id),\n      **job_params,\n  )\n  experiment_run = aiplatform.ExperimentRun.create(\n      job_id,\n      experiment=experiment_name)\n\n  try:\n    job.submit(\n        service_account=None if service_account == '-' else service_account,\n        network=None if network == '-' else network)\n    job.wait()\n  except RuntimeError as err:\n    # job.wait() raises a RuntimeError if the job fails. We log the failure\n    # and continue with updating the ExperimentRun accordingly.\n    logging.warning('Job with ID %s failed: %s', job_id, err)\n\n  experiment_run.log(pipeline_job=job)\n  experiment_run.log_metrics({\n      'duration':\n          datetime.timedelta(\n              seconds=int(job.gca_resource.end_time.timestamp() -\n                          job.gca_resource.start_time.timestamp())).__str__(),\n  })\n  experiment_run.update_state(\n      state=states_map.get(job.state,\n                           execution.Execution.State.STATE_UNSPECIFIED))\n  job_metrics = aiplatform.Artifact.list(\n      filter=metadata_utils._make_filter_string(  # pylint: disable=protected-access\n          in_context=[job._get_context().resource_name],  # pylint: disable=protected-access\n          schema_title=[\n              metadata_constants.SYSTEM_METRICS,\n              metadata_constants.GOOGLE_CLASSIFICATION_METRICS,\n              metadata_constants.GOOGLE_REGRESSION_METRICS,\n              metadata_constants.GOOGLE_FORECASTING_METRICS,\n          ],\n      ))\n\n  for metric in job_metrics:\n    experiment_run.log_metrics(metric.metadata)\n\n"
            ],
            "image": "python:3.8-slim"
          }
        },
        "exec-validate-inputs": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "validate_inputs"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.14' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef validate_inputs(\n    project: str,\n    location: str,\n    prediction_type: str,\n    training_jobs: Dict[str, Dict[str, Any]],\n    data_source_csv_filenames: Optional[str] = None,\n    data_source_bigquery_table_path: Optional[str] = None,\n    evaluation_data_source_csv_filenames: Optional[str] = None,\n    evaluation_data_source_bigquery_table_path: Optional[str] = None,\n) -> None:\n  \"\"\"Checks training pipeline input parameters are valid.\"\"\"\n\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n  import re\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\n  bq_prefix = r'(bq\\:\\/\\/)?'\n  project_pattern = r'([a-z0-9.-]+:)?[a-z][a-z0-9-_]{4,28}[a-z0-9]'\n  location_pattern = r'[a-zA-Z0-9-_]+'\n  dataset_pattern = r'\\.[a-zA-Z0-9_]+'\n  table_pattern = r'\\.[^\\.\\:`]+'\n  gcs_csv_pattern = r'gs:\\/\\/(.+)\\/([^\\/]+)([a-zA-Z0-9_]+)\\.csv'\n  resource_pattern = r'[a-z0-9][a-z0-9-]{0,127}'\n\n  supported_prediction_types = [\n      'regression',\n      'classification',\n      'forecasting',\n  ]\n\n  # Validate project id.\n  project_id_pattern = re.compile(project_pattern)\n  if not project_id_pattern.fullmatch(project):\n    raise ValueError(f'Invalid project id: {project}.')\n\n  # Validate location.\n  if not re.compile(location_pattern).fullmatch(location):\n    raise ValueError(f'Invalid location: {location}.')\n\n  # Validate problem type.\n  if prediction_type.lower() not in supported_prediction_types:\n    raise ValueError(\n        f'Invalid prediction type provided: {prediction_type}. Must be one of '\n        f'the following: {supported_prediction_types}.')\n\n  # Validate training jobs.\n  first_model = True\n  predefined_split_key = None\n  timestamp_split_key = None\n  training_fraction = None\n  validation_fraction = None\n  window_column = None\n  window_stride_length = None\n  window_max_count = None\n\n  # training_jobs is a mapping of model names to job parameters.\n  for model_name, job_params in training_jobs.items():\n    # Verify that model_name matches the VertexAI resource name regex. This is\n    # needed because we use it when constructing job ids for sub-pipelines.\n    if not re.compile(resource_pattern).fullmatch(model_name):\n      raise ValueError(\n          f'Invalid model name: {model_name}. Name does not match regex pattern of {resource_pattern}'\n      )\n\n    # Verify that template_path exists and it points to a json file.\n    if 'template_path' not in job_params:\n      raise ValueError(\n          f'Invalid training job provided: {job_params}. No template_path present in training job parameters.'\n      )\n\n    template_path = job_params['template_path']\n    if not isinstance(template_path, str):\n      raise ValueError(\n          f'Invalid training job provided: {job_params}. Expecting template_path to be string, got {type(template_path)}'\n      )\n\n    if not template_path.endswith('.json'):\n      raise ValueError(\n          f'Invalid training job provided: {job_params}. Expecting template_path to point to a json file, got {template_path}'\n      )\n\n    # Verify that parameter_values field exists in training job.\n    if 'parameter_values' not in job_params:\n      raise ValueError(\n          f'Invalid training job provided: {job_params}. No parameter_values field provided.'\n      )\n\n    parameter_values = job_params['parameter_values']\n    if not isinstance(parameter_values, dict):\n      raise ValueError(\n          f'Invalid training job provided: {job_params}. Expecting parameter_values to be dictionary, got {type(parameter_values)}'\n      )\n\n    # Verify that no data_source is specified in parameter_values.\n    if 'data_source' in parameter_values and parameter_values[\n        'data_source'] is not None:\n      raise ValueError(\n          f'Invalid training job provided: {job_params}. No data_source is allowed under parameter_values field.'\n      )\n\n    # Verify that project matches the project value in pipeline.\n    if 'project' in parameter_values:\n      if parameter_values['project'] != project:\n        raise ValueError(\n            f'Invalid training job provided: {job_params}. Project value {job_params[\"project\"]} does not match project value in the model comparison pipeline - {project}.'\n        )\n\n    # Verify that location matches the location value in pipeline.\n    if 'location' in parameter_values:\n      if parameter_values['location'] != location:\n        raise ValueError(\n            f'Invalid training job provided: {job_params}. Location value {job_params[\"location\"]} does not match location value in the model comparison pipeline - {location}.'\n        )\n\n    # Verify that split_spec and window config are the same across all models.\n    if 'predefined_split_key' in parameter_values:\n      if first_model:\n        predefined_split_key = parameter_values['predefined_split_key']\n      elif predefined_split_key != parameter_values['predefined_split_key']:\n        raise ValueError(\n            f'Expecting the same predefined_split_key value {predefined_split_key}, got {parameter_values[\"predefined_split_key\"]}'\n        )\n\n    if 'timestamp_split_key' in parameter_values:\n      if first_model:\n        timestamp_split_key = parameter_values['timestamp_split_key']\n      elif timestamp_split_key != parameter_values['timestamp_split_key']:\n        raise ValueError(\n            f'Expecting the same timestamp_split_key value {timestamp_split_key}, got {parameter_values[\"timestamp_split_key\"]}'\n        )\n\n    if 'training_fraction' in parameter_values:\n      if first_model:\n        training_fraction = parameter_values['training_fraction']\n      elif training_fraction != parameter_values['training_fraction']:\n        raise ValueError(\n            f'Expecting the same training_fraction value {training_fraction}, got {parameter_values[\"training_fraction\"]}'\n        )\n\n    if 'validation_fraction' in parameter_values:\n      if first_model:\n        validation_fraction = parameter_values['validation_fraction']\n      elif validation_fraction != parameter_values['validation_fraction']:\n        raise ValueError(\n            f'Expecting the same validation_fraction value {validation_fraction}, got {parameter_values[\"validation_fraction\"]}'\n        )\n\n    if 'window_column' in parameter_values:\n      if first_model:\n        window_column = parameter_values['window_column']\n      elif window_column != parameter_values['window_column']:\n        raise ValueError(\n            f'Expecting the same window_column value {window_column}, got {parameter_values[\"window_column\"]}'\n        )\n\n    if 'window_stride_length' in parameter_values:\n      if first_model:\n        window_stride_length = parameter_values['window_stride_length']\n      elif window_stride_length != parameter_values['window_stride_length']:\n        raise ValueError(\n            f'Expecting the same window_stride_length value {window_stride_length}, got {parameter_values[\"window_stride_length\"]}'\n        )\n\n    if 'window_max_count' in parameter_values:\n      if first_model:\n        window_max_count = parameter_values['window_max_count']\n      elif window_max_count != parameter_values['window_max_count']:\n        raise ValueError(\n            f'Expecting the same window_max_count value {window_max_count}, got {parameter_values[\"window_max_count\"]}'\n        )\n\n    first_model = False\n\n  # Validate data sources.\n  if data_source_csv_filenames == '-':\n    data_source_csv_filenames = None\n  if data_source_bigquery_table_path == '-':\n    data_source_bigquery_table_path = None\n  if evaluation_data_source_csv_filenames == '-':\n    evaluation_data_source_csv_filenames = None\n  if evaluation_data_source_bigquery_table_path == '-':\n    evaluation_data_source_bigquery_table_path = None\n\n  if data_source_csv_filenames and data_source_bigquery_table_path:\n    raise ValueError(\n        'Both CSV data source and BigQuery data source are provided. Only one data source allowed.'\n    )\n\n  # Validate CSV paths from GCS.\n  if data_source_csv_filenames:\n    gcs_path_pattern = re.compile(gcs_csv_pattern)\n    for gcs_csv in data_source_csv_filenames.split(','):\n      if not gcs_path_pattern.fullmatch(gcs_csv.strip()):\n        raise ValueError(\n            f'Invalid GCS CSV file path: {gcs_csv}. Path does not match regex pattern of {gcs_csv_pattern}'\n        )\n\n  # Validate bigquery table path.\n  if data_source_bigquery_table_path:\n    table_uri_pattern = re.compile(bq_prefix + project_pattern +\n                                   dataset_pattern + table_pattern)\n    if not table_uri_pattern.fullmatch(data_source_bigquery_table_path):\n      raise ValueError(\n          f'Invalid BigQuery table URI: {data_source_bigquery_table_path}.')\n\n  # Validate evaluation datasets.\n  evaluation_data_sources = [\n      evaluation_data_source_csv_filenames,\n      evaluation_data_source_bigquery_table_path\n  ]\n  if all(evaluation_data_sources) or not any(evaluation_data_sources):\n    raise ValueError(\n        f'Wrong number of evaluation data sources provided: must provide exactly one of evaluation_data_source_csv_filenames and evaluation_data_source_bigquery_table_path. Evaluation csv {evaluation_data_source_csv_filenames}, evaluation bq {evaluation_data_source_bigquery_table_path}.'\n    )\n\n  if (data_source_csv_filenames and evaluation_data_source_bigquery_table_path\n     ) or (data_source_bigquery_table_path and\n           evaluation_data_source_csv_filenames):\n    raise ValueError(\n        'Cannot support mismatch input types. Two input sources need to be the same type.'\n    )\n\n"
            ],
            "image": "python:3.8-slim"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "model-comparison"
    },
    "root": {
      "dag": {
        "tasks": {
          "bigquery-delete-dataset-with-prefix": {
            "componentRef": {
              "name": "comp-bigquery-delete-dataset-with-prefix"
            },
            "dependentTasks": [
              "exit-handler-1"
            ],
            "inputs": {
              "parameters": {
                "dataset_prefix": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "model_comparison_tmp_{{$.pipeline_job_uuid}}"
                    }
                  }
                },
                "delete_contents": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "1"
                    }
                  }
                },
                "project": {
                  "componentInputParameter": "project"
                }
              }
            },
            "taskInfo": {
              "name": "delete-tmp-dataset"
            },
            "triggerPolicy": {
              "strategy": "ALL_UPSTREAM_TASKS_COMPLETED"
            }
          },
          "exit-handler-1": {
            "componentRef": {
              "name": "comp-exit-handler-1"
            },
            "inputs": {
              "parameters": {
                "pipelineparam--data_source_bigquery_table_path": {
                  "componentInputParameter": "data_source_bigquery_table_path"
                },
                "pipelineparam--data_source_csv_filenames": {
                  "componentInputParameter": "data_source_csv_filenames"
                },
                "pipelineparam--evaluation_data_source_bigquery_table_path": {
                  "componentInputParameter": "evaluation_data_source_bigquery_table_path"
                },
                "pipelineparam--evaluation_data_source_csv_filenames": {
                  "componentInputParameter": "evaluation_data_source_csv_filenames"
                },
                "pipelineparam--experiment": {
                  "componentInputParameter": "experiment"
                },
                "pipelineparam--location": {
                  "componentInputParameter": "location"
                },
                "pipelineparam--network": {
                  "componentInputParameter": "network"
                },
                "pipelineparam--prediction_type": {
                  "componentInputParameter": "prediction_type"
                },
                "pipelineparam--project": {
                  "componentInputParameter": "project"
                },
                "pipelineparam--root_dir": {
                  "componentInputParameter": "root_dir"
                },
                "pipelineparam--service_account": {
                  "componentInputParameter": "service_account"
                },
                "pipelineparam--training_jobs": {
                  "componentInputParameter": "training_jobs"
                }
              }
            },
            "taskInfo": {
              "name": "exit-handler-1"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "data_source_bigquery_table_path": {
            "type": "STRING"
          },
          "data_source_csv_filenames": {
            "type": "STRING"
          },
          "evaluation_data_source_bigquery_table_path": {
            "type": "STRING"
          },
          "evaluation_data_source_csv_filenames": {
            "type": "STRING"
          },
          "experiment": {
            "type": "STRING"
          },
          "location": {
            "type": "STRING"
          },
          "network": {
            "type": "STRING"
          },
          "prediction_type": {
            "type": "STRING"
          },
          "project": {
            "type": "STRING"
          },
          "root_dir": {
            "type": "STRING"
          },
          "service_account": {
            "type": "STRING"
          },
          "training_jobs": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.14"
  },
  "runtimeConfig": {}
}