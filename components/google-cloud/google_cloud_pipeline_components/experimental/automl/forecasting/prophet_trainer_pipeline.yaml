# PIPELINE DEFINITION
# Name: prophet-train
# Description: Trains one Prophet model per time series.
# Inputs:
#    data_granularity_unit: str
#    data_source_bigquery_table_path: str [Default: '']
#    data_source_csv_filenames: str [Default: '']
#    dataflow_service_account: str [Default: '']
#    dataflow_subnetwork: str [Default: '']
#    dataflow_use_public_ips: bool [Default: True]
#    encryption_spec_key_name: str [Default: '']
#    evaluation_dataflow_disk_size_gb: int [Default: 40.0]
#    evaluation_dataflow_machine_type: str [Default: 'n1-standard-1']
#    evaluation_dataflow_max_num_workers: int [Default: 10.0]
#    forecast_horizon: int
#    location: str
#    max_num_trials: int [Default: 6.0]
#    optimization_objective: str
#    predefined_split_key: str [Default: '']
#    project: str
#    root_dir: str
#    target_column: str
#    test_fraction: float [Default: -1.0]
#    time_column: str
#    time_series_identifier_column: str
#    timestamp_split_key: str [Default: '']
#    trainer_dataflow_disk_size_gb: int [Default: 40.0]
#    trainer_dataflow_machine_type: str [Default: 'n1-standard-1']
#    trainer_dataflow_max_num_workers: int [Default: 10.0]
#    training_fraction: float [Default: -1.0]
#    validation_fraction: float [Default: -1.0]
#    window_column: str [Default: '']
#    window_max_count: int [Default: -1.0]
#    window_stride_length: int [Default: -1.0]
components:
  comp-bigquery-create-dataset:
    executorLabel: exec-bigquery-create-dataset
    inputDefinitions:
      parameters:
        dataset:
          parameterType: STRING
        exists_ok:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        location:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-bigquery-delete-dataset-with-prefix:
    executorLabel: exec-bigquery-delete-dataset-with-prefix
    inputDefinitions:
      parameters:
        dataset_prefix:
          parameterType: STRING
        delete_contents:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        project:
          parameterType: STRING
  comp-bigquery-query-job:
    executorLabel: exec-bigquery-query-job
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          isOptional: true
          parameterType: STRING
        job_configuration_query:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
        query:
          parameterType: STRING
        query_parameters:
          defaultValue: []
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        destination_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-build-job-configuration-query:
    executorLabel: exec-build-job-configuration-query
    inputDefinitions:
      parameters:
        dataset_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        priority:
          defaultValue: INTERACTIVE
          isOptional: true
          parameterType: STRING
        project_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        table_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        write_disposition:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-exit-handler-1:
    dag:
      tasks:
        bigquery-create-dataset:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-create-dataset
          dependentTasks:
          - get-table-location
          - validate-inputs
          inputs:
            parameters:
              dataset:
                runtimeValue:
                  constant: tmp_{{$.pipeline_job_uuid}}
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: create-tmp-dataset
        bigquery-query-job:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-query-job
          dependentTasks:
          - bigquery-create-dataset
          - build-job-configuration-query
          - get-fte-suffix
          - get-table-location
          inputs:
            parameters:
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              job_configuration_query:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: build-job-configuration-query
              location:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-table-location
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--get-fte-suffix-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-fte-suffix
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
              pipelinechannel--time_column:
                componentInputParameter: pipelinechannel--time_column
              pipelinechannel--time_series_identifier_column:
                componentInputParameter: pipelinechannel--time_series_identifier_column
              project:
                componentInputParameter: pipelinechannel--project
              query:
                runtimeValue:
                  constant: "\n      WITH\n        base_data AS (\n          SELECT\
                    \ * FROM `{{$.inputs.parameters['pipelinechannel--bigquery-create-dataset-project_id']}}.{{$.inputs.parameters['pipelinechannel--bigquery-create-dataset-dataset_id']}}.fte_time_series_output_{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}}`\n\
                    \        )\n      SELECT\n        CAST({{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}}\
                    \ AS STRING) AS {{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}},\n\
                    \        ARRAY_AGG(TIMESTAMP({{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ ORDER BY {{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ AS {{$.inputs.parameters['pipelinechannel--time_column']}},\n\
                    \        ARRAY_AGG({{$.inputs.parameters['pipelinechannel--target_column']}}\
                    \ ORDER BY {{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ AS {{$.inputs.parameters['pipelinechannel--target_column']}},\n\
                    \        ARRAY_AGG(split__{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}}\
                    \ ORDER BY {{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ AS split__{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}},\n\
                    \        ARRAY_AGG(window__{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}}\
                    \ ORDER BY {{$.inputs.parameters['pipelinechannel--time_column']}})\
                    \ AS window__{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}},\n\
                    \      FROM base_data\n      GROUP BY {{$.inputs.parameters['pipelinechannel--time_series_identifier_column']}}\n\
                    \  "
          taskInfo:
            name: aggregate-by-time-series-id
        build-job-configuration-query:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-build-job-configuration-query
          dependentTasks:
          - bigquery-create-dataset
          inputs:
            parameters:
              dataset_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-dataset_id'']}}'
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              project_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-project_id'']}}'
              table_id:
                runtimeValue:
                  constant: data
              write_disposition:
                runtimeValue:
                  constant: WRITE_EMPTY
          taskInfo:
            name: build-job-configuration-query
        feature-transform-engine:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-feature-transform-engine
          dependentTasks:
          - bigquery-create-dataset
          inputs:
            parameters:
              autodetect_csv_schema:
                runtimeValue:
                  constant: 1.0
              bigquery_staging_full_dataset_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-project_id'']}}.{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-dataset_id'']}}'
              data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
              data_source_csv_filenames:
                componentInputParameter: pipelinechannel--data_source_csv_filenames
              forecasting_apply_windowing:
                runtimeValue:
                  constant: 0.0
              forecasting_context_window:
                runtimeValue:
                  constant: 0.0
              forecasting_forecast_horizon:
                componentInputParameter: pipelinechannel--forecast_horizon
              forecasting_predefined_window_column:
                componentInputParameter: pipelinechannel--window_column
              forecasting_time_column:
                componentInputParameter: pipelinechannel--time_column
              forecasting_time_series_identifier_column:
                componentInputParameter: pipelinechannel--time_series_identifier_column
              forecasting_window_max_count:
                componentInputParameter: pipelinechannel--window_max_count
              forecasting_window_stride_length:
                componentInputParameter: pipelinechannel--window_stride_length
              location:
                componentInputParameter: pipelinechannel--location
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              predefined_split_key:
                componentInputParameter: pipelinechannel--predefined_split_key
              prediction_type:
                runtimeValue:
                  constant: time_series
              project:
                componentInputParameter: pipelinechannel--project
              root_dir:
                componentInputParameter: pipelinechannel--root_dir
              target_column:
                componentInputParameter: pipelinechannel--target_column
              test_fraction:
                componentInputParameter: pipelinechannel--test_fraction
              timestamp_split_key:
                componentInputParameter: pipelinechannel--timestamp_split_key
              training_fraction:
                componentInputParameter: pipelinechannel--training_fraction
              validation_fraction:
                componentInputParameter: pipelinechannel--validation_fraction
          taskInfo:
            name: feature-transform-engine
        get-fte-suffix:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-fte-suffix
          dependentTasks:
          - bigquery-create-dataset
          - feature-transform-engine
          inputs:
            parameters:
              bigquery_staging_full_dataset_id:
                runtimeValue:
                  constant: '{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-project_id'']}}.{{$.inputs.parameters[''pipelinechannel--bigquery-create-dataset-dataset_id'']}}'
              fte_table:
                runtimeValue:
                  constant: fte_time_series_output
              location:
                componentInputParameter: pipelinechannel--location
              pipelinechannel--bigquery-create-dataset-dataset_id:
                taskOutputParameter:
                  outputParameterKey: dataset_id
                  producerTask: bigquery-create-dataset
              pipelinechannel--bigquery-create-dataset-project_id:
                taskOutputParameter:
                  outputParameterKey: project_id
                  producerTask: bigquery-create-dataset
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: get-fte-suffix
        get-table-location:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-table-location
          inputs:
            parameters:
              default_location:
                componentInputParameter: pipelinechannel--location
              project:
                componentInputParameter: pipelinechannel--project
              table:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
          taskInfo:
            name: get-table-location
        model-evaluation-regression:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-evaluation-regression
          dependentTasks:
          - prophet-trainer
          inputs:
            artifacts:
              predictions_gcs_source:
                taskOutputArtifact:
                  outputArtifactKey: evaluated_examples_directory
                  producerTask: prophet-trainer
            parameters:
              dataflow_disk_size:
                componentInputParameter: pipelinechannel--evaluation_dataflow_disk_size_gb
              dataflow_machine_type:
                componentInputParameter: pipelinechannel--evaluation_dataflow_machine_type
              dataflow_max_workers_num:
                componentInputParameter: pipelinechannel--evaluation_dataflow_max_num_workers
              dataflow_service_account:
                componentInputParameter: pipelinechannel--dataflow_service_account
              dataflow_subnetwork:
                componentInputParameter: pipelinechannel--dataflow_subnetwork
              dataflow_use_public_ips:
                componentInputParameter: pipelinechannel--dataflow_use_public_ips
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              ground_truth_gcs_source:
                runtimeValue:
                  constant: []
              location:
                componentInputParameter: pipelinechannel--location
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
              prediction_score_column:
                runtimeValue:
                  constant: prediction.predicted_{{$.inputs.parameters['pipelinechannel--target_column']}}
              predictions_format:
                runtimeValue:
                  constant: jsonl
              project:
                componentInputParameter: pipelinechannel--project
              root_dir:
                componentInputParameter: pipelinechannel--root_dir
              target_field_name:
                componentInputParameter: pipelinechannel--target_column
          taskInfo:
            name: model-evaluation-regression
        model-upload:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-upload
          dependentTasks:
          - prophet-trainer
          inputs:
            artifacts:
              unmanaged_container_model:
                taskOutputArtifact:
                  outputArtifactKey: unmanaged_container_model
                  producerTask: prophet-trainer
            parameters:
              description:
                runtimeValue:
                  constant: Prophet model.
              display_name:
                runtimeValue:
                  constant: prophet_{{$.pipeline_job_uuid}}
              location:
                componentInputParameter: pipelinechannel--location
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: model-upload
        prophet-trainer:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-prophet-trainer
          dependentTasks:
          - get-fte-suffix
          - table-to-uri
          inputs:
            parameters:
              data_granularity_unit:
                componentInputParameter: pipelinechannel--data_granularity_unit
              dataflow_disk_size_gb:
                componentInputParameter: pipelinechannel--trainer_dataflow_disk_size_gb
              dataflow_machine_type:
                componentInputParameter: pipelinechannel--trainer_dataflow_machine_type
              dataflow_max_num_workers:
                componentInputParameter: pipelinechannel--trainer_dataflow_max_num_workers
              dataflow_service_account:
                componentInputParameter: pipelinechannel--dataflow_service_account
              dataflow_subnetwork:
                componentInputParameter: pipelinechannel--dataflow_subnetwork
              dataflow_use_public_ips:
                componentInputParameter: pipelinechannel--dataflow_use_public_ips
              encryption_spec_key_name:
                componentInputParameter: pipelinechannel--encryption_spec_key_name
              forecast_horizon:
                componentInputParameter: pipelinechannel--forecast_horizon
              location:
                componentInputParameter: pipelinechannel--location
              max_num_trials:
                componentInputParameter: pipelinechannel--max_num_trials
              optimization_objective:
                componentInputParameter: pipelinechannel--optimization_objective
              pipelinechannel--get-fte-suffix-Output:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-fte-suffix
              pipelinechannel--table-to-uri-uri:
                taskOutputParameter:
                  outputParameterKey: uri
                  producerTask: table-to-uri
              predefined_split_column:
                runtimeValue:
                  constant: split__{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}}
              project:
                componentInputParameter: pipelinechannel--project
              root_dir:
                componentInputParameter: pipelinechannel--root_dir
              source_bigquery_uri:
                runtimeValue:
                  constant: bq://{{$.inputs.parameters['pipelinechannel--table-to-uri-uri']}}
              target_column:
                componentInputParameter: pipelinechannel--target_column
              time_column:
                componentInputParameter: pipelinechannel--time_column
              time_series_identifier_column:
                componentInputParameter: pipelinechannel--time_series_identifier_column
              window_column:
                runtimeValue:
                  constant: window__{{$.inputs.parameters['pipelinechannel--get-fte-suffix-Output']}}
          taskInfo:
            name: prophet-trainer
        table-to-uri:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-table-to-uri
          dependentTasks:
          - bigquery-query-job
          inputs:
            artifacts:
              table:
                taskOutputArtifact:
                  outputArtifactKey: destination_table
                  producerTask: bigquery-query-job
          taskInfo:
            name: table-to-uri
        validate-inputs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-validate-inputs
          inputs:
            parameters:
              data_granularity_unit:
                componentInputParameter: pipelinechannel--data_granularity_unit
              data_source_bigquery_table_path:
                componentInputParameter: pipelinechannel--data_source_bigquery_table_path
              data_source_csv_filenames:
                componentInputParameter: pipelinechannel--data_source_csv_filenames
              optimization_objective:
                componentInputParameter: pipelinechannel--optimization_objective
              predefined_split_key:
                componentInputParameter: pipelinechannel--predefined_split_key
              target_column:
                componentInputParameter: pipelinechannel--target_column
              test_fraction:
                componentInputParameter: pipelinechannel--test_fraction
              time_column:
                componentInputParameter: pipelinechannel--time_column
              time_series_identifier_column:
                componentInputParameter: pipelinechannel--time_series_identifier_column
              timestamp_split_key:
                componentInputParameter: pipelinechannel--timestamp_split_key
              training_fraction:
                componentInputParameter: pipelinechannel--training_fraction
              validation_fraction:
                componentInputParameter: pipelinechannel--validation_fraction
              window_column:
                componentInputParameter: pipelinechannel--window_column
              window_max_count:
                componentInputParameter: pipelinechannel--window_max_count
              window_stride_length:
                componentInputParameter: pipelinechannel--window_stride_length
          taskInfo:
            name: validate-inputs
    inputDefinitions:
      parameters:
        pipelinechannel--data_granularity_unit:
          parameterType: STRING
        pipelinechannel--data_source_bigquery_table_path:
          parameterType: STRING
        pipelinechannel--data_source_csv_filenames:
          parameterType: STRING
        pipelinechannel--dataflow_service_account:
          parameterType: STRING
        pipelinechannel--dataflow_subnetwork:
          parameterType: STRING
        pipelinechannel--dataflow_use_public_ips:
          parameterType: BOOLEAN
        pipelinechannel--encryption_spec_key_name:
          parameterType: STRING
        pipelinechannel--evaluation_dataflow_disk_size_gb:
          parameterType: NUMBER_INTEGER
        pipelinechannel--evaluation_dataflow_machine_type:
          parameterType: STRING
        pipelinechannel--evaluation_dataflow_max_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--forecast_horizon:
          parameterType: NUMBER_INTEGER
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--max_num_trials:
          parameterType: NUMBER_INTEGER
        pipelinechannel--optimization_objective:
          parameterType: STRING
        pipelinechannel--predefined_split_key:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--root_dir:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
        pipelinechannel--test_fraction:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--time_column:
          parameterType: STRING
        pipelinechannel--time_series_identifier_column:
          parameterType: STRING
        pipelinechannel--timestamp_split_key:
          parameterType: STRING
        pipelinechannel--trainer_dataflow_disk_size_gb:
          parameterType: NUMBER_INTEGER
        pipelinechannel--trainer_dataflow_machine_type:
          parameterType: STRING
        pipelinechannel--trainer_dataflow_max_num_workers:
          parameterType: NUMBER_INTEGER
        pipelinechannel--training_fraction:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--validation_fraction:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--window_column:
          parameterType: STRING
        pipelinechannel--window_max_count:
          parameterType: NUMBER_INTEGER
        pipelinechannel--window_stride_length:
          parameterType: NUMBER_INTEGER
  comp-feature-transform-engine:
    executorLabel: exec-feature-transform-engine
    inputDefinitions:
      parameters:
        autodetect_csv_schema:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        bigquery_staging_full_dataset_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        data_source_bigquery_table_path:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        data_source_csv_filenames:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_disk_size_gb:
          defaultValue: 40.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_machine_type:
          defaultValue: n1-standard-16
          isOptional: true
          parameterType: STRING
        dataflow_max_num_workers:
          defaultValue: 25.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_service_account:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_subnetwork:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_use_public_ips:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        dataset_level_custom_transformation_definitions:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        dataset_level_transformations:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        feature_selection_algorithm:
          defaultValue: AMI
          isOptional: true
          parameterType: STRING
        forecasting_apply_windowing:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        forecasting_available_at_forecast_columns:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecasting_context_window:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_forecast_horizon:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_predefined_window_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecasting_time_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecasting_time_series_attribute_columns:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecasting_time_series_identifier_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecasting_unavailable_at_forecast_columns:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecasting_window_max_count:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        forecasting_window_stride_length:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        legacy_transformations_path:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        location:
          parameterType: STRING
        materialized_examples_format:
          defaultValue: tfrecords_gzip
          isOptional: true
          parameterType: STRING
        max_selected_features:
          defaultValue: 1000.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_type:
          isOptional: true
          parameterType: STRING
        predefined_split_key:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        prediction_type:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
        root_dir:
          parameterType: STRING
        run_distill:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        run_feature_selection:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        stratified_split_key:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        target_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        test_fraction:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        tf_auto_transform_features:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        tf_custom_transformation_definitions:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        tf_transform_execution_engine:
          defaultValue: dataflow
          isOptional: true
          parameterType: STRING
        tf_transformations_path:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        timestamp_split_key:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        training_fraction:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        validation_fraction:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        weight_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_stats:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        feature_ranking:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        instance_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        materialized_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        training_schema:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        transform_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        bigquery_downsampled_test_split_uri:
          parameterType: STRING
        bigquery_test_split_uri:
          parameterType: STRING
        gcp_resources:
          parameterType: STRING
        split_example_counts:
          parameterType: STRING
  comp-get-fte-suffix:
    executorLabel: exec-get-fte-suffix
    inputDefinitions:
      parameters:
        bigquery_staging_full_dataset_id:
          parameterType: STRING
        fte_table:
          parameterType: STRING
        location:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-table-location:
    executorLabel: exec-get-table-location
    inputDefinitions:
      parameters:
        default_location:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
        table:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-model-evaluation-regression:
    executorLabel: exec-model-evaluation-regression
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          isOptional: true
        predictions_bigquery_source:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
          isOptional: true
        predictions_gcs_source:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          isOptional: true
      parameters:
        dataflow_disk_size:
          defaultValue: 50.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_machine_type:
          defaultValue: n1-standard-4
          isOptional: true
          parameterType: STRING
        dataflow_max_workers_num:
          defaultValue: 5.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_service_account:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_subnetwork:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_use_public_ips:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        dataflow_workers_num:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        example_weight_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        ground_truth_bigquery_source:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        ground_truth_format:
          defaultValue: jsonl
          isOptional: true
          parameterType: STRING
        ground_truth_gcs_source:
          defaultValue: {}
          isOptional: true
          parameterType: LIST
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        prediction_score_column:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        predictions_format:
          defaultValue: jsonl
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
        root_dir:
          parameterType: STRING
        target_field_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        evaluation_metrics:
          artifactType:
            schemaTitle: google.RegressionMetrics
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-model-upload:
    executorLabel: exec-model-upload
    inputDefinitions:
      artifacts:
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          isOptional: true
      parameters:
        artifact_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        description:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        display_name:
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        explanation_metadata:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        instance_schema_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        labels:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        parameters_schema_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        prediction_schema_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        project:
          parameterType: STRING
        serving_container_args:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        serving_container_command:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        serving_container_environment_variables:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        serving_container_health_route:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        serving_container_image_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        serving_container_ports:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        serving_container_predict_route:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-prophet-trainer:
    executorLabel: exec-prophet-trainer
    inputDefinitions:
      parameters:
        data_granularity_unit:
          parameterType: STRING
        dataflow_disk_size_gb:
          defaultValue: 40.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_machine_type:
          defaultValue: n1-standard-1
          isOptional: true
          parameterType: STRING
        dataflow_max_num_workers:
          defaultValue: 10.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataflow_service_account:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_subnetwork:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        dataflow_use_public_ips:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        forecast_horizon:
          parameterType: NUMBER_INTEGER
        location:
          parameterType: STRING
        max_num_trials:
          defaultValue: 6.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        optimization_objective:
          defaultValue: rmse
          isOptional: true
          parameterType: STRING
        predefined_split_column:
          parameterType: STRING
        project:
          parameterType: STRING
        root_dir:
          parameterType: STRING
        source_bigquery_uri:
          parameterType: STRING
        target_column:
          parameterType: STRING
        time_column:
          parameterType: STRING
        time_series_identifier_column:
          parameterType: STRING
        window_column:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        evaluated_examples_directory:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-table-to-uri:
    executorLabel: exec-table-to-uri
    inputDefinitions:
      artifacts:
        table:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        use_bq_prefix:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
    outputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
        table_id:
          parameterType: STRING
        uri:
          parameterType: STRING
  comp-validate-inputs:
    executorLabel: exec-validate-inputs
    inputDefinitions:
      parameters:
        bigquery_destination_uri:
          isOptional: true
          parameterType: STRING
        data_granularity_unit:
          isOptional: true
          parameterType: STRING
        data_source_bigquery_table_path:
          isOptional: true
          parameterType: STRING
        data_source_csv_filenames:
          isOptional: true
          parameterType: STRING
        optimization_objective:
          isOptional: true
          parameterType: STRING
        predefined_split_key:
          isOptional: true
          parameterType: STRING
        source_model_uri:
          isOptional: true
          parameterType: STRING
        target_column:
          isOptional: true
          parameterType: STRING
        test_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        time_column:
          isOptional: true
          parameterType: STRING
        time_series_identifier_column:
          isOptional: true
          parameterType: STRING
        timestamp_split_key:
          isOptional: true
          parameterType: STRING
        training_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        validation_fraction:
          isOptional: true
          parameterType: NUMBER_DOUBLE
        window_column:
          isOptional: true
          parameterType: STRING
        window_max_count:
          isOptional: true
          parameterType: NUMBER_INTEGER
        window_stride_length:
          isOptional: true
          parameterType: NUMBER_INTEGER
deploymentSpec:
  executors:
    exec-bigquery-create-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_create_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_create_dataset(\n    project: str,\n    location: str,\n\
          \    dataset: str,\n    exists_ok: bool = False,\n) -> NamedTuple('Outputs',\
          \ [('project_id', str), ('dataset_id', str)]):\n  \"\"\"Creates a BigQuery\
          \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
          \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project, location=location)\n  ref\
          \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
          \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
          \   ref.project, ref.dataset_id)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
    exec-bigquery-delete-dataset-with-prefix:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bigquery_delete_dataset_with_prefix
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bigquery_delete_dataset_with_prefix(\n    project: str,\n   \
          \ dataset_prefix: str,\n    delete_contents: bool = False,\n) -> None:\n\
          \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
          \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
          \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
          \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
    exec-bigquery-query-job:
      container:
        args:
        - --type
        - BigqueryQueryJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --payload
        - '{"Concat": ["{", "\"configuration\": {", "\"query\": ", "{{$.inputs.parameters[''job_configuration_query'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}", "}"]}'
        - --job_configuration_query_override
        - '{"Concat": ["{", "\"query\": \"", "{{$.inputs.parameters[''query'']}}",
          "\"", ", \"query_parameters\": ", "{{$.inputs.parameters[''query_parameters'']}}",
          ", \"destination_encryption_configuration\": {", "\"kmsKeyName\": \"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.bigquery.query_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.33
    exec-build-job-configuration-query:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - build_job_configuration_query
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef build_job_configuration_query(\n    project_id: str = '',\n \
          \   dataset_id: str = '',\n    table_id: str = '',\n    write_disposition:\
          \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
          \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
          \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
          \    config['destinationTable'] = {\n        'projectId': project_id,\n\
          \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
          \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
          \  return config\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
    exec-feature-transform-engine:
      container:
        args:
        - feature_transform_engine
        - '{"Concat": ["--project=", "{{$.inputs.parameters[''project'']}}"]}'
        - '{"Concat": ["--location=", "{{$.inputs.parameters[''location'']}}"]}'
        - '{"Concat": ["--dataset_level_custom_transformation_definitions=", "{{$.inputs.parameters[''dataset_level_custom_transformation_definitions'']}}"]}'
        - '{"Concat": ["--dataset_level_transformations=", "{{$.inputs.parameters[''dataset_level_transformations'']}}"]}'
        - '{"Concat": ["--forecasting_time_column=", "{{$.inputs.parameters[''forecasting_time_column'']}}"]}'
        - '{"Concat": ["--forecasting_time_series_identifier_column=", "{{$.inputs.parameters[''forecasting_time_series_identifier_column'']}}"]}'
        - '{"Concat": ["--forecasting_time_series_attribute_columns=", "{{$.inputs.parameters[''forecasting_time_series_attribute_columns'']}}"]}'
        - '{"Concat": ["--forecasting_unavailable_at_forecast_columns=", "{{$.inputs.parameters[''forecasting_unavailable_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--forecasting_available_at_forecast_columns=", "{{$.inputs.parameters[''forecasting_available_at_forecast_columns'']}}"]}'
        - '{"Concat": ["--forecasting_forecast_horizon=", "{{$.inputs.parameters[''forecasting_forecast_horizon'']}}"]}'
        - '{"Concat": ["--forecasting_context_window=", "{{$.inputs.parameters[''forecasting_context_window'']}}"]}'
        - '{"Concat": ["--forecasting_predefined_window_column=", "{{$.inputs.parameters[''forecasting_predefined_window_column'']}}"]}'
        - '{"Concat": ["--forecasting_window_stride_length=", "{{$.inputs.parameters[''forecasting_window_stride_length'']}}"]}'
        - '{"Concat": ["--forecasting_window_max_count=", "{{$.inputs.parameters[''forecasting_window_max_count'']}}"]}'
        - '{"Concat": ["--forecasting_apply_windowing=", "{{$.inputs.parameters[''forecasting_apply_windowing'']}}"]}'
        - '{"Concat": ["--predefined_split_key=", "{{$.inputs.parameters[''predefined_split_key'']}}"]}'
        - '{"Concat": ["--stratified_split_key=", "{{$.inputs.parameters[''stratified_split_key'']}}"]}'
        - '{"Concat": ["--timestamp_split_key=", "{{$.inputs.parameters[''timestamp_split_key'']}}"]}'
        - '{"Concat": ["--training_fraction=", "{{$.inputs.parameters[''training_fraction'']}}"]}'
        - '{"Concat": ["--validation_fraction=", "{{$.inputs.parameters[''validation_fraction'']}}"]}'
        - '{"Concat": ["--test_fraction=", "{{$.inputs.parameters[''test_fraction'']}}"]}'
        - '{"Concat": ["--tf_transform_execution_engine=", "{{$.inputs.parameters[''tf_transform_execution_engine'']}}"]}'
        - '{"Concat": ["--tf_auto_transform_features=", "{{$.inputs.parameters[''tf_auto_transform_features'']}}"]}'
        - '{"Concat": ["--tf_custom_transformation_definitions=", "{{$.inputs.parameters[''tf_custom_transformation_definitions'']}}"]}'
        - '{"Concat": ["--tf_transformations_path=", "{{$.inputs.parameters[''tf_transformations_path'']}}"]}'
        - '{"Concat": ["--legacy_transformations_path=", "{{$.inputs.parameters[''legacy_transformations_path'']}}"]}'
        - '{"Concat": ["--data_source_csv_filenames=", "{{$.inputs.parameters[''data_source_csv_filenames'']}}"]}'
        - '{"Concat": ["--data_source_bigquery_table_path=", "{{$.inputs.parameters[''data_source_bigquery_table_path'']}}"]}'
        - '{"Concat": ["--bigquery_staging_full_dataset_id=", "{{$.inputs.parameters[''bigquery_staging_full_dataset_id'']}}"]}'
        - '{"Concat": ["--target_column=", "{{$.inputs.parameters[''target_column'']}}"]}'
        - '{"Concat": ["--weight_column=", "{{$.inputs.parameters[''weight_column'']}}"]}'
        - '{"Concat": ["--prediction_type=", "{{$.inputs.parameters[''prediction_type'']}}"]}'
        - '{"IfPresent": {"InputName": "model_type", "Then": {"Concat": ["--model_type=",
          "{{$.inputs.parameters[''model_type'']}}"]}}}'
        - '{"Concat": ["--run_distill=", "{{$.inputs.parameters[''run_distill'']}}"]}'
        - '{"Concat": ["--run_feature_selection=", "{{$.inputs.parameters[''run_feature_selection'']}}"]}'
        - '{"Concat": ["--materialized_examples_format=", "{{$.inputs.parameters[''materialized_examples_format'']}}"]}'
        - '{"Concat": ["--max_selected_features=", "{{$.inputs.parameters[''max_selected_features'']}}"]}'
        - '{"Concat": ["--feature_selection_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/feature_selection_staging_dir"]}'
        - '{"Concat": ["--feature_selection_algorithm=", "{{$.inputs.parameters[''feature_selection_algorithm'']}}"]}'
        - '{"Concat": ["--feature_ranking_path=", "{{$.outputs.artifacts[''feature_ranking''].uri}}"]}'
        - '{"Concat": ["--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.txt"]}'
        - '{"Concat": ["--stats_result_path=", "{{$.outputs.artifacts[''dataset_stats''].uri}}"]}'
        - '{"Concat": ["--transform_output_artifact_path=", "{{$.outputs.artifacts[''transform_output''].uri}}"]}'
        - '{"Concat": ["--transform_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform"]}'
        - '{"Concat": ["--materialized_examples_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/materialized"]}'
        - '{"Concat": ["--export_data_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/export"]}'
        - '{"Concat": ["--materialized_data_path=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/materialized_data"]}'
        - '{"Concat": ["--materialized_data_artifact_path=", "{{$.outputs.artifacts[''materialized_data''].uri}}"]}'
        - '{"Concat": ["--bigquery_test_split_uri_path=", "{{$.outputs.parameters[''bigquery_test_split_uri''].output_file}}"]}'
        - '{"Concat": ["--bigquery_downsampled_test_split_uri_path=", "{{$.outputs.parameters[''bigquery_downsampled_test_split_uri''].output_file}}"]}'
        - '{"Concat": ["--split_example_counts_path=", "{{$.outputs.parameters[''split_example_counts''].output_file}}"]}'
        - '{"Concat": ["--instance_schema_path=", "{{$.outputs.artifacts[''instance_schema''].path}}"]}'
        - '{"Concat": ["--training_schema_path=", "{{$.outputs.artifacts[''training_schema''].path}}"]}'
        - --job_name=feature-transform-engine-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
        - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
        - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
        - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
        - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
        - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20230424_1325
        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230424_1325
        - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
        - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
        - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
        - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
        - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
        - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
        - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20230424_1325
    exec-get-fte-suffix:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_fte_suffix
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_fte_suffix(\n    project: str,\n    location: str,\n    bigquery_staging_full_dataset_id:\
          \ str,\n    fte_table: str,\n) -> str:\n  \"\"\"Infers the FTE suffix from\
          \ the intermediate FTE table name.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  client = bigquery.Client(project=project, location=location)\n  for\
          \ table in client.list_tables(bigquery_staging_full_dataset_id):\n    if\
          \ table.table_id.startswith(fte_table):\n      return table.table_id[len(fte_table)\
          \ + 1:]\n  raise ValueError(\n      f'No FTE output tables found in {bigquery_staging_full_dataset_id}.')\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
    exec-get-table-location:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_table_location
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.24.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_table_location(\n    project: str,\n    table: Optional[str],\n\
          \    default_location: str = '',\n) -> str:\n  \"\"\"Returns the region\
          \ the given table belongs to.\n\n  Args:\n    project: The GCP project.\n\
          \    table: The BigQuery table to get a location for.\n    default_location:\
          \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
          \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
          \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
          \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
          \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
          \  return client.get_table(table).location\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
    exec-model-evaluation-regression:
      container:
        args:
        - --setup_file
        - /setup.py
        - --json_mode
        - 'true'
        - --project_id
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --problem_type
        - regression
        - --batch_prediction_format
        - '{{$.inputs.parameters[''predictions_format'']}}'
        - '{"IfPresent": {"InputName": "predictions_gcs_source", "Then": ["--batch_prediction_gcs_source",
          "{{$.inputs.artifacts[''predictions_gcs_source''].uri}}"]}}'
        - '{"IfPresent": {"InputName": "predictions_bigquery_source", "Then": ["--batch_prediction_bigquery_source",
          "bq://{{$.inputs.artifacts[''predictions_bigquery_source''].metadata[''projectId'']}}.{{$.inputs.artifacts[''predictions_bigquery_source''].metadata[''datasetId'']}}.{{$.inputs.artifacts[''predictions_bigquery_source''].metadata[''tableId'']}}"]}}'
        - '{"IfPresent": {"InputName": "model", "Then": ["--model_name", "{{$.inputs.artifacts[''model''].metadata[''resourceName'']}}"]}}'
        - --ground_truth_format
        - '{{$.inputs.parameters[''ground_truth_format'']}}'
        - --ground_truth_gcs_source
        - '{{$.inputs.parameters[''ground_truth_gcs_source'']}}'
        - --ground_truth_bigquery_source
        - '{{$.inputs.parameters[''ground_truth_bigquery_source'']}}'
        - --root_dir
        - '{{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}'
        - --target_field_name
        - instance.{{$.inputs.parameters['target_field_name']}}
        - --prediction_score_column
        - '{{$.inputs.parameters[''prediction_score_column'']}}'
        - --dataflow_job_prefix
        - evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}
        - --dataflow_service_account
        - '{{$.inputs.parameters[''dataflow_service_account'']}}'
        - --dataflow_disk_size
        - '{{$.inputs.parameters[''dataflow_disk_size'']}}'
        - --dataflow_machine_type
        - '{{$.inputs.parameters[''dataflow_machine_type'']}}'
        - --dataflow_workers_num
        - '{{$.inputs.parameters[''dataflow_workers_num'']}}'
        - --dataflow_max_workers_num
        - '{{$.inputs.parameters[''dataflow_max_workers_num'']}}'
        - --dataflow_subnetwork
        - '{{$.inputs.parameters[''dataflow_subnetwork'']}}'
        - --dataflow_use_public_ips
        - '{{$.inputs.parameters[''dataflow_use_public_ips'']}}'
        - --kms_key_name
        - '{{$.inputs.parameters[''encryption_spec_key_name'']}}'
        - --output_metrics_gcs_path
        - '{{$.outputs.artifacts[''evaluation_metrics''].uri}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python
        - /main.py
        image: gcr.io/ml-pipeline/model-evaluation:v0.7
    exec-model-upload:
      container:
        args:
        - --type
        - UploadModel
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
          "\"", ", \"description\": \"", "{{$.inputs.parameters[''description'']}}",
          "\"", ", \"predict_schemata\": {", "\"instance_schema_uri\": \"", "{{$.inputs.parameters[''instance_schema_uri'']}}",
          "\"", ", \"parameters_schema_uri\": \"", "{{$.inputs.parameters[''parameters_schema_uri'']}}",
          "\"", ", \"prediction_schema_uri\": \"", "{{$.inputs.parameters[''prediction_schema_uri'']}}",
          "\"", "}", ", \"container_spec\": {", "\"image_uri\": \"", "{{$.inputs.parameters[''serving_container_image_uri'']}}",
          "\"", ", \"command\": ", "{{$.inputs.parameters[''serving_container_command'']}}",
          ", \"args\": ", "{{$.inputs.parameters[''serving_container_args'']}}", ",
          \"env\": ", "{{$.inputs.parameters[''serving_container_environment_variables'']}}",
          ", \"ports\": ", "{{$.inputs.parameters[''serving_container_ports'']}}",
          ", \"predict_route\": \"", "{{$.inputs.parameters[''serving_container_predict_route'']}}",
          "\"", ", \"health_route\": \"", "{{$.inputs.parameters[''serving_container_health_route'']}}",
          "\"", "}", ", \"artifact_uri\": \"", "{{$.inputs.parameters[''artifact_uri'']}}",
          "\"", ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.model.upload_model.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.33
    exec-prophet-trainer:
      container:
        args:
        - --type
        - CustomJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --payload
        - '{"Concat": ["{\"display_name\": \"prophet-trainer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
          ", "\"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}, ", "\"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\"1\",
          ", "\"machine_spec\": {\"machine_type\": \"n1-standard-4\"}, ", "\"container_spec\":
          {\"image_uri\":\"us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20230424_1325\",
          ", "\"args\": [\"prophet_trainer\", \"", "--job_name={{$.pipeline_job_name}}\",
          \"", "--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20230424_1325\",
          \"", "--prediction_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/fte-prediction-server:20230424_1325\",
          \"", "--artifacts_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/model/\",
          \"", "--evaluated_examples_dir=", "{{$.inputs.parameters[''root_dir'']}}",
          "/{{$.pipeline_job_uuid}}/eval/\", \"", "--region=", "{{$.inputs.parameters[''location'']}}",
          "\", \"", "--source_bigquery_uri=", "{{$.inputs.parameters[''source_bigquery_uri'']}}",
          "\", \"", "--target_column=", "{{$.inputs.parameters[''target_column'']}}",
          "\", \"", "--time_column=", "{{$.inputs.parameters[''time_column'']}}",
          "\", \"", "--time_series_identifier_column=", "{{$.inputs.parameters[''time_series_identifier_column'']}}",
          "\", \"", "--forecast_horizon=", "{{$.inputs.parameters[''forecast_horizon'']}}",
          "\", \"", "--window_column=", "{{$.inputs.parameters[''window_column'']}}",
          "\", \"", "--optimization_objective=", "{{$.inputs.parameters[''optimization_objective'']}}",
          "\", \"", "--data_granularity_unit=", "{{$.inputs.parameters[''data_granularity_unit'']}}",
          "\", \"", "--predefined_split_column=", "{{$.inputs.parameters[''predefined_split_column'']}}",
          "\", \"", "--max_num_trials=", "{{$.inputs.parameters[''max_num_trials'']}}",
          "\", \"", "--dataflow_project=", "{{$.inputs.parameters[''project'']}}",
          "\", \"", "--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
          "\", \"", "--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
          "\", \"", "--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
          "\", \"", "--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
          "\", \"", "--dataflow_subnetwork=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
          "\", \"", "--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
          "\", \"", "--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}",
          "\", \"", "--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.custom_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.32
    exec-table-to-uri:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - table_to_uri
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef table_to_uri(\n    table: dsl.Input[dsl.Artifact],\n    use_bq_prefix:\
          \ bool = False,\n) -> NamedTuple(\n    'Outputs',\n    [\n        ('project_id',\
          \ str),\n        ('dataset_id', str),\n        ('table_id', str),\n    \
          \    ('uri', str),\n    ],\n):\n  \"\"\"Converts a google.BQTable to a URI.\"\
          \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
          \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
          \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
          \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
          \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
          \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
          \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
    exec-validate-inputs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_inputs
        command:
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef validate_inputs(\n    time_column: Optional[str] = None,\n  \
          \  time_series_identifier_column: Optional[str] = None,\n    target_column:\
          \ Optional[str] = None,\n    data_source_bigquery_table_path: Optional[str]\
          \ = None,\n    training_fraction: Optional[float] = None,\n    validation_fraction:\
          \ Optional[float] = None,\n    test_fraction: Optional[float] = None,\n\
          \    predefined_split_key: Optional[str] = None,\n    timestamp_split_key:\
          \ Optional[str] = None,\n    data_source_csv_filenames: Optional[str] =\
          \ None,\n    source_model_uri: Optional[str] = None,\n    bigquery_destination_uri:\
          \ Optional[str] = None,\n    window_column: Optional[str] = None,\n    window_stride_length:\
          \ Optional[int] = None,\n    window_max_count: Optional[int] = None,\n \
          \   optimization_objective: Optional[str] = None,\n    data_granularity_unit:\
          \ Optional[str] = None,\n) -> None:\n  \"\"\"Checks training pipeline input\
          \ parameters are valid.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
          \  import re\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
          \n  project_pattern = r'([a-z0-9.-]+:)?[a-z][a-z0-9-_]{4,28}[a-z0-9]'\n\
          \  dataset_pattern = r'[a-zA-Z0-9_]+'\n  table_pattern = r'[^\\.\\:`]+'\n\
          \  dataset_uri_pattern = re.compile(\n      f'(bq://)?{project_pattern}[.:]{dataset_pattern}')\n\
          \  table_uri_pattern = re.compile(\n      f'(bq://)?{project_pattern}[.:]{dataset_pattern}[.:]{table_pattern}')\n\
          \n  # Validate BigQuery column and dataset names.\n  bigquery_column_parameters\
          \ = [\n      time_column,\n      time_series_identifier_column,\n      target_column,\n\
          \  ]\n  column_pattern = re.compile(r'[a-zA-Z_][a-zA-Z0-9_]{1,300}')\n \
          \ for column in bigquery_column_parameters:\n    if column and not column_pattern.fullmatch(column):\n\
          \      raise ValueError(f'Invalid column name: {column}.')\n  if (bigquery_destination_uri\
          \ and\n      not dataset_uri_pattern.fullmatch(bigquery_destination_uri)):\n\
          \    raise ValueError(\n        f'Invalid BigQuery dataset URI: {bigquery_destination_uri}.')\n\
          \  if (source_model_uri and not table_uri_pattern.fullmatch(source_model_uri)):\n\
          \    raise ValueError(f'Invalid BigQuery table URI: {source_model_uri}.')\n\
          \n  # Validate data source.\n  data_source_count = sum([bool(source) for\
          \ source in [\n      data_source_bigquery_table_path, data_source_csv_filenames]])\n\
          \  if data_source_count > 1:\n    raise ValueError(f'Expected 1 data source,\
          \ found {data_source_count}.')\n  if (data_source_bigquery_table_path\n\
          \      and not table_uri_pattern.fullmatch(data_source_bigquery_table_path)):\n\
          \    raise ValueError(\n        f'Invalid BigQuery table URI: {data_source_bigquery_table_path}.')\n\
          \  gcs_path_pattern = re.compile(r'gs:\\/\\/(.+)\\/([^\\/]+)')\n  if data_source_csv_filenames:\n\
          \    csv_list = [filename.strip()\n                for filename in data_source_csv_filenames.split(',')]\n\
          \    for gcs_path in csv_list:\n      if not gcs_path_pattern.fullmatch(gcs_path):\n\
          \        raise ValueError(f'Invalid path to CSV stored in GCS: {gcs_path}.')\n\
          \n  # Validate split spec.\n  fraction_splits = [\n      training_fraction,\n\
          \      validation_fraction,\n      test_fraction,\n  ]\n  split_count =\
          \ sum([\n      bool(source)\n      for source in [predefined_split_key,\n\
          \                     any(fraction_splits)]\n  ])\n  if split_count > 1:\n\
          \    raise ValueError(f'Expected 1 split type, found {split_count}.')\n\
          \  if (predefined_split_key and\n      not column_pattern.fullmatch(predefined_split_key)):\n\
          \    raise ValueError(f'Invalid column name: {predefined_split_key}.')\n\
          \  if any(fraction_splits):\n    if not all(fraction_splits):\n      raise\
          \ ValueError(\n          f'All fractions must be non-zero. Got: {fraction_splits}.')\n\
          \    if sum(fraction_splits) != 1:\n      raise ValueError(\n          f'Fraction\
          \ splits must sum to 1. Got: {sum(fraction_splits)}.')\n  if (timestamp_split_key\
          \ and\n      not column_pattern.fullmatch(timestamp_split_key)):\n    raise\
          \ ValueError(f'Invalid column name: {timestamp_split_key}.')\n  if timestamp_split_key\
          \ and not all(fraction_splits):\n    raise ValueError('All fractions must\
          \ be non-zero for timestamp split.')\n\n  # Validate window config.\n  if\
          \ window_stride_length == -1:\n    window_stride_length = None\n  if window_max_count\
          \ == -1:\n    window_max_count = None\n  window_configs = [window_column,\
          \ window_stride_length, window_max_count]\n  window_config_count = sum([bool(config)\
          \ for config in window_configs])\n  if window_config_count > 1:\n    raise\
          \ ValueError(f'Expected 1 window config, found {window_config_count}.')\n\
          \  if window_column and not column_pattern.fullmatch(window_column):\n \
          \   raise ValueError(f'Invalid column name: {window_column}.')\n  if window_stride_length\
          \ and (window_stride_length < 1 or\n                               window_stride_length\
          \ > 1000):\n    raise ValueError('Stride must be between 1 and 1000. Got:\
          \ '\n                     f'{window_stride_length}.')\n  if window_max_count\
          \ and (window_max_count < 1000 or\n                           window_max_count\
          \ > int(1e8)):\n    raise ValueError('Max count must be between 1000 and\
          \ 100000000. Got: '\n                     f'{window_max_count}.')\n\n  #\
          \ Validate eval metric.\n  valid_optimization_objectives = ['rmse', 'mae',\
          \ 'rmsle']\n  if optimization_objective:\n    if optimization_objective\
          \ not in valid_optimization_objectives:\n      raise ValueError(\n     \
          \     'Optimization objective should be one of the following: '\n      \
          \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
          \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
          \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
          \    if data_granularity_unit not in valid_data_granularity_units:\n   \
          \   raise ValueError(\n          'Granularity unit should be one of the\
          \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
          \n"
        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20230424_1325
pipelineInfo:
  name: prophet-train
root:
  dag:
    tasks:
      bigquery-delete-dataset-with-prefix:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-bigquery-delete-dataset-with-prefix
        dependentTasks:
        - exit-handler-1
        inputs:
          parameters:
            dataset_prefix:
              runtimeValue:
                constant: tmp_{{$.pipeline_job_uuid}}
            delete_contents:
              runtimeValue:
                constant: 1.0
            project:
              componentInputParameter: project
        taskInfo:
          name: delete-tmp-dataset
        triggerPolicy:
          strategy: ALL_UPSTREAM_TASKS_COMPLETED
      exit-handler-1:
        componentRef:
          name: comp-exit-handler-1
        inputs:
          parameters:
            pipelinechannel--data_granularity_unit:
              componentInputParameter: data_granularity_unit
            pipelinechannel--data_source_bigquery_table_path:
              componentInputParameter: data_source_bigquery_table_path
            pipelinechannel--data_source_csv_filenames:
              componentInputParameter: data_source_csv_filenames
            pipelinechannel--dataflow_service_account:
              componentInputParameter: dataflow_service_account
            pipelinechannel--dataflow_subnetwork:
              componentInputParameter: dataflow_subnetwork
            pipelinechannel--dataflow_use_public_ips:
              componentInputParameter: dataflow_use_public_ips
            pipelinechannel--encryption_spec_key_name:
              componentInputParameter: encryption_spec_key_name
            pipelinechannel--evaluation_dataflow_disk_size_gb:
              componentInputParameter: evaluation_dataflow_disk_size_gb
            pipelinechannel--evaluation_dataflow_machine_type:
              componentInputParameter: evaluation_dataflow_machine_type
            pipelinechannel--evaluation_dataflow_max_num_workers:
              componentInputParameter: evaluation_dataflow_max_num_workers
            pipelinechannel--forecast_horizon:
              componentInputParameter: forecast_horizon
            pipelinechannel--location:
              componentInputParameter: location
            pipelinechannel--max_num_trials:
              componentInputParameter: max_num_trials
            pipelinechannel--optimization_objective:
              componentInputParameter: optimization_objective
            pipelinechannel--predefined_split_key:
              componentInputParameter: predefined_split_key
            pipelinechannel--project:
              componentInputParameter: project
            pipelinechannel--root_dir:
              componentInputParameter: root_dir
            pipelinechannel--target_column:
              componentInputParameter: target_column
            pipelinechannel--test_fraction:
              componentInputParameter: test_fraction
            pipelinechannel--time_column:
              componentInputParameter: time_column
            pipelinechannel--time_series_identifier_column:
              componentInputParameter: time_series_identifier_column
            pipelinechannel--timestamp_split_key:
              componentInputParameter: timestamp_split_key
            pipelinechannel--trainer_dataflow_disk_size_gb:
              componentInputParameter: trainer_dataflow_disk_size_gb
            pipelinechannel--trainer_dataflow_machine_type:
              componentInputParameter: trainer_dataflow_machine_type
            pipelinechannel--trainer_dataflow_max_num_workers:
              componentInputParameter: trainer_dataflow_max_num_workers
            pipelinechannel--training_fraction:
              componentInputParameter: training_fraction
            pipelinechannel--validation_fraction:
              componentInputParameter: validation_fraction
            pipelinechannel--window_column:
              componentInputParameter: window_column
            pipelinechannel--window_max_count:
              componentInputParameter: window_max_count
            pipelinechannel--window_stride_length:
              componentInputParameter: window_stride_length
        taskInfo:
          name: exit-handler-1
  inputDefinitions:
    parameters:
      data_granularity_unit:
        parameterType: STRING
      data_source_bigquery_table_path:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      data_source_csv_filenames:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      dataflow_service_account:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      dataflow_subnetwork:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      dataflow_use_public_ips:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      encryption_spec_key_name:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      evaluation_dataflow_disk_size_gb:
        defaultValue: 40.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      evaluation_dataflow_machine_type:
        defaultValue: n1-standard-1
        isOptional: true
        parameterType: STRING
      evaluation_dataflow_max_num_workers:
        defaultValue: 10.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      forecast_horizon:
        parameterType: NUMBER_INTEGER
      location:
        parameterType: STRING
      max_num_trials:
        defaultValue: 6.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      optimization_objective:
        parameterType: STRING
      predefined_split_key:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      project:
        parameterType: STRING
      root_dir:
        parameterType: STRING
      target_column:
        parameterType: STRING
      test_fraction:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      time_column:
        parameterType: STRING
      time_series_identifier_column:
        parameterType: STRING
      timestamp_split_key:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      trainer_dataflow_disk_size_gb:
        defaultValue: 40.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      trainer_dataflow_machine_type:
        defaultValue: n1-standard-1
        isOptional: true
        parameterType: STRING
      trainer_dataflow_max_num_workers:
        defaultValue: 10.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_fraction:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      validation_fraction:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      window_column:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      window_max_count:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      window_stride_length:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0-beta.13
