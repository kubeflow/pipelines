{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vertex Pipelines: Distributed Training with Vertex AI Custom Job OP\n",
    "\n",
    "## Overview\n",
    "This notebook shows how to use the `custom_training_job_op` wrapper to create a distributed training job on Vertex AI. This allows users to take advantage of the vertical and horizontal scaling for computation heavy tasks on Vertex AI. The underlying component must support distributed computation, in this example we will use the [Tensroflow distribution strategy](https://www.tensorflow.org/guide/distributed_training). To learn more about Vertex AI Custom Job see [Vertex AI Custom Training](https://cloud.google.com/vertex-ai/docs/training/custom-training). \n",
    "\n",
    "\n",
    "For `custom_training_job_op` interface please see the [souce code here](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/experimental/custom_job/custom_job.py#L30)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install additional packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install  -U google-cloud-pipeline-components -q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Before you begin\n",
    "### Set your Project ID and Location\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PROJECT_ID = \"python-docs-samples-tests\"\n",
    "LOCATION = \"us-central1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google_cloud_pipeline_components.experimental.custom_job import custom_training_job_op\n",
    "from kfp.v2 import dsl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a component with distributed strategy support\n",
    "In this example we use the tf.distribute.Strategy to create a component with distribute training across multiple machines. For additional distribution strategies such as using multiple GPUs or TPUs please see the [Tensroflow distribution strategy guide](https://www.tensorflow.org/guide/distributed_training)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@dsl.component\n",
    "def distributed_train_mnist(num_epochs: int = 5):\n",
    "    \"\"\"Distribute trians mnist across multiple machines.\n",
    "    Args:\n",
    "        num_epochs: Optional, number of epochs to run the training. \n",
    "        output_model: A locally accessible filepath for\n",
    "            output artifact of type `Model`.\n",
    "    \"\"\"\n",
    "    # Installing Tensorflow in the default image. \n",
    "    # Alternatively you can use a custom base image.\n",
    "    import subprocess\n",
    "    subprocess.call(['pip3', 'install', 'tensorflow'])\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Setup a distribution strategy. The component must able to\n",
    "    # Support the distribution strategy set by custom job wrapper.\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Load and prepare the MNIST dataset. \n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "        \n",
    "        # Define a simple model \n",
    "        model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "          tf.keras.layers.Dense(128, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.2),\n",
    "          tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "\n",
    "        # Choose a loss function for training\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "        # Run the training for a few epochs\n",
    "        model.fit(x_train, y_train, epochs=5)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the component to a Vertex AI Custom Job and define the cluster configuration. In this case we are using three CPU instances to run the training as a MultiWorkerMirroredStrategy job. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "custom_job_distributed_training_op= custom_training_job_op(distributed_train_mnist, replica_count= 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the pipeline:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@dsl.pipeline(name='distributed-custom-job-sample-pipeline')\n",
    "def pipeline(num_epochs: int = 5):\n",
    "    custom_producer_task = custom_job_distributed_training_op(num_epochs=num_epochs, project=PROJECT_ID ,location=LOCATION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can proceed to complie and run the pipeline from here as usual. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}