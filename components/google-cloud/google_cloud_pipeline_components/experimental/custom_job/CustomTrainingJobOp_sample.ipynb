{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e42AtYAjGo",
      "metadata": {
        "id": "38e42AtYAjGo"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z0vb-2PHuqQN",
      "metadata": {
        "id": "Z0vb-2PHuqQN"
      },
      "source": [
        "# Vertex Pipelines: Vertex AI Custom Job OP training wrapper\n",
        "\n",
        "## Overview\n",
        "This notebook shows how to use the `custom_training_job_op` wrapper to convert any component to run it as a Vertex AI custom job. This allows users to take advantage of the vertical and horizontal scaling for computation heavy tasks on Vertex AI. This requires that the underlying component has built in support distributed computation. To learn more about Vertex AI Custom Job see [Vertex AI Custom Training](https://cloud.google.com/vertex-ai/docs/training/custom-training).\n",
        "\n",
        "\n",
        "For `custom_training_job_op` interface please see the [API Docs here](https://google-cloud-pipeline-components.readthedocs.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K6gPttSSAjGu",
      "metadata": {
        "id": "K6gPttSSAjGu"
      },
      "source": [
        "### Install additional packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KnIrzMFCAjGv",
      "metadata": {
        "id": "KnIrzMFCAjGv"
      },
      "outputs": [],
      "source": [
        "!pip3 install  -U google-cloud-pipeline-components -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jyq5Vb29AjGv",
      "metadata": {
        "id": "jyq5Vb29AjGv"
      },
      "source": [
        "## Before you begin\n",
        "### Set your Project ID and Location\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EiCoj2xpAjGv",
      "metadata": {
        "id": "EiCoj2xpAjGv"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"python-docs-samples-tests\"\n",
        "LOCATION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iPbmgej5AjGw",
      "metadata": {
        "id": "iPbmgej5AjGw"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pudnr3BGAjGx",
      "metadata": {
        "id": "Pudnr3BGAjGx"
      },
      "outputs": [],
      "source": [
        "from google_cloud_pipeline_components.experimental.custom_job import custom_training_job_op\n",
        "from kfp.v2 import dsl\n",
        "import kfp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hCR3Nu1ZvKCB",
      "metadata": {
        "id": "hCR3Nu1ZvKCB"
      },
      "source": [
        "## Create a component, convert it to a custom job to use in a pipeline.\n",
        "Create a simple component that takes an input and produces an output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oWmpEkn8vJcc",
      "metadata": {
        "id": "oWmpEkn8vJcc"
      },
      "outputs": [],
      "source": [
        "producer_op = kfp.components.load_component_from_text(\n",
        "    \"\"\"\n",
        "name: Producer\n",
        "inputs:\n",
        "- {name: input_text, type: String, description: 'Represents an input parameter.'}\n",
        "outputs:\n",
        "- {name: output_value, type: String, description: 'Represents an output paramter.'}\n",
        "implementation:\n",
        "  container:\n",
        "    image: google/cloud-sdk:latest\n",
        "    command:\n",
        "    - sh\n",
        "    - -c\n",
        "    - |\n",
        "      set -e -x\n",
        "      echo \"$0, this is an output parameter\" | gsutil cp - \"$1\"\n",
        "    - {inputValue: input_text}\n",
        "    - {outputPath: output_value}\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CxcTxOXgvq45",
      "metadata": {
        "id": "CxcTxOXgvq45"
      },
      "source": [
        "## Define the pipeline:\n",
        "Use `custom_training_job_op` to convert the `producer_op` to `custom_training_producer_op` and use it to construct the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlAm4Jy9vsH4",
      "metadata": {
        "id": "rlAm4Jy9vsH4"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(pipeline_root='', name='custom-job-sample-pipeline')\n",
        "def pipeline(text: str = 'message'):\n",
        "    custom_training_producer_op= gcpc.experimental.custom_job.custom_training_job_op(producer_op)\n",
        "    custom_producer_task = custom_training_producer_op(input_text=text, project=PROJECT_ID ,location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jax0U_VBv3Kd",
      "metadata": {
        "id": "Jax0U_VBv3Kd"
      },
      "source": [
        "You can proceed to complie and run the pipeline from here as usual."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yoj9qpUIAjGs",
      "metadata": {
        "id": "yoj9qpUIAjGs"
      },
      "source": [
        "# Vertex Pipelines: Distributed Training with Vertex AI Custom Job OP\n",
        "\n",
        "## Overview\n",
        "This sample shows how to use the `custom_training_job_op` wrapper to create a distributed training job on Vertex AI. This allows users to take advantage of the vertical and horizontal scaling for computation heavy tasks on Vertex AI. The underlying component must support distributed computation, in this example we will use the [Tensorflow distribution strategy](https://www.tensorflow.org/guide/distributed_training).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hgI1aO4mAjGy",
      "metadata": {
        "id": "hgI1aO4mAjGy"
      },
      "source": [
        "## Create a component with distributed strategy support\n",
        "In this example we use the tf.distribute.Strategy to create a component with distribute training across multiple machines. For additional distribution strategies such as using multiple GPUs or TPUs please see the [Tensorflow distribution strategy guide](https://www.tensorflow.org/guide/distributed_training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HSU809QMAjGy",
      "metadata": {
        "id": "HSU809QMAjGy"
      },
      "outputs": [],
      "source": [
        "@dsl.component\n",
        "def distributed_train_mnist(num_epochs: int = 5):\n",
        "    \"\"\"Distribute train mnist across multiple machines.\n",
        "    Args:\n",
        "        num_epochs: Optional, number of epochs to run the training.\n",
        "        output_model: A locally accessible filepath for\n",
        "            output artifact of type `Model`.\n",
        "    \"\"\"\n",
        "    # Installing Tensorflow in the default image.\n",
        "    # Alternatively you can use a custom base image.\n",
        "    import subprocess\n",
        "    subprocess.call(['pip3', 'install', 'tensorflow'])\n",
        "\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Setup a distribution strategy. The component must able to\n",
        "    # Support the distribution strategy set by custom job wrapper.\n",
        "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Load and prepare the MNIST dataset.\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "        # Define a simple model\n",
        "        model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dropout(0.2),\n",
        "          tf.keras.layers.Dense(10)\n",
        "        ])\n",
        "\n",
        "        # Choose a loss function for training\n",
        "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        # Run the training for a few epochs\n",
        "        model.fit(x_train, y_train, epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BEPgM_NOAjGz",
      "metadata": {
        "id": "BEPgM_NOAjGz"
      },
      "source": [
        "## Define the pipeline\n",
        "Convert the component to a Vertex AI Custom Job and define the cluster configuration. In this case we are using three CPU instances to run the training as a MultiWorkerMirroredStrategy job. We use the resulting training_op to in the pipeline definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vbb4NUjrAjG1",
      "metadata": {
        "id": "vbb4NUjrAjG1"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name='distributed-custom-job-sample-pipeline')\n",
        "def pipeline(num_epochs: int = 5):\n",
        "    custom_job_distributed_training_op= custom_training_job_op(distributed_train_mnist, replica_count= 3)\n",
        "    custom_producer_task = custom_job_distributed_training_op(num_epochs=num_epochs, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mbOwVM2lAjG1",
      "metadata": {
        "id": "mbOwVM2lAjG1"
      },
      "source": [
        "You can proceed to complie and run the pipeline from here as usual. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CustomTrainingJobOp_samples.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "environment": {
      "name": "common-cpu.m73",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
