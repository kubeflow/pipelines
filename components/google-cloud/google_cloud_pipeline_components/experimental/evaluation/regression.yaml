name: model_evaluation_regression
description: |
  Compute evaluation metrics on a trained model's batch prediction results.
  Creates a dataflow job with Apache Beam and TFMA to compute evaluation metrics.
  Supports regression, point forecasting, and quantile forecasting evaluation.

  Args:
      project (str):
          Project to run evaluation container.
      location (Optional[str]):
          Location for running the evaluation.
          If not set, defaulted to `us-central1`.
      root_dir (str):
          The GCS directory for keeping staging files.
          A random subdirectory will be created under the directory to keep job info for resuming
          the job in case of failure.
      problem_type (str):
          The problem type being addressed by this regression evaluation run.
          `regression`, `point_forecasting`, and `quantile_forecasting` are the supported types.
      predictions_format (Optional[str]):
          The file format for the batch prediction results. `jsonl` is currently the only allowed
          format.
          If not set, defaulted to `jsonl`.
      batch_prediction_job (system.Artifact):
          The VertexBatchPredictionJob with prediction or explanation results for this evaluation.
          For prediction results, the files should be in format "prediction.results-*".
          For explanation results, the files should be in format "explanation.results-*".
      ground_truth_column (str):
          The column name of the feature containing ground truth.
          Formatted to be able to find nested columns, delimeted by `.`.
          Prefixed with 'instance.' internally for Vertex Batch Prediction.
      prediction_score_column (Optional[str]):
          Optional. The column name of the field containing batch prediction scores.
          Formatted to be able to find nested columns, delimeted by `.`.
          If not set, defaulted to `prediction.value` for a `regression` problem_type.
          TODO: Set default for both forecasting types if needed.
      forecasting_quantiles (Optional[Sequence[Float]]):
          The list of quentiles in the same order they appear in the quantile prediction score column.
          TODO: Unsure if this field is actually needed.
      example_weight_column (Optional[str]):
          Optional. The column name of the field containing example weights.
          Formatted to be able to find nested columns, delimeted by `.`.
      generate_feature_attribution (Optional[bool]):
          Optional. If set to True, then the explanations generated by the
          VertexBatchPredictionJob will be used to generate feature attributions. This will only
          pass if the input VertexBatchPredictionJob generated explanations.
          If not set, defaulted to `False`.
      dataflow_service_account (Optional[str]):
          Optional. Service account to run the dataflow job.
          If not set, dataflow will use the default woker service account.

          For more details, see https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#default_worker_service_account
      dataflow_disk_size (Optional[int]):
          Optional. The disk size (in GB) of the machine executing the evaluation run.
          If not set, defaulted to `50`.
      dataflow_machine_type (Optional[str]):
          Optional. The machine type executing the evaluation run.
          If not set, defaulted to `n1-standard-4`.
      dataflow_workers_num (Optional[int]):
          Optional. The number of workers executing the evaluation run.
          If not set, defaulted to `10`.
      dataflow_max_workers_num (Optional[int]):
          Optional. The max number of workers executing the evaluation run.
          If not set, defaulted to `25`.
      dataflow_subnetwork (Optional[str]):
          Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be
          used. More details:
          https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
      dataflow_use_public_ips (Optional[bool]):
          Specifies whether Dataflow workers use public IP addresses.
      encryption_spec_key_name (Optional[str]):
          Customer-managed encryption key.
  Returns:
      evaluation_metrics (google.RegressionMetrics):
          google.RegressionMetrics artifact representing the regression evaluation metrics in GCS.
inputs:
  - { name: project, type: String }
  - { name: location, type: String, default: "us-central1" }
  - { name: root_dir, type: String }
  - { name: problem_type, type: String }
  - { name: predictions_format, type: String, default: "jsonl" }
  - { name: batch_prediction_job, type: Artifact }
  - { name: ground_truth_column, type: String }
  - { name: prediction_score_column, type: String, default: "prediction.value" }
  - { name: forecasting_quantiles, type: JsonArray, default: '{}' }
  - { name: example_weight_column, type: String, default: "" }
  - { name: generate_feature_attribution, type: Boolean, default: False }
  - { name: dataflow_service_account, type: String, default: "" }
  - { name: dataflow_disk_size, type: Integer, default: 50 }
  - { name: dataflow_machine_type, type: String, default: "n1-standard-4" }
  - { name: dataflow_workers_num, type: Integer, default: "10" }
  - { name: dataflow_max_workers_num, type: Integer, default: "25" }
  - { name: dataflow_subnetwork, type: String, default: "" }
  - { name: dataflow_use_public_ips, type: Boolean, default: "true" }
  - { name: encryption_spec_key_name, type: String, default: "" }
outputs:
  - { name: evaluation_metrics, type: google.RegressionMetrics }
  - { name: gcp_resources, type: String }
implementation:
  container:
    image: gcr.io/ml-pipeline/model-evaluation:latest
    command:
      - python
      - /main.py
    args:
      - --setup_file
      - /setup.py
      - --json_mode
      - "true"
      - --project_id
      - { inputValue: project }
      - --location
      - { inputValue: location }
      - --problem_type
      - { inputValue: problem_type }
      - --batch_prediction_format
      - { inputValue: predictions_format }
      - --batch_prediction_gcs_source
      - "{{$.inputs.artifacts['batch_prediction_job'].uri}}"
      - --root_dir
      - "{{$.inputs.parameters['root_dir']}}/{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}"
      - --ground_truth_column
      - "instance.{{$.inputs.parameters['ground_truth_column']}}"
      - --prediction_score_column
      - { inputValue: prediction_score_column }
      - --example_weight_column
      - { inputValue: example_weight_column }
      - --generate_feature_attribution
      - { inputValue: generate_feature_attribution }
      - --dataflow_job_prefix
      - "evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}"
      - --dataflow_service_account
      - { inputValue: dataflow_service_account }
      - --dataflow_disk_size
      - { inputValue: dataflow_disk_size }
      - --dataflow_machine_type
      - { inputValue: dataflow_machine_type }
      - --dataflow_workers_num
      - { inputValue: dataflow_workers_num }
      - --dataflow_max_workers_num
      - { inputValue: dataflow_max_workers_num }
      - --dataflow_subnetwork
      - { inputValue: dataflow_subnetwork }
      - --dataflow_use_public_ips
      - { inputValue: dataflow_use_public_ips }
      - --kms_key_name
      - { inputValue: encryption_spec_key_name }
      - --output_metrics_gcs_path
      - { outputUri: evaluation_metrics }
      - --gcp_resources
      - { outputPath: gcp_resources }
      - --executor_input
      - "{{$}}"
