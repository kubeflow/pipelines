syntax = "proto3";


import "lib/proto/configuration.proto";

// A top level message containing all the information necessary for doing an
// evaluation.
message EvaluationConfig {
  // Name associated with this evaluation. Example: "my_evaluation"
  string name = 1;

  // Specification for the dataset for which evaluation is performed. It
  // communicates which columns contain ground truth, example weights
  // and how to obtain predictions to be evaluated.
  DataSpec data_spec = 2;

  // A set of slice specs for which metrics are computed.
  repeated SlicingSpec slicing_specs = 3;

  // A set of metric specs defining which metrics to compute.
  repeated MetricsSpec metrics_specs = 4;

  // Specification for how metrics should be computed. For example, whether to
  // execute beam pipeline locally or using Dataflow, and which settings to use
  // in each case.
  ExecutionSpec execution_spec = 5;

  // Specification for where to write computed metrics.
  OutputSpec output_spec = 6;
}

// Message describing data for which evaluation metrics are computed.
message DataSpec {
  // Specification for how input data is stored.
  InputSourceSpec input_source_spec = 1;

  // Name spec of the column containing ground truth. For example,
  // ColumnSpec 'name: "animal"'.
  ColumnSpec label_key_spec = 2;

  // Name of the column containing example weights, if the different examples
  // need to be weighed differently. For example, ColumnSpec 'name:
  // "importance"'.
  ColumnSpec
      example_weight_key_spec = 3;

  // Name spec for the field within predictions dictionary containing labels
  // being scored. For example, ColumnSpec 'name: "label"'.
  ColumnSpec
      predicted_label_key_spec = 4;

  // Name spec for the field within predictions dictionary containing ids for
  // labels being scored. For example, ColumnSpec 'name: "label_id"'.
  ColumnSpec
      predicted_label_id_key_spec = 5;

  // Name spec for the field within predictions dictionary containing scores
  // returned by the model. For example, ColumnSpec 'name: "scores"'.
  ColumnSpec
      predicted_score_key_spec = 6;

  // A list of labels in the same order as predictions appear in the column
  // above. For example, if a model produces prediction [0.6, 0.3, 0.1], where
  // 0.6 is a probability of an animal being a dog, 0.3 - cat and 0.1 - mouse,
  // then labels should be ["dog", "cat", "mouse"].
  repeated string labels = 7;

  // A list of quantiles in the same order they appear in quantile
  // prediction score column.
  repeated float quantiles = 8;

  // The index in quantiles used to compute point accuracy metrics.
  // Set to -1 if point_evaluation is disabled.
  int32 quantile_index = 9;
}

// Message providing settings for supported input data formats.
message InputSourceSpec {
  message JsonlFileSpec {
    // List of JSON files containing datast to be evaluated. For example,
    // "gs://my_bucket/sub_folder/file1.jsonl" or
    // "gs://my_bucket/sub_folder/prediction.results-*-of-*"
    repeated string file_names = 1;
  }

  JsonlFileSpec jsonl_file_spec = 1;
}

// Specification for how the data should be sliced. Empty slicing spec denotes
// overall metrics.
message SlicingSpec {
  // Names of the columns to be used for slicing. For example, ColumnSpec
  // 'name: "zip_code"'.
  repeated ColumnSpec
      feature_key_specs = 1;

  // Slicing spec for the case users only care about some particular slices for
  // a given column. For example,
  // 'name_spec {names: "zip_code"} value: "12345"';
  message FeatureValueSpec {
    ColumnSpec name_spec = 1;
    string value = 2;
  }

  // Slicing specs for slices defined at a specific value for a given feature.
  repeated FeatureValueSpec feature_values = 2;
}

// Specifications for a set of metrics to be computed.
message MetricsSpec {
  // Configurations for metrics in this set.
  repeated MetricConfig metrics = 1;

  // Specification for binarization to apply to the data prior to computing
  // metrics in this set, if necessary.
  Binarization binarize = 2;

  // Specification for agregation to apply to the metrics, if necessary.
  Aggregation aggregate = 3;

  // Names of models the metrics should be calculated for.
  repeated string model_names = 4;
}

// Configuration for a specific metric.
message MetricConfig {
  // Configuration for a TFMA metric.
  message TfmaMetricConfig {
    // Which module contains the metric. For example,
    // "tensorflow.keras.metrics".
    string module_name = 1;

    // Which class contains the metric within the module above. For example,
    // "CategoricalCrossentropy".
    string class_name = 2;

    // Additional configuration for the metric, if any. The configuration is
    // encoded a a JSON string. For example,
    // "{'name':  'categorical_crossentropy'}".
    string config = 3;
  }

  // Name of the metric. For example, "CategoricalCrossEntropy".
  string name = 1;

  // Configuration for a TFMA metric. Additional types of metrics can be added
  // using a oneof.
  TfmaMetricConfig tfma_metric_config = 2;
}

// Specification for binarization performed on the data.
message Binarization {
  // For each label in the list the data is transformed into a one-vs-rest
  // problem and metrics are computed for that problem. For example, ["cat",
  // "dog"].
  repeated string class_ids = 1;

  // For each value k in this list we compute metrics based on whether the
  // ground truth is in top-k predictions or not.
  repeated int32 top_k_list = 2;
}

// Specification for aggregating metrics.
message Aggregation {
  oneof type {
    // Aggregate metrics by treating all example as being equal. Appropriate for
    // use with multiclass classification.
    bool micro_average = 1;

    // Aggregate metrics by rteating all classes as being equal. Appropriate for
    // use with multilabel classification.
    bool macro_average = 2;
  }

  // Weights to apply to classes during aggregation (for macro aggregation).
  map<int32, float> class_weights = 5;
}

// Specification for how the data metrics are sliced. Empty slicing spec denotes
// overall metrics.
message OutputSlicingSpec {
  // Name of the feature to be used for slicing. For example, ColumnSpec
  // 'name: "zip_code"'.
  ColumnSpec feature_name_spec = 1;
  // If a specific value for the column is required it is specified here. For
  // example, "15218"
  oneof kind {
    bytes bytes_value = 2;
    float float_value = 3;
    int64 int64_value = 4;
  }
}

// Evaluation metric output formats.
message SlicedMetrics {
  OutputSlicingSpec single_output_slicing_spec = 1;
  ModelEvaluationMetrics metrics = 2;
}

message SlicedMetricsSet {
  repeated SlicedMetrics sliced_metrics = 1;
}

message ModelEvaluationMetrics {
  oneof metrics {
    ClassificationEvaluationMetrics classification = 1;
    RegressionEvaluationMetrics regression = 2;
    ForecastingEvaluationMetrics forecasting = 9;
  }
}

// Metrics for classification evaluation results.
message ClassificationEvaluationMetrics {
  // The Area Under Precision-Recall Curve metric. Micro-averaged for the
  // overall evaluation.
  float au_prc = 1;
  // The Area Under Receiver Operating Characteristic curve metric.
  // Micro-averaged for the overall evaluation.
  float au_roc = 2;
  // The Log Loss metric.
  float log_loss = 3;

  message ConfidenceMetrics {
    // Metrics are computed with an assumption that the Model never returns
    // predictions with score lower than this value.
    float confidence_threshold = 1;
    // Metrics are computed with an assumption that the Model always returns at
    // most this many predictions (ordered by their score, descendingly), but
    // they all still need to meet the confidence_threshold.
    int32 max_predictions = 2;
    // Recall (True Positive Rate) for the given confidence threshold.
    float recall = 3;
    // Precision for the given confidence threshold.
    float precision = 4;
    // False Positive Rate for the given confidence threshold.
    float false_positive_rate = 5;
    // The harmonic mean of recall and precision.
    float f1_score = 6;
    // The Recall (True Positive Rate) when only considering the label that has
    // the highest prediction score and not below the confidence threshold for
    // each DataItem.
    float recall_at1 = 7;
    // The precision when only considering the label that has the highest
    // prediction score and not below the confidence threshold for each
    // DataItem.
    float precision_at1 = 8;
    // The False Positive Rate when only considering the label that has the
    // highest prediction score and not below the confidence threshold for each
    // DataItem.
    float false_positive_rate_at1 = 9;

    // The harmonic mean of recallAt1 and precisionAt1.
    float f1_score_at1 = 10;
    // The number of Model created labels that match a ground truth label.
    int64 true_positive_count = 11;
    // The number of Model created labels that do not match a ground truth
    // label.
    int64 false_positive_count = 12;
    // The number of ground truth labels that are not matched by a Model created
    // label.
    int64 false_negative_count = 13;
    // The number of labels that were not created by the Model, but if they
    // would, they would not match a ground truth label.
    int64 true_negative_count = 14;
  }
  // Metrics for each confidenceThreshold in
  // 0.00,0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 and positionThreshold =
  // INT32_MAX_VALUE.
  //
  // ROC and precision-recall curves, and other aggregated metrics are derived
  // from them. The confidence metrics entries may also be supplied for
  // additional values of positionThreshold, but from these no aggregated
  // metrics are computed.
  repeated ConfidenceMetrics confidence_metrics = 4;

  // Confusion matrix of the evaluation.
  ConfusionMatrix confusion_matrix = 5;
}

// Metrics for regression evaluation results.
message RegressionEvaluationMetrics {
  // Root Mean Squared Error (RMSE).
  optional float root_mean_squared_error = 1;

  // Mean Absolute Error (MAE).
  optional float mean_absolute_error = 2;

  // Mean absolute percentage error. Infinity when there are zeros in the ground
  // truth.
  optional float mean_absolute_percentage_error = 3;

  // Coefficient of determination as Pearson correlation coefficient. Undefined
  // when ground truth or predictions are constant or near constant.
  optional float r_squared = 4;

  // Root mean squared log error. Undefined when there are negative ground truth
  // values or predictions.
  optional float root_mean_squared_log_error = 5;

  // Weighted Absolute Percentage Error
  optional float weighted_absolute_percentage_error = 8;

  // Root Mean Square Percentage Error
  optional float root_mean_squared_percentage_error = 9;
}

message ConfusionMatrix {
  message AnnotationSpecRef {
    // ID of the AnnotationSpec.
    string id = 1;
    // Display name of the AnnotationSpec.
    string display_name = 2;
  }

  // AnnotationSpecs used in the confusion matrix.
  repeated AnnotationSpecRef annotation_specs = 1;

  message Row {
    repeated int32 data_item_counts = 1;
  }

  // Rows in the confusion matrix. The number of rows is equal to the size of
  // `annotationSpecs`. `rows[i].data_item_counts[j]` is the number of DataItems
  // that have ground truth of the `annotationSpecs[i]` and are predicted as
  // `annotationSpecs[j]` by the Model being evaluated.
  repeated Row rows = 2;
}

// Metrics for forecasting evaluation results.
message ForecastingEvaluationMetrics {
  // Root Mean Squared Error (RMSE).
  optional float root_mean_squared_error = 1;

  // Mean Absolute Error (MAE).
  optional float mean_absolute_error = 2;

  // Mean absolute percentage error. Infinity when there are zeros in the ground
  // truth.
  optional float mean_absolute_percentage_error = 3;

  // Coefficient of determination as Pearson correlation coefficient. Undefined
  // when ground truth or predictions are constant or near constant.
  optional float r_squared = 4;

  // Root mean squared log error. Undefined when there are negative ground truth
  // values or predictions.
  optional float root_mean_squared_log_error = 5;

  // Entry for the Quantiles loss type optimization objective.
  message QuantileMetricsEntry {
    // The quantile for this entry.
    double quantile = 1;
    // The scaled pinball loss of this quantile.
    optional float scaled_pinball_loss = 2;
    // This is a custom metric that calculates the percentage of true values
    // that were less than the predicted value for that quantile. Should be
    // compared with the quantile itself.
    optional double observed_quantile = 3;
  }

  // The quantile metrics entries for each quantile.
  repeated QuantileMetricsEntry quantile_metrics = 7;

  // Weighted Absolute Percentage Error
  optional float weighted_absolute_percentage_error = 8;

  // Root Mean Square Percentage Error
  optional float root_mean_squared_percentage_error = 9;
}
