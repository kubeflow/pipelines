name: Execute notebook
description: Function that creates the component file `output_component_file`.
inputs:
- {name: project_id, type: String}
- {name: input_notebook_file, type: String}
- {name: output_notebook_folder, type: String}
- {name: execution_id, type: String}
- {name: location, type: String, default: us-central1, optional: true}
- {name: master_type, type: String, default: n1-standard-4, optional: true}
- {name: accelerator_type, type: String, optional: true}
- {name: accelerator_core_count, type: String, default: '0', optional: true}
- {name: labels, type: String, default: src=notebooks_executor_api, optional: true}
- {name: container_image_uri, type: String, default: 'gcr.io/deeplearning-platform-release/base-cpu:latest',
  optional: true}
- {name: params_yaml_file, type: String, optional: true}
- {name: parameters, type: String, optional: true}
- name: block_pipeline
  type: Boolean
  default: "True"
  optional: true
- name: fail_pipeline
  type: Boolean
  default: "True"
  optional: true
outputs:
- {name: state, type: String}
- {name: output_notebook_file, type: String}
- {name: error, type: String}
implementation:
  container:
    image: python:3.8
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'google-cloud-notebooks' 'google-cloud-aiplatform' || PIP_DISABLE_PIP_VERSION_CHECK=1
      python3 -m pip install --quiet --no-warn-script-location 'google-cloud-notebooks'
      'google-cloud-aiplatform' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def execute_notebook(
          project_id,
          input_notebook_file,
          output_notebook_folder,
          execution_id,
          location = 'us-central1',
          master_type = 'n1-standard-4',
          accelerator_type = None,
          accelerator_core_count = '0',
          labels = 'src=notebooks_executor_api',
          container_image_uri = 'gcr.io/deeplearning-platform-release/base-cpu:latest',
          params_yaml_file = None,
          parameters = None,
          block_pipeline = True,
          fail_pipeline = True
      ):
        """Function that creates the component file `output_component_file`.

        Executes a notebooks and returns details about the execution. Function either
        returns a tuple or raises an error depending on the `fail_pipeline` parameter.

        Args:
          project_id: str
          input_notebook_file: str
          output_notebook_folder: str
          execution_id: str
          location: str
          master_type: str
          accelerator_type: str
          accelerator_core_count: str
          labels: str
          container_image_uri: str
          params_yaml_file: str
          parameters: str
          block: bool
          fail_pipeline: bool
          block_pipeline:bool whether to block the pipeline while waiting for the execution.
          fail_pipeline:bool whether to fail the pipeline if execution has error.

        Returns:
          state:str State of the execution. Generelly empty if error.
          output_notebook_file:str Executed notebook GCS path. Usually empty if error.
          error:str error message.

        Raises:
          RuntimeError with the error message.
        """
        import time  # pylint: disable=g-import-not-at-top
        from typing import NamedTuple  # pylint: disable=g-import-not-at-top,redefined-outer-name,reimported
        from google.cloud import notebooks
        from google.cloud import aiplatform as vertex_ai
        from google.cloud import aiplatform_v1beta1 as vertex_ai_beta
        from google.cloud.aiplatform.compat.types import job_state
        from google.cloud.notebooks import Execution

        _STATES_COMPLETED = (
            job_state.JobState.JOB_STATE_SUCCEEDED,
            job_state.JobState.JOB_STATE_FAILED,
            job_state.JobState.JOB_STATE_CANCELLED,
            job_state.JobState.JOB_STATE_PAUSED,
            Execution.State.SUCCEEDED,
            Execution.State.FAILED,
            Execution.State.CANCELLING,
            Execution.State.CANCELLED,
        )

        _STATES_ERROR = (
            job_state.JobState.JOB_STATE_FAILED,
            job_state.JobState.JOB_STATE_CANCELLED,
            Execution.State.FAILED,
            Execution.State.CANCELLED
        )

        _STATES_JOB_URI = (
          Execution.State.PREPARING,
          Execution.State.RUNNING,
        )

        client_notebooks = notebooks.NotebookServiceClient()
        client_vertexai_jobs = vertex_ai_beta.JobServiceClient(
            client_options={'api_endpoint': f'{location}-aiplatform.googleapis.com'})

        # ----------------------------------
        # Helpers
        # ----------------------------------
        def _check_prefix(s, prefix='gs://'):
          """Adds the prefix gs:// to a GCS path when missing."""
          if not s.startswith(prefix):
            s = f'{prefix}{s}'
          return s

        def _handle_error(error_response):
          """Build the error logic.

          Manages how errors behave based on the fail_pipeline pipeline parameter. Can
          either fails the pipeline by raising an error or silently return the error.

          Args:
            error_response: tuple Object to return when not failing the pipeline.

          Returns:
            A tuple matching error_response with the error message generally at the
            last index if fail_pipeline is False.

          Raises:
            RuntimeError: with the error message if fail_pipeline is True.
          """
          if fail_pipeline:
            error_message = error_response[-1]
            raise RuntimeError(error_message)
          return error_response

        def _build_execution():
          """Builds the body object for the Notebooks Executor API."""
          betpl = {}
          betpl['master_type'] = master_type
          betpl['accelerator_config'] = {}
          betpl['input_notebook_file'] = _check_prefix(input_notebook_file)
          betpl['container_image_uri'] = container_image_uri
          betpl['output_notebook_folder'] = _check_prefix(output_notebook_folder)
          if labels:
            betpl['labels'] = dict(l.split('=') for l in labels.split(','))
          if accelerator_type:
            betpl['accelerator_config']['type'] = accelerator_type
            betpl['accelerator_config']['core_count'] = accelerator_core_count
          if params_yaml_file:
            betpl['params_yaml_file'] = params_yaml_file
          if parameters:
            betpl['parameters'] = parameters
          body = {}
          body['execution_template'] = betpl
          body['description'] = f'Executor for notebook {input_notebook_file}'
          return body

        # ----------------------------------
        # Component logic
        # ----------------------------------

        # Creates execution.
        execution = _build_execution()
        try:
          print('Try create_execution()...')
          execution_create_operation = client_notebooks.create_execution(
            parent=f'projects/{project_id}/locations/{location}',
            execution_id=execution_id,
            execution=execution)
        except Exception as e:
          response = ('', '', f'create_execution() failed: {e}')
          _handle_error(response)
          return response

        # Gets initial execution
        try:
          print('Try get_execution()...')
          execution = client_notebooks.get_execution(
              name=f'projects/{project_id}/locations/{location}/executions/{execution_id}')
        except Exception as e:
          response = ('', '', f'get_execution() failed: {e}')
          _handle_error(response)
          return response

        if not block_pipeline:
          print('Not blocking pipeline...')
          return (Execution.State(execution.state).name, execution.output_notebook_file, '')

        # Waits for execution to finish.
        print('Blocking pipeline...')
        execution_state = ''
        execution_job_uri = ''
        while True:
          try:
            execution = client_notebooks.get_execution(
              name=f'projects/{project_id}/locations/{location}/executions/{execution_id}')
            execution_state = getattr(execution, 'state', '')
            print(f'execution.state is {Execution.State(execution_state).name}')
          except Exception as e:
            response = ('', '', f'get_execution() for blocking pipeline failed: {e}')
            _handle_error(response)
            return response

          # Job URI is not available when state is INITIALIZING.
          if execution_state in _STATES_JOB_URI and not execution_job_uri:
            execution_job_uri = getattr(execution, 'job_uri', '')
            print(f'execution.job_uri is {execution_job_uri}')

          if execution_state in _STATES_COMPLETED:
            break
          time.sleep(30)

        # For some reason, execution.get and job.get might not return the same state
        # so we check if we can get the error message using the AI Plaform API.
        # It is only possible if there is job_uri available though.
        if execution_state in _STATES_ERROR:
          if execution_job_uri:
            while True:
              try:
                custom_job = client_vertexai_jobs.get_custom_job(name=execution_job_uri)
              except Exception as e:
                response = ('', '', f'get_custom_job() failed: {e}')
                _handle_error(response)
                return response

              custom_job_state = getattr(custom_job, 'state', None)
              if custom_job_state in _STATES_COMPLETED:
                break
              time.sleep(30)

            # == to `if state in _JOB_ERROR_STATES`
            custom_job_error = getattr(custom_job, 'error', None)
            if custom_job_error:
              response = ('', '', f'Error {custom_job_error.code}: {custom_job_error.message}')
              _handle_error((None, response))
              return response

          # The job might be successful but we need to address that the execution
          # had a problem. The previous loop was in hope to find the error message,
          # we didn't have any so we return the execution state as the message.
          response = ('', '', f'Execution finished with state: {Execution.State(execution_state).name}')
          _handle_error((None, response))
          return response

        return (Execution.State(execution_state).name, execution.output_notebook_file, '')

      def _deserialize_bool(s) -> bool:
          from distutils.util import strtobool
          return strtobool(s) == 1

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Execute notebook', description='Function that creates the component file `output_component_file`.')
      _parser.add_argument("--project-id", dest="project_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--input-notebook-file", dest="input_notebook_file", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--output-notebook-folder", dest="output_notebook_folder", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--execution-id", dest="execution_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--master-type", dest="master_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--accelerator-type", dest="accelerator_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--accelerator-core-count", dest="accelerator_core_count", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--labels", dest="labels", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--container-image-uri", dest="container_image_uri", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--params-yaml-file", dest="params_yaml_file", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--parameters", dest="parameters", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--block-pipeline", dest="block_pipeline", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--fail-pipeline", dest="fail_pipeline", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = execute_notebook(**_parsed_args)

      _output_serializers = [
          _serialize_str,
          _serialize_str,
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --project-id
    - {inputValue: project_id}
    - --input-notebook-file
    - {inputValue: input_notebook_file}
    - --output-notebook-folder
    - {inputValue: output_notebook_folder}
    - --execution-id
    - {inputValue: execution_id}
    - if:
        cond: {isPresent: location}
        then:
        - --location
        - {inputValue: location}
    - if:
        cond: {isPresent: master_type}
        then:
        - --master-type
        - {inputValue: master_type}
    - if:
        cond: {isPresent: accelerator_type}
        then:
        - --accelerator-type
        - {inputValue: accelerator_type}
    - if:
        cond: {isPresent: accelerator_core_count}
        then:
        - --accelerator-core-count
        - {inputValue: accelerator_core_count}
    - if:
        cond: {isPresent: labels}
        then:
        - --labels
        - {inputValue: labels}
    - if:
        cond: {isPresent: container_image_uri}
        then:
        - --container-image-uri
        - {inputValue: container_image_uri}
    - if:
        cond: {isPresent: params_yaml_file}
        then:
        - --params-yaml-file
        - {inputValue: params_yaml_file}
    - if:
        cond: {isPresent: parameters}
        then:
        - --parameters
        - {inputValue: parameters}
    - if:
        cond: {isPresent: block_pipeline}
        then:
        - --block-pipeline
        - {inputValue: block_pipeline}
    - if:
        cond: {isPresent: fail_pipeline}
        then:
        - --fail-pipeline
        - {inputValue: fail_pipeline}
    - '----output-paths'
    - {outputPath: state}
    - {outputPath: output_notebook_file}
    - {outputPath: error}
