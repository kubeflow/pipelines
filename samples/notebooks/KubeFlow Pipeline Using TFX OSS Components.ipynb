{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline Using TFX OSS Components\n",
    "\n",
    "In this notebook, we will demo: \n",
    "\n",
    "* Defining a KubeFlow pipeline with Python DSL\n",
    "* Submiting it to Pipelines System\n",
    "* Customize a step in the pipeline\n",
    "\n",
    "We will use a pipeline that includes some TFX OSS components such as [TFDV](https://github.com/tensorflow/data-validation), [TFT](https://github.com/tensorflow/transform), [TFMA](https://github.com/tensorflow/model-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install Pipeline SDK\n",
    "!pip3 install https://storage.googleapis.com/ml-pipeline/release/0.1.1/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.notebook\n",
    "\n",
    "\n",
    "# Set your output and project. !!!Must Do before you can proceed!!!\n",
    "OUTPUT_DIR = 'Your-Gcs-Path' # Such as gs://bucket/objact/path\n",
    "PROJECT_NAME = 'Your-Gcp-Project-Name'\n",
    "BASE_IMAGE='gcr.io/%s/pusherbase:dev' % PROJECT_NAME\n",
    "TARGET_IMAGE='gcr.io/%s/pusher:dev' % PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Experiment in the Pipeline System\n",
    "\n",
    "Pipeline system requires an \"Experiment\" to group pipeline runs. You can create a new experiment, or call client.list_experiments() to get existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/a13c1b50-93db-40b5-89a2-a72a8129606b\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that this notebook should be running in JupyterHub in the same cluster as the pipeline system.\n",
    "# Otherwise it will fail to talk to the pipeline system.\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name='demo_exp')\n",
    "# See Screenshot 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline with OSS TFX components\n",
    "\n",
    "Authoring a pipeline is just like authoring a normal Python function. The pipeline function describes the topology of the pipeline. Each step in the pipeline is typically a ContainerOp --- a simple class or function describing how to interact with a docker container image. In the below pipeline, all the container images referenced in the pipeline are already built. The pipeline starts with a TFDV step which is used to infer the schema of the data. Then it uses TFT to transform the data for training. After a single node training step, it analyze the test data predictions and generate a feature slice metrics view using a TFMA component. At last, it deploys the model to TF-Serving inside the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "\n",
    "# Below are a list of helper functions to wrap the components to provide a simpler interface for pipeline function.\n",
    "def dataflow_tf_data_validation_op(inference_data: 'GcsUri', validation_data: 'GcsUri', column_names: 'GcsUri[text/json]', key_columns, project: 'GcpProject', mode, validation_output: 'GcsUri[Directory]', step_name='validation'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfdv:dev',\n",
    "        arguments = [\n",
    "            '--csv-data-for-inference', inference_data,\n",
    "            '--csv-data-to-validate', validation_data,\n",
    "            '--column-names', column_names,\n",
    "            '--key-columns', key_columns,\n",
    "            '--project', project,\n",
    "            '--mode', mode,\n",
    "            '--output', validation_output,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'output': '/output.txt',\n",
    "            'schema': '/output_schema.json',\n",
    "        }\n",
    "    )\n",
    "\n",
    "def dataflow_tf_transform_op(train_data: 'GcsUri', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', preprocess_mode, preprocess_module: 'GcsUri[text/code/python]', transform_output: 'GcsUri[Directory]', step_name='preprocess'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:0.0.42',\n",
    "        arguments = [\n",
    "            '--train', train_data,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', preprocess_mode,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--output', transform_output,\n",
    "        ],\n",
    "        file_outputs = {'transformed': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_train_op(transformed_data_dir, schema: 'GcsUri[text/json]', learning_rate: float, hidden_layer_size: int, steps: int, target: str, preprocess_module: 'GcsUri[text/code/python]', training_output: 'GcsUri[Directory]', step_name='training'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-tf-trainer:0.0.42',\n",
    "        arguments = [\n",
    "            '--transformed-data-dir', transformed_data_dir,\n",
    "            '--schema', schema,\n",
    "            '--learning-rate', learning_rate,\n",
    "            '--hidden-layer-size', hidden_layer_size,\n",
    "            '--steps', steps,\n",
    "            '--target', target,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--job-dir', training_output,\n",
    "        ],\n",
    "        file_outputs = {'train': '/output.txt'}\n",
    "    )\n",
    "\n",
    "def dataflow_tf_model_analyze_op(model: 'TensorFlow model', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', analyze_mode, analyze_slice_column, analysis_output: 'GcsUri', step_name='analysis'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfma:0.0.42',\n",
    "        arguments = [\n",
    "            '--model', model,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', analyze_mode,\n",
    "            '--slice-columns', analyze_slice_column,\n",
    "            '--output', analysis_output,\n",
    "        ],\n",
    "        file_outputs = {'analysis': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def dataflow_tf_predict_op(evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', target: str, model: 'TensorFlow model', predict_mode, project: 'GcpProject', prediction_output: 'GcsUri', step_name='prediction'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tf-predict:0.0.42',\n",
    "        arguments = [\n",
    "            '--data', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--target', target,\n",
    "            '--model',  model,\n",
    "            '--mode', predict_mode,\n",
    "            '--project', project,\n",
    "            '--output', prediction_output,\n",
    "        ],\n",
    "        file_outputs = {'prediction': '/output.txt'}\n",
    "    )\n",
    "\n",
    "def kubeflow_deploy_op(model: 'TensorFlow model', tf_server_name, step_name='deploy'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:dev',\n",
    "        arguments = [\n",
    "            '--model-path', model,\n",
    "            '--server-name', tf_server_name\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# The pipeline definition\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    column_names=dsl.PipelineParam(name='column-names', value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(name='train', value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv'),\n",
    "    evaluation=dsl.PipelineParam(name='evaluation', value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv'),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(name='preprocess-module', value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value='1500'),\n",
    "    steps=dsl.PipelineParam(name='steps', value=3000),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.name}}'\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output)\n",
    "    schema = '%s/schema.json' % validation.outputs['output']\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(train, evaluation, schema, project, preprocess_mode, preprocess_module, transform_output)\n",
    "    training = tf_train_op(preprocess.output, schema, learning_rate, hidden_layer_size, steps, target, preprocess_module, training_output)\n",
    "    analysis = dataflow_tf_model_analyze_op(training.output, evaluation, schema, project, analyze_mode, analyze_slice_column, analysis_output)\n",
    "    prediction = dataflow_tf_predict_op(evaluation, schema, target, training.output, predict_mode, project, prediction_output)\n",
    "    deploy = kubeflow_deploy_op(training.output, tf_server_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Job link <a href=\"/pipeline/#/runs/details/7df8f63f-e2c4-11e8-93d0-42010a800048\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compiler.Compiler().compile(taxi_cab_classification,  'tfx.tar.gz')\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'tfx', 'tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME})\n",
    "# See Screenshot 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a step in the above pipeline\n",
    "\n",
    "Let's say I got the pipeline source code from github, and I want to modify the pipeline a little bit by swapping the last deployer step with my own deployer. Instead of tf-serving deployer, I want to deploy it to Cloud ML Engine service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and test a python function for the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to run it locally we need a python package\n",
    "!pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='cmle_deployer',\n",
    "    description='deploys a model to GCP CMLE',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def deploy_model(model_dot_version: str, model_path: str, gcp_project: str, runtime: str):\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import os\n",
    "    \n",
    "    model_path = file_io.get_matching_files(os.path.join(model_path, 'export', 'export', '*'))[0]\n",
    "    api = discovery.build('ml', 'v1')\n",
    "    model_name, version_name = model_dot_version.split('.')\n",
    "    body = {'name': model_name}\n",
    "    parent = 'projects/%s' % gcp_project\n",
    "    try:\n",
    "        api.projects().models().create(body=body, parent=parent).execute()\n",
    "    except:\n",
    "        # Trying to create an already existing model gets an error. Ignore it.\n",
    "        pass\n",
    "\n",
    "    import time\n",
    "\n",
    "    body = {\n",
    "        'name': version_name,\n",
    "        'deployment_uri': model_path,\n",
    "        'runtime_version': runtime\n",
    "    }\n",
    "\n",
    "    full_mode_name = 'projects/%s/models/%s' % (gcp_project, model_name)\n",
    "    response = api.projects().models().versions().create(body=body, parent=full_mode_name).execute()\n",
    "    \n",
    "    while True:\n",
    "        response = api.projects().operations().get(name=response['name']).execute()\n",
    "        if 'done' not in response or response['done'] is not True:\n",
    "            time.sleep(5)\n",
    "            print('still deploying...')\n",
    "        else:\n",
    "            if 'error' in response:\n",
    "                print(response['error'])\n",
    "            else:\n",
    "                print('Done.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function and make sure it works.\n",
    "path = 'gs://ml-pipeline-playground/sampledata/taxi/train'\n",
    "deploy_model('taxidev.beta', path, PROJECT_NAME, '1.9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Pipeline Step With the Above Function\n",
    "\n",
    "Now that we've tested the function locally, we want to build a component that can run as a step in the pipeline. First we need to build a base docker container image. We need TensorFlow and google-api-python-client packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking path: gs://bradley-playground...\n",
      "INFO:root:Generate build files.\n",
      "INFO:root:Start a kaniko job for build.\n",
      "INFO:root:5 seconds: waiting for job to complete\n",
      "INFO:root:10 seconds: waiting for job to complete\n",
      "INFO:root:15 seconds: waiting for job to complete\n",
      "INFO:root:20 seconds: waiting for job to complete\n",
      "INFO:root:25 seconds: waiting for job to complete\n",
      "INFO:root:30 seconds: waiting for job to complete\n",
      "INFO:root:35 seconds: waiting for job to complete\n",
      "INFO:root:40 seconds: waiting for job to complete\n",
      "INFO:root:45 seconds: waiting for job to complete\n",
      "INFO:root:50 seconds: waiting for job to complete\n",
      "INFO:root:55 seconds: waiting for job to complete\n",
      "INFO:root:60 seconds: waiting for job to complete\n",
      "INFO:root:65 seconds: waiting for job to complete\n",
      "INFO:root:70 seconds: waiting for job to complete\n",
      "INFO:root:75 seconds: waiting for job to complete\n",
      "INFO:root:80 seconds: waiting for job to complete\n",
      "INFO:root:85 seconds: waiting for job to complete\n",
      "INFO:root:90 seconds: waiting for job to complete\n",
      "INFO:root:95 seconds: waiting for job to complete\n",
      "INFO:root:100 seconds: waiting for job to complete\n",
      "INFO:root:105 seconds: waiting for job to complete\n",
      "INFO:root:110 seconds: waiting for job to complete\n",
      "INFO:root:115 seconds: waiting for job to complete\n",
      "INFO:root:Kaniko job complete.\n",
      "INFO:root:Build image complete.\n"
     ]
    }
   ],
   "source": [
    "%%docker {BASE_IMAGE} {OUTPUT_DIR}\n",
    "FROM tensorflow/tensorflow:1.10.0-py3\n",
    "RUN pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the base docker container image is built, we can build a \"target\" container image that is base_image plus the python function as entry point. The target container image can be used as a step in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Build an image that is based on gcr.io/bradley-playground/pusher:dev and push the image to gcr.io/bradley-playground/pusher:latest\n",
      "INFO:root:Checking path: gs://bradley-playground...\n",
      "INFO:root:Generate entrypoint and serialization codes.\n",
      "INFO:root:Generate build files.\n",
      "INFO:root:Start a kaniko job for build.\n",
      "INFO:root:5 seconds: waiting for job to complete\n",
      "INFO:root:10 seconds: waiting for job to complete\n",
      "INFO:root:15 seconds: waiting for job to complete\n",
      "INFO:root:20 seconds: waiting for job to complete\n",
      "INFO:root:25 seconds: waiting for job to complete\n",
      "INFO:root:30 seconds: waiting for job to complete\n",
      "INFO:root:35 seconds: waiting for job to complete\n",
      "INFO:root:40 seconds: waiting for job to complete\n",
      "INFO:root:45 seconds: waiting for job to complete\n",
      "INFO:root:50 seconds: waiting for job to complete\n",
      "INFO:root:55 seconds: waiting for job to complete\n",
      "INFO:root:60 seconds: waiting for job to complete\n",
      "INFO:root:65 seconds: waiting for job to complete\n",
      "INFO:root:70 seconds: waiting for job to complete\n",
      "INFO:root:75 seconds: waiting for job to complete\n",
      "INFO:root:80 seconds: waiting for job to complete\n",
      "INFO:root:85 seconds: waiting for job to complete\n",
      "INFO:root:90 seconds: waiting for job to complete\n",
      "INFO:root:95 seconds: waiting for job to complete\n",
      "INFO:root:100 seconds: waiting for job to complete\n",
      "INFO:root:105 seconds: waiting for job to complete\n",
      "INFO:root:110 seconds: waiting for job to complete\n",
      "INFO:root:115 seconds: waiting for job to complete\n",
      "INFO:root:120 seconds: waiting for job to complete\n",
      "INFO:root:125 seconds: waiting for job to complete\n",
      "INFO:root:130 seconds: waiting for job to complete\n",
      "INFO:root:135 seconds: waiting for job to complete\n",
      "INFO:root:140 seconds: waiting for job to complete\n",
      "INFO:root:145 seconds: waiting for job to complete\n",
      "INFO:root:150 seconds: waiting for job to complete\n",
      "INFO:root:Kaniko job complete.\n",
      "INFO:root:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    target_image=TARET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the pipeline with the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My New Pipeline. It's almost the same as the original one with the last step deployer replaced.\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def my_taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    model,\n",
    "    column_names=dsl.PipelineParam(\n",
    "        name='column-names',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(\n",
    "        name='train',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv'),\n",
    "    evaluation=dsl.PipelineParam(\n",
    "        name='evaluation',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv'),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(\n",
    "        name='preprocess-module',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value='1500'),\n",
    "    steps=dsl.PipelineParam(name='steps', value=3000),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output)\n",
    "    schema = '%s/schema.json' % validation.outputs['output']\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(train, evaluation, schema, project, preprocess_mode, preprocess_module, transform_output)\n",
    "    training = tf_train_op(preprocess.output, schema, learning_rate, hidden_layer_size, steps, target, preprocess_module, training_output)\n",
    "    analysis = dataflow_tf_model_analyze_op(training.output, evaluation, schema, project, analyze_mode, analyze_slice_column, analysis_output)\n",
    "    prediction = dataflow_tf_predict_op(evaluation, schema, target, training.output, predict_mode, project, prediction_output)\n",
    "    \n",
    "    # The new deployer. Note that the DeployerOp interface is similar to the function \"deploy_model\".\n",
    "    deploy = DeployerOp(gcp_project=project, model_dot_version=model, runtime='1.9', model_path=training.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Job link <a href=\"/pipeline/#/runs/details/cb2d51d0-e2be-11e8-93d0-42010a800048\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compiler.Compiler().compile(my_taxi_cab_classification,  'my-tfx.tar.gz')\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'my-tfx', 'my-tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME,\n",
    "                                  'model': 'mytaxi.beta'})\n",
    "# See screenshot 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
