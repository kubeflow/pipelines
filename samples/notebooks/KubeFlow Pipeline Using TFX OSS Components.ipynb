{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline Using TFX OSS Components\n",
    "\n",
    "In this notebook, we will demo: \n",
    "\n",
    "* Defining a KubeFlow pipeline with Python DSL\n",
    "* Submiting it to Pipelines System\n",
    "* Customize a step in the pipeline\n",
    "\n",
    "We will use a pipeline that includes some TFX OSS components such as [TFDV](https://github.com/tensorflow/data-validation), [TFT](https://github.com/tensorflow/transform), [TFMA](https://github.com/tensorflow/model-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set your output and project. !!!Must Do before you can proceed!!!\n",
    "EXPERIMENT_NAME = 'demo'\n",
    "OUTPUT_DIR = 'Your-Gcs-Path' # Such as gs://bucket/objact/path\n",
    "PROJECT_NAME = 'Your-Gcp-Project-Name'\n",
    "BASE_IMAGE='gcr.io/%s/pusherbase:dev' % PROJECT_NAME\n",
    "TARGET_IMAGE='gcr.io/%s/pusher:dev' % PROJECT_NAME\n",
    "KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.3-rc.2/kfp.tar.gz'\n",
    "TRAIN_DATA = 'gs://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv'\n",
    "EVAL_DATA = 'gs://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv'\n",
    "HIDDEN_LAYER_SIZE = '1500'\n",
    "STEPS = 3000\n",
    "DATAFLOW_TFDV_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfdv:0.1.3-rc.2'#TODO-release: update the release tag for the next release\n",
    "DATAFLOW_TFT_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:0.1.3-rc.2'#TODO-release: update the release tag for the next release\n",
    "DATAFLOW_TFMA_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfma:0.1.3-rc.2'#TODO-release: update the release tag for the next release\n",
    "DATAFLOW_TF_PREDICT_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tf-predict:0.1.3-rc.2'#TODO-release: update the release tag for the next release\n",
    "KUBEFLOW_TF_TRAINER_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-tf-trainer:0.1.3-rc.2'#TODO-release: update the release tag for the next release\n",
    "KUBEFLOW_DEPLOYER_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:0.1.3-rc.2'#TODO-release: update the release tag for the next release\n",
    "DEV_DEPLOYER_MODEL = 'notebook-tfx-devtaxi.beta'\n",
    "PROD_DEPLOYER_MODEL = 'notebook-tfx-prodtaxi.beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.3-rc.2/kfp.tar.gz\n",
      "\u001b[?25l  Downloading https://storage.googleapis.com/ml-pipeline/release/0.1.3-rc.2/kfp.tar.gz (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 9.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3>=1.15 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (2018.10.15)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage==1.13.0 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes==8.0.0 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (8.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=0.1.1 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<0.29dev,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (0.28.1)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media>=0.3.1 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: adal>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (38.4.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (0.54.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (1.5.5)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=0.6.2 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib->kubernetes==8.0.0->kfp==0.1) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (2.1.4)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (1.6.4)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->kubernetes==8.0.0->kfp==0.1) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->kubernetes==8.0.0->kfp==0.1) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (0.24.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (1.11.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=1.1.0->adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (2.18)\n",
      "Building wheels for collected packages: kfp\n",
      "  Running setup.py bdist_wheel for kfp ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-bctiow25/wheels/f9/43/15/db34c8d6291d495360ac6eea475e6bf473750320547f837caf\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Found existing installation: kfp 0.1\n",
      "    Uninstalling kfp-0.1:\n",
      "      Successfully uninstalled kfp-0.1\n",
      "Successfully installed kfp-0.1\n"
     ]
    }
   ],
   "source": [
    "# Install Pipeline SDK\n",
    "!pip3 install $KFP_PACKAGE --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Experiment in the Pipeline System\n",
    "\n",
    "Pipeline system requires an \"Experiment\" to group pipeline runs. You can create a new experiment, or call client.list_experiments() to get existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/a13c1b50-93db-40b5-89a2-a72a8129606b\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that this notebook should be running in JupyterHub in the same cluster as the pipeline system.\n",
    "# Otherwise it will fail to talk to the pipeline system.\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.notebook\n",
    "import kfp.gcp as gcp\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ml-pipeline-playground/coin.tar.gz...\n",
      "/ [1 files][  978.0 B/  978.0 B]                                                \n",
      "Operation completed over 1 objects/978.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "# Download a pipeline package\n",
    "!gsutil cp gs://ml-pipeline-playground/coin.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/67b216a8-f1ad-11e8-927c-42010a8000f7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(exp.id, 'coin', 'coin.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline\n",
    "\n",
    "Authoring a pipeline is just like authoring a normal Python function. The pipeline function describes the topology of the pipeline. Each step in the pipeline is typically a ContainerOp --- a simple class or function describing how to interact with a docker container image. In the below pipeline, all the container images referenced in the pipeline are already built. The pipeline starts with a TFDV step which is used to infer the schema of the data. Then it uses TFT to transform the data for training. After a single node training step, it analyze the test data predictions and generate a feature slice metrics view using a TFMA component. At last, it deploys the model to TF-Serving inside the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "\n",
    "# Below are a list of helper functions to wrap the components to provide a simpler interface for pipeline function.\n",
    "def dataflow_tf_data_validation_op(inference_data: 'GcsUri', validation_data: 'GcsUri', column_names: 'GcsUri[text/json]', key_columns, project: 'GcpProject', mode, validation_output: 'GcsUri[Directory]', step_name='validation'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TFDV_IMAGE,\n",
    "        arguments = [\n",
    "            '--csv-data-for-inference', inference_data,\n",
    "            '--csv-data-to-validate', validation_data,\n",
    "            '--column-names', column_names,\n",
    "            '--key-columns', key_columns,\n",
    "            '--project', project,\n",
    "            '--mode', mode,\n",
    "            '--output', validation_output,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'output': '/output.txt',\n",
    "            'schema': '/output_schema.json',\n",
    "        }\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "def dataflow_tf_transform_op(train_data: 'GcsUri', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', preprocess_mode, preprocess_module: 'GcsUri[text/code/python]', transform_output: 'GcsUri[Directory]', step_name='preprocess'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TFT_IMAGE,\n",
    "        arguments = [\n",
    "            '--train', train_data,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', preprocess_mode,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--output', transform_output,\n",
    "        ],\n",
    "        file_outputs = {'transformed': '/output.txt'}\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "\n",
    "def tf_train_op(transformed_data_dir, schema: 'GcsUri[text/json]', learning_rate: float, hidden_layer_size: int, steps: int, target: str, preprocess_module: 'GcsUri[text/code/python]', training_output: 'GcsUri[Directory]', step_name='training'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = KUBEFLOW_TF_TRAINER_IMAGE,\n",
    "        arguments = [\n",
    "            '--transformed-data-dir', transformed_data_dir,\n",
    "            '--schema', schema,\n",
    "            '--learning-rate', learning_rate,\n",
    "            '--hidden-layer-size', hidden_layer_size,\n",
    "            '--steps', steps,\n",
    "            '--target', target,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--job-dir', training_output,\n",
    "        ],\n",
    "        file_outputs = {'train': '/output.txt'}\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "def dataflow_tf_model_analyze_op(model: 'TensorFlow model', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', analyze_mode, analyze_slice_column, analysis_output: 'GcsUri', step_name='analysis'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TFMA_IMAGE,\n",
    "        arguments = [\n",
    "            '--model', model,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', analyze_mode,\n",
    "            '--slice-columns', analyze_slice_column,\n",
    "            '--output', analysis_output,\n",
    "        ],\n",
    "        file_outputs = {'analysis': '/output.txt'}\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "\n",
    "def dataflow_tf_predict_op(evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', target: str, model: 'TensorFlow model', predict_mode, project: 'GcpProject', prediction_output: 'GcsUri', step_name='prediction'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TF_PREDICT_IMAGE,\n",
    "        arguments = [\n",
    "            '--data', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--target', target,\n",
    "            '--model',  model,\n",
    "            '--mode', predict_mode,\n",
    "            '--project', project,\n",
    "            '--output', prediction_output,\n",
    "        ],\n",
    "        file_outputs = {'prediction': '/output.txt'}\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "def kubeflow_deploy_op(model: 'TensorFlow model', tf_server_name, step_name='deploy'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = KUBEFLOW_DEPLOYER_IMAGE,\n",
    "        arguments = [\n",
    "            '--model-path', model,\n",
    "            '--server-name', tf_server_name\n",
    "        ]\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "\n",
    "# The pipeline definition\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    column_names=dsl.PipelineParam(name='column-names', value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(name='train', value=TRAIN_DATA),\n",
    "    evaluation=dsl.PipelineParam(name='evaluation', value=EVAL_DATA),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(name='preprocess-module', value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value=HIDDEN_LAYER_SIZE),\n",
    "    steps=dsl.PipelineParam(name='steps', value=STEPS),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.name}}'\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output)\n",
    "    schema = '%s/schema.json' % validation.outputs['output']\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(train, evaluation, schema, project, preprocess_mode, preprocess_module, transform_output)\n",
    "    training = tf_train_op(preprocess.output, schema, learning_rate, hidden_layer_size, steps, target, preprocess_module, training_output)\n",
    "    analysis = dataflow_tf_model_analyze_op(training.output, evaluation, schema, project, analyze_mode, analyze_slice_column, analysis_output)\n",
    "    prediction = dataflow_tf_predict_op(evaluation, schema, target, training.output, predict_mode, project, prediction_output)\n",
    "    deploy = kubeflow_deploy_op(training.output, tf_server_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/d1a380ae-f1ad-11e8-927c-42010a8000f7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile it into a tar package.\n",
    "compiler.Compiler().compile(taxi_cab_classification,  'tfx.tar.gz')\n",
    "\n",
    "# Submit a run.\n",
    "run = client.run_pipeline(exp.id, 'tfx', 'tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a step in the above pipeline\n",
    "\n",
    "Let's say I got the pipeline source code from github, and I want to modify the pipeline a little bit by swapping the last deployer step with my own deployer. Instead of tf-serving deployer, I want to deploy it to Cloud ML Engine service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and test a python function for the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/conda/lib/python3.6/site-packages (1.7.4)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (1.6.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (0.11.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (3.0.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (1.11.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (0.0.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (3.0.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# in order to run it locally we need a python package\n",
    "!pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='cmle_deployer',\n",
    "    description='deploys a model to GCP CMLE',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def deploy_model(model_dot_version: str, model_path: str, gcp_project: str, runtime: str):\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import os\n",
    "    \n",
    "    model_path = file_io.get_matching_files(os.path.join(model_path, 'export', 'export', '*'))[0]\n",
    "    api = discovery.build('ml', 'v1')\n",
    "    model_name, version_name = model_dot_version.split('.')\n",
    "    body = {'name': model_name}\n",
    "    parent = 'projects/%s' % gcp_project\n",
    "    try:\n",
    "        api.projects().models().create(body=body, parent=parent).execute()\n",
    "    except:\n",
    "        # Trying to create an already existing model gets an error. Ignore it.\n",
    "        pass\n",
    "\n",
    "    import time\n",
    "\n",
    "    body = {\n",
    "        'name': version_name,\n",
    "        'deployment_uri': model_path,\n",
    "        'runtime_version': runtime\n",
    "    }\n",
    "\n",
    "    full_mode_name = 'projects/%s/models/%s' % (gcp_project, model_name)\n",
    "    response = api.projects().models().versions().create(body=body, parent=full_mode_name).execute()\n",
    "    \n",
    "    while True:\n",
    "        response = api.projects().operations().get(name=response['name']).execute()\n",
    "        if 'done' not in response or response['done'] is not True:\n",
    "            time.sleep(5)\n",
    "            print('still deploying...')\n",
    "        else:\n",
    "            if 'error' in response:\n",
    "                print(response['error'])\n",
    "            else:\n",
    "                print('Done.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function and make sure it works.\n",
    "path = 'gs://ml-pipeline-playground/sampledata/taxi/train'\n",
    "deploy_model(DEV_DEPLOYER_MODEL, path, PROJECT_NAME, '1.9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Pipeline Step With the Above Function(Note: run either of the two options below)\n",
    "#### Option One: Specify the dependency directly\n",
    "Now that we've tested the function locally, we want to build a component that can run as a step in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-26 19:29:13:INFO:Build an image that is based on gcr.io/ml-pipeline-dogfood/pusherbase:dev and push the image to gcr.io/ml-pipeline-dogfood/pusher:dev\n",
      "2018-11-26 19:29:13:INFO:Checking path: gs://ngao-bugbash...\n",
      "2018-11-26 19:29:13:INFO:Generate entrypoint and serialization codes.\n",
      "2018-11-26 19:29:13:INFO:Generate build files.\n",
      "2018-11-26 19:29:13:INFO:Start a kaniko job for build.\n",
      "2018-11-26 19:29:18:INFO:5 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:23:INFO:10 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:28:INFO:15 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:33:INFO:20 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:38:INFO:25 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:43:INFO:30 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:48:INFO:35 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:53:INFO:40 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:58:INFO:45 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:03:INFO:50 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:08:INFO:55 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:13:INFO:60 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:18:INFO:65 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:23:INFO:70 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:28:INFO:75 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:33:INFO:80 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:38:INFO:85 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:43:INFO:90 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:48:INFO:95 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:53:INFO:100 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:58:INFO:105 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:03:INFO:110 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:08:INFO:115 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:13:INFO:120 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:18:INFO:125 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:24:INFO:130 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:29:INFO:135 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:34:INFO:140 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:39:INFO:145 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:39:INFO:Kaniko job complete.\n",
      "2018-11-26 19:31:39:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='google-api-python-client', version='1.7.0')],\n",
    "    base_image='tensorflow/tensorflow:1.12.0',\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option Two: build a base docker container image with both tensorflow and google api client packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking path: gs://bradley-playground...\n",
      "INFO:root:Generate build files.\n",
      "INFO:root:Start a kaniko job for build.\n",
      "INFO:root:5 seconds: waiting for job to complete\n",
      "INFO:root:10 seconds: waiting for job to complete\n",
      "INFO:root:15 seconds: waiting for job to complete\n",
      "INFO:root:20 seconds: waiting for job to complete\n",
      "INFO:root:25 seconds: waiting for job to complete\n",
      "INFO:root:30 seconds: waiting for job to complete\n",
      "INFO:root:35 seconds: waiting for job to complete\n",
      "INFO:root:40 seconds: waiting for job to complete\n",
      "INFO:root:45 seconds: waiting for job to complete\n",
      "INFO:root:50 seconds: waiting for job to complete\n",
      "INFO:root:55 seconds: waiting for job to complete\n",
      "INFO:root:60 seconds: waiting for job to complete\n",
      "INFO:root:65 seconds: waiting for job to complete\n",
      "INFO:root:70 seconds: waiting for job to complete\n",
      "INFO:root:75 seconds: waiting for job to complete\n",
      "INFO:root:80 seconds: waiting for job to complete\n",
      "INFO:root:85 seconds: waiting for job to complete\n",
      "INFO:root:90 seconds: waiting for job to complete\n",
      "INFO:root:95 seconds: waiting for job to complete\n",
      "INFO:root:100 seconds: waiting for job to complete\n",
      "INFO:root:105 seconds: waiting for job to complete\n",
      "INFO:root:110 seconds: waiting for job to complete\n",
      "INFO:root:115 seconds: waiting for job to complete\n",
      "INFO:root:Kaniko job complete.\n",
      "INFO:root:Build image complete.\n"
     ]
    }
   ],
   "source": [
    "%%docker {BASE_IMAGE} {OUTPUT_DIR}\n",
    "FROM tensorflow/tensorflow:1.10.0-py3\n",
    "RUN pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the base docker container image is built, we can build a \"target\" container image that is base_image plus the python function as entry point. The target container image can be used as a step in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Build an image that is based on gcr.io/bradley-playground/pusher:dev and push the image to gcr.io/bradley-playground/pusher:latest\n",
      "INFO:root:Checking path: gs://bradley-playground...\n",
      "INFO:root:Generate entrypoint and serialization codes.\n",
      "INFO:root:Generate build files.\n",
      "INFO:root:Start a kaniko job for build.\n",
      "INFO:root:5 seconds: waiting for job to complete\n",
      "INFO:root:10 seconds: waiting for job to complete\n",
      "INFO:root:15 seconds: waiting for job to complete\n",
      "INFO:root:20 seconds: waiting for job to complete\n",
      "INFO:root:25 seconds: waiting for job to complete\n",
      "INFO:root:30 seconds: waiting for job to complete\n",
      "INFO:root:35 seconds: waiting for job to complete\n",
      "INFO:root:40 seconds: waiting for job to complete\n",
      "INFO:root:45 seconds: waiting for job to complete\n",
      "INFO:root:50 seconds: waiting for job to complete\n",
      "INFO:root:55 seconds: waiting for job to complete\n",
      "INFO:root:60 seconds: waiting for job to complete\n",
      "INFO:root:65 seconds: waiting for job to complete\n",
      "INFO:root:70 seconds: waiting for job to complete\n",
      "INFO:root:75 seconds: waiting for job to complete\n",
      "INFO:root:80 seconds: waiting for job to complete\n",
      "INFO:root:85 seconds: waiting for job to complete\n",
      "INFO:root:90 seconds: waiting for job to complete\n",
      "INFO:root:95 seconds: waiting for job to complete\n",
      "INFO:root:100 seconds: waiting for job to complete\n",
      "INFO:root:105 seconds: waiting for job to complete\n",
      "INFO:root:110 seconds: waiting for job to complete\n",
      "INFO:root:115 seconds: waiting for job to complete\n",
      "INFO:root:120 seconds: waiting for job to complete\n",
      "INFO:root:125 seconds: waiting for job to complete\n",
      "INFO:root:130 seconds: waiting for job to complete\n",
      "INFO:root:135 seconds: waiting for job to complete\n",
      "INFO:root:140 seconds: waiting for job to complete\n",
      "INFO:root:145 seconds: waiting for job to complete\n",
      "INFO:root:150 seconds: waiting for job to complete\n",
      "INFO:root:Kaniko job complete.\n",
      "INFO:root:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the pipeline with the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My New Pipeline. It's almost the same as the original one with the last step deployer replaced.\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def my_taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    model,\n",
    "    column_names=dsl.PipelineParam(\n",
    "        name='column-names',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(\n",
    "        name='train',\n",
    "        value=TRAIN_DATA),\n",
    "    evaluation=dsl.PipelineParam(\n",
    "        name='evaluation',\n",
    "        value=EVAL_DATA),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(\n",
    "        name='preprocess-module',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value=HIDDEN_LAYER_SIZE),\n",
    "    steps=dsl.PipelineParam(name='steps', value=STEPS),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output)\n",
    "    schema = '%s/schema.json' % validation.outputs['output']\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(train, evaluation, schema, project, preprocess_mode, preprocess_module, transform_output)\n",
    "    training = tf_train_op(preprocess.output, schema, learning_rate, hidden_layer_size, steps, target, preprocess_module, training_output)\n",
    "    analysis = dataflow_tf_model_analyze_op(training.output, evaluation, schema, project, analyze_mode, analyze_slice_column, analysis_output)\n",
    "    prediction = dataflow_tf_predict_op(evaluation, schema, target, training.output, predict_mode, project, prediction_output)\n",
    "    \n",
    "    # The new deployer. Note that the DeployerOp interface is similar to the function \"deploy_model\".\n",
    "    deploy = DeployerOp(gcp_project=project, model_dot_version=model, runtime='1.9', model_path=training.output).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/cb2d51d0-e2be-11e8-93d0-42010a800048\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compiler.Compiler().compile(my_taxi_cab_classification,  'my-tfx.tar.gz')\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'my-tfx', 'my-tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME,\n",
    "                                  'model': PROD_DEPLOYER_MODEL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
