{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline DSL Static Type Checking\n",
    "\n",
    "In this notebook, we will demo: \n",
    "\n",
    "* Defining a KubeFlow pipeline with Python DSL\n",
    "* Compile the pipeline with type checking\n",
    "\n",
    "Since this sample focuses on the DSL type checking, we will use components that are not runnable in the system but with various type checking scenarios.\n",
    "\n",
    "## Component definition\n",
    "Components can be defined in either YAML or functions decorated by dsl.component.\n",
    "\n",
    "## Type definition\n",
    "Types can be defined as string or a dictionary formatted as:\n",
    "{\n",
    "    type_name: {\n",
    "        property_a: value_a,\n",
    "        property_b: value_b\n",
    "    }\n",
    "}\n",
    "If you define the component using the function decorator, there are a list of [core types](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/dsl/_types.py).\n",
    "\n",
    "## Type check switch\n",
    "Type checking is disabled by default. It can be enabled as --type-check argument if dsl-compile is run in the command line, or dsl.compiler.Compiler().compile(type_check=True).\n",
    "\n",
    "## How does type checking work?\n",
    "DSL compiler checks the type consistencies among components by checking the type_name as well as the property keys and values. Some special cases are listed here:\n",
    "1. Type checking succeed: If the upstream component output has fewer properties than the downstream component input.\n",
    "2. Type checking fail: If the upstream component has more properties than the downstream component input.\n",
    "3. Type checking succeed: If the upstream/downstream components lack the type information.\n",
    "4. Type checking succeed: If the type check is disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set your output and project. !!!Must Do before you can proceed!!!\n",
    "KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.12/kfp.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install Pipeline SDK\n",
    "!pip3 install $KFP_PACKAGE --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components in YAML(successful type checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_a = '''\\\n",
    "name: component a\n",
    "description: component a desc\n",
    "inputs:\n",
    "  - {name: field_l, type: Integer}\n",
    "outputs:\n",
    "  - {name: field_m, type: {GCSPath: {path_type: file, file_type: csv}}}\n",
    "  - {name: field_n, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_o, type: GcsUri} \n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3, /pipelines/component/src/train.py]\n",
    "    args: [\n",
    "      --field-l, {inputValue: field_l},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      field_m: /schema.txt\n",
    "      field_n: /feature.txt\n",
    "      field_o: /output.txt\n",
    "'''\n",
    "component_b = '''\\\n",
    "name: component b\n",
    "description: component b desc\n",
    "inputs:\n",
    "  - {name: field_x, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_y, type: GcsUri}\n",
    "  - {name: field_z, type: {GCSPath: {path_type: file, file_type: csv}}}\n",
    "outputs:\n",
    "  - {name: output_model_uri, type: GcsUri}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3]\n",
    "    args: [\n",
    "      --field-x, {inputValue: field_x},\n",
    "      --field-y, {inputValue: field_y},\n",
    "      --field-z, {inputValue: field_z},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      output_model_uri: /schema.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "task_factory_a = comp.load_component_from_text(text=component_a)\n",
    "task_factory_b = comp.load_component_from_text(text=component_b)\n",
    "\n",
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check',\n",
    "    description='')\n",
    "def pipeline_a():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_a, 'pipeline_a.tar.gz', type_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components in YAML(failed type checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_a = '''\\\n",
    "name: component a\n",
    "description: component a desc\n",
    "inputs:\n",
    "  - {name: field_l, type: Integer}\n",
    "outputs:\n",
    "  - {name: field_m, type: {GCSPath: {path_type: file, file_type: csv}}}\n",
    "  - {name: field_n, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_o, type: GcsUri} \n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3, /pipelines/component/src/train.py]\n",
    "    args: [\n",
    "      --field-l, {inputValue: field_l},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      field_m: /schema.txt\n",
    "      field_n: /feature.txt\n",
    "      field_o: /output.txt\n",
    "'''\n",
    "component_b = '''\\\n",
    "name: component b\n",
    "description: component b desc\n",
    "inputs:\n",
    "  - {name: field_x, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_y, type: GcsUri}\n",
    "  - {name: field_z, type: {GCSPath: {path_type: file, file_type: tsv}}}\n",
    "outputs:\n",
    "  - {name: output_model_uri, type: GcsUri}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3]\n",
    "    args: [\n",
    "      --field-x, {inputValue: field_x},\n",
    "      --field-y, {inputValue: field_y},\n",
    "      --field-z, {inputValue: field_z},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      output_model_uri: /schema.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCSPath has a property file_type with value: csv and tsv\n"
     ]
    },
    {
     "ename": "InconsistentTypeException",
     "evalue": "Component \"component b\" is expecting field_z to be type({'GCSPath': OrderedDict([('path_type', 'file'), ('file_type', 'tsv')])}), but the passed argument is type({'GCSPath': OrderedDict([('path_type', 'file'), ('file_type', 'csv')])})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInconsistentTypeException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-869a14a01256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_factory_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field_n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field_m'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline_a.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mkfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mworkflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDumper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_aliases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0myaml_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_flow_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(self, pipeline_func)\u001b[0m\n\u001b[1;32m    524\u001b[0m                  for arg_name in argspec.args]\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m       \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;31m# Remove when argo supports local exit handler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-869a14a01256>\u001b[0m in \u001b[0;36mpipeline_a\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpipeline_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_factory_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_factory_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field_n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field_m'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline_a.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kfp/components/_dynamic.py\u001b[0m in \u001b[0;36mcomponent b\u001b[0;34m(field_x, field_y, field_z)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpass_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpass_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kfp/components/_components.py\u001b[0m in \u001b[0;36mcreate_task_from_component_and_arguments\u001b[0;34m(pythonic_arguments)\u001b[0m\n\u001b[1;32m    217\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict_or_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                                 \u001b[0;32mraise\u001b[0m \u001b[0mInconsistentTypeException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Component \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\" is expecting '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to be type('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'), but the passed argument is type('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInconsistentTypeException\u001b[0m: Component \"component b\" is expecting field_z to be type({'GCSPath': OrderedDict([('path_type', 'file'), ('file_type', 'tsv')])}), but the passed argument is type({'GCSPath': OrderedDict([('path_type', 'file'), ('file_type', 'csv')])})"
     ]
    }
   ],
   "source": [
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "task_factory_a = comp.load_component_from_text(text=component_a)\n",
    "task_factory_b = comp.load_component_from_text(text=component_b)\n",
    "\n",
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check',\n",
    "    description='')\n",
    "def pipeline_a():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_a, 'pipeline_a.tar.gz', type_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components but type checking disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_a, 'pipeline_a.tar.gz', type_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components with decorator(successful type checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='google-api-python-client', version='1.7.0')],\n",
    "    base_image='tensorflow/tensorflow:1.12.0-py3',\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option Two: build a base docker container image with both tensorflow and google api client packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%docker {BASE_IMAGE} {OUTPUT_DIR}\n",
    "FROM tensorflow/tensorflow:1.10.0-py3\n",
    "RUN pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the base docker container image is built, we can build a \"target\" container image that is base_image plus the python function as entry point. The target container image can be used as a step in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the pipeline with the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My New Pipeline. It's almost the same as the original one with the last step deployer replaced.\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def my_taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    model,\n",
    "    version,\n",
    "    column_names=dsl.PipelineParam(\n",
    "        name='column-names',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(\n",
    "        name='train',\n",
    "        value=TRAIN_DATA),\n",
    "    evaluation=dsl.PipelineParam(\n",
    "        name='evaluation',\n",
    "        value=EVAL_DATA),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(\n",
    "        name='preprocess-module',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value=HIDDEN_LAYER_SIZE),\n",
    "    steps=dsl.PipelineParam(name='steps', value=STEPS),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(\n",
    "        train, evaluation, column_names, key_columns, project,\n",
    "        validation_mode, validation_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        train, evaluation, validation.outputs['schema'], project, preprocess_mode,\n",
    "        preprocess_module, transform_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    training = tf_train_op(\n",
    "        preprocess.output, validation.outputs['schema'], learning_rate, hidden_layer_size,\n",
    "        steps, target, preprocess_module, training_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    analysis = dataflow_tf_model_analyze_op(\n",
    "        training.output, evaluation, validation.outputs['schema'], project,\n",
    "        analyze_mode, analyze_slice_column, analysis_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        evaluation, validation.outputs['schema'], target, training.output,\n",
    "        predict_mode, project, prediction_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    # The new deployer. Note that the DeployerOp interface is similar to the function \"deploy_model\".\n",
    "    deploy = DeployerOp(\n",
    "        gcp_project=project, model_name=model, version_name=version, runtime='1.9',\n",
    "        model_path=training.output).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(my_taxi_cab_classification,  'my-tfx.tar.gz')\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'my-tfx', 'my-tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME,\n",
    "                                  'model': DEPLOYER_MODEL,\n",
    "                                  'version': DEPLOYER_VERSION_PROD})\n",
    "\n",
    "result = client.wait_for_run_completion(run.id, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a step in Python2\n",
    "Let's reuse the deploy_model function defined above. However, this time we will use python2 instead of the default python3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "#TODO: demonstrate the python2 support in another sample.\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='google-api-python-client', version='1.7.0')],\n",
    "    base_image='tensorflow/tensorflow:1.12.0',\n",
    "    target_image=TARGET_IMAGE_TWO,\n",
    "    python_version='python2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the pipeline with the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My New Pipeline. It's almost the same as the original one with the last step deployer replaced.\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def my_taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    model,\n",
    "    version,\n",
    "    column_names=dsl.PipelineParam(\n",
    "        name='column-names',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(\n",
    "        name='train',\n",
    "        value=TRAIN_DATA),\n",
    "    evaluation=dsl.PipelineParam(\n",
    "        name='evaluation',\n",
    "        value=EVAL_DATA),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(\n",
    "        name='preprocess-module',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value=HIDDEN_LAYER_SIZE),\n",
    "    steps=dsl.PipelineParam(name='steps', value=STEPS),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(\n",
    "        train, evaluation, column_names, key_columns, project,\n",
    "        validation_mode, validation_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        train, evaluation, validation.outputs['schema'], project, preprocess_mode,\n",
    "        preprocess_module, transform_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    training = tf_train_op(\n",
    "        preprocess.output, validation.outputs['schema'], learning_rate, hidden_layer_size,\n",
    "        steps, target, preprocess_module, training_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    analysis = dataflow_tf_model_analyze_op(\n",
    "        training.output, evaluation, validation.outputs['schema'], project,\n",
    "        analyze_mode, analyze_slice_column, analysis_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        evaluation, validation.outputs['schema'], target, training.output,\n",
    "        predict_mode, project, prediction_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    # The new deployer. Note that the DeployerOp interface is similar to the function \"deploy_model\".\n",
    "    deploy = DeployerOp(\n",
    "        gcp_project=project, model_name=model, version_name=version, runtime='1.9',\n",
    "        model_path=training.output).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(my_taxi_cab_classification,  'my-tfx-two.tar.gz')\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'my-tfx-two', 'my-tfx-two.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME,\n",
    "                                  'model': DEPLOYER_MODEL,\n",
    "                                  'version': DEPLOYER_VERSION_PROD_TWO})\n",
    "\n",
    "result = client.wait_for_run_completion(run.id, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow3-user@bradley-playground.iam.gserviceaccount.com]\n",
      "Deleting version [prod]......done.                                             \n",
      "Deleting version [dev]......done.                                              \n",
      "Deleting model [notebook_tfx_taxi]...done.                                     \n"
     ]
    }
   ],
   "source": [
    "# the step is only needed if you are using an in-cluster JupyterHub instance.\n",
    "!gcloud auth activate-service-account --key-file /var/run/secrets/sa/user-gcp-sa.json\n",
    "\n",
    "\n",
    "!gcloud ml-engine versions delete $DEPLOYER_VERSION_PROD --model $DEPLOYER_MODEL -q\n",
    "!gcloud ml-engine versions delete $DEPLOYER_VERSION_PROD_TWO --model $DEPLOYER_MODEL -q\n",
    "!gcloud ml-engine versions delete $DEPLOYER_VERSION_DEV --model $DEPLOYER_MODEL -q\n",
    "!gcloud ml-engine models delete $DEPLOYER_MODEL -q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
