{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline DSL Static Type Checking\n",
    "\n",
    "In this notebook, we will demo: \n",
    "\n",
    "* Defining a KubeFlow pipeline with Python DSL\n",
    "* Compile the pipeline with type checking\n",
    "\n",
    "Since this sample focuses on the DSL type checking, we will use components that are not runnable in the system but with various type checking scenarios.\n",
    "\n",
    "## Component definition\n",
    "Components can be defined in either YAML or functions decorated by dsl.component.\n",
    "\n",
    "## Type definition\n",
    "Types can be defined as string or a dictionary formatted as:\n",
    "{\n",
    "    type_name: {\n",
    "        property_a: value_a,\n",
    "        property_b: value_b\n",
    "    }\n",
    "}\n",
    "If you define the component using the function decorator, there are a list of [core types](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/dsl/_types.py).\n",
    "\n",
    "## Type check switch\n",
    "Type checking is disabled by default. It can be enabled as --type-check argument if dsl-compile is run in the command line, or dsl.compiler.Compiler().compile(type_check=True).\n",
    "\n",
    "## How does type checking work?\n",
    "DSL compiler checks the type consistencies among components by checking the type_name as well as the property keys and values. Some special cases are listed here:\n",
    "1. Type checking succeed: If the upstream component output has fewer properties than the downstream component input.\n",
    "2. Type checking fail: If the upstream component has more properties than the downstream component input.\n",
    "3. Type checking succeed: If the upstream/downstream components lack the type information.\n",
    "4. Type checking succeed: If the type check is disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set your output and project. !!!Must Do before you can proceed!!!\n",
    "KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.12/kfp-experiment.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Install Pipeline SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for ngao: \n",
      "pam_glogin: invalid password\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install $KFP_PACKAGE --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check with YAML components: successful scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components in YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_a = '''\\\n",
    "name: component a\n",
    "description: component a desc\n",
    "inputs:\n",
    "  - {name: field_l, type: Integer}\n",
    "outputs:\n",
    "  - {name: field_m, type: {GCSPath: {path_type: file, file_type: csv}}}\n",
    "  - {name: field_n, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_o, type: GcsUri} \n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3, /pipelines/component/src/train.py]\n",
    "    args: [\n",
    "      --field-l, {inputValue: field_l},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      field_m: /schema.txt\n",
    "      field_n: /feature.txt\n",
    "      field_o: /output.txt\n",
    "'''\n",
    "component_b = '''\\\n",
    "name: component b\n",
    "description: component b desc\n",
    "inputs:\n",
    "  - {name: field_x, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_y, type: GcsUri}\n",
    "  - {name: field_z, type: {GCSPath: {path_type: file, file_type: csv}}}\n",
    "outputs:\n",
    "  - {name: output_model_uri, type: GcsUri}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3]\n",
    "    args: [\n",
    "      --field-x, {inputValue: field_x},\n",
    "      --field-y, {inputValue: field_y},\n",
    "      --field-z, {inputValue: field_z},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      output_model_uri: /schema.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "task_factory_a = comp.load_component_from_text(text=component_a)\n",
    "task_factory_b = comp.load_component_from_text(text=component_b)\n",
    "\n",
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check_a',\n",
    "    description='')\n",
    "def pipeline_a():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_a, 'pipeline_a.tar.gz', type_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check with YAML components: failed scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components in YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_a = '''\\\n",
    "name: component a\n",
    "description: component a desc\n",
    "inputs:\n",
    "  - {name: field_l, type: Integer}\n",
    "outputs:\n",
    "  - {name: field_m, type: {GCSPath: {path_type: file, file_type: csv}}}\n",
    "  - {name: field_n, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_o, type: GcsUri} \n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3, /pipelines/component/src/train.py]\n",
    "    args: [\n",
    "      --field-l, {inputValue: field_l},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      field_m: /schema.txt\n",
    "      field_n: /feature.txt\n",
    "      field_o: /output.txt\n",
    "'''\n",
    "component_b = '''\\\n",
    "name: component b\n",
    "description: component b desc\n",
    "inputs:\n",
    "  - {name: field_x, type: {customized_type: {property_a: value_a, property_b: value_b}}}\n",
    "  - {name: field_y, type: GcsUri}\n",
    "  - {name: field_z, type: {GCSPath: {path_type: file, file_type: tsv}}}\n",
    "outputs:\n",
    "  - {name: output_model_uri, type: GcsUri}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/component-a\n",
    "    command: [python3]\n",
    "    args: [\n",
    "      --field-x, {inputValue: field_x},\n",
    "      --field-y, {inputValue: field_y},\n",
    "      --field-z, {inputValue: field_z},\n",
    "    ]\n",
    "    fileOutputs: \n",
    "      output_model_uri: /schema.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCSPath has a property file_type with value: csv and tsv\n",
      "Component \"component b\" is expecting field_z to be type({'GCSPath': OrderedDict([('path_type', 'file'), ('file_type', 'tsv')])}), but the passed argument is type({'GCSPath': OrderedDict([('path_type', 'file'), ('file_type', 'csv')])})\n"
     ]
    }
   ],
   "source": [
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp.dsl._types import InconsistentTypeException\n",
    "task_factory_a = comp.load_component_from_text(text=component_a)\n",
    "task_factory_b = comp.load_component_from_text(text=component_b)\n",
    "\n",
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check_b',\n",
    "    description='')\n",
    "def pipeline_b():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "try:\n",
    "    compiler.Compiler().compile(pipeline_b, 'pipeline_b.tar.gz', type_check=True)\n",
    "except InconsistentTypeException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components but type checking disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_b, 'pipeline_b.tar.gz', type_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check with decorated components: successful scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components with decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl._component import component\n",
    "from kfp.dsl._types import Integer, GCSPath\n",
    "from kfp.dsl import ContainerOp\n",
    "@component\n",
    "def task_factory_a(field_l: Integer()) -> {'field_m': {'GCSPath': {'path_type': 'file', 'file_type':'tsv'}}, \n",
    "                                           'field_n': {'customized_type': {'property_a': 'value_a', 'property_b': 'value_b'}},\n",
    "                                           'field_o': 'Integer'\n",
    "                                          }:\n",
    "    return ContainerOp(\n",
    "        name = 'operator a',\n",
    "        image = 'gcr.io/ml-pipeline/component-a',\n",
    "        arguments = [\n",
    "            '--field-l', field_l,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'field_m': '/schema.txt',\n",
    "            'field_n': '/feature.txt',\n",
    "            'field_o': '/output.txt'\n",
    "        }\n",
    "    )\n",
    "\n",
    "@component\n",
    "def task_factory_b(field_x: {'customized_type': {'property_a': 'value_a', 'property_b': 'value_b'}},\n",
    "        field_y: Integer(),\n",
    "        field_z: GCSPath(path_type='file', file_type='tsv')) -> {'output_model_uri': 'GcsUri'}:\n",
    "    return ContainerOp(\n",
    "        name = 'operator b',\n",
    "        image = 'gcr.io/ml-pipeline/component-a',\n",
    "        command = [\n",
    "            'python3',\n",
    "            field_x,\n",
    "        ],\n",
    "        arguments = [\n",
    "            '--field-y', field_y,\n",
    "            '--field-z', field_z,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'output_model_uri': '/schema.txt',\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check_c',\n",
    "    description='')\n",
    "def pipeline_c():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_c, 'pipeline_c.tar.gz', type_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check with decorated components: failure scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components with decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl._component import component\n",
    "from kfp.dsl._types import Integer, GCSPath\n",
    "from kfp.dsl import ContainerOp\n",
    "@component\n",
    "def task_factory_a(field_l: Integer()) -> {'field_m': {'GCSPaths': {'path_type': 'file', 'file_type':'tsv'}}, \n",
    "                                           'field_n': {'customized_type': {'property_a': 'value_a', 'property_b': 'value_b'}},\n",
    "                                           'field_o': 'Integer'\n",
    "                                          }:\n",
    "    return ContainerOp(\n",
    "        name = 'operator a',\n",
    "        image = 'gcr.io/ml-pipeline/component-a',\n",
    "        arguments = [\n",
    "            '--field-l', field_l,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'field_m': '/schema.txt',\n",
    "            'field_n': '/feature.txt',\n",
    "            'field_o': '/output.txt'\n",
    "        }\n",
    "    )\n",
    "\n",
    "@component\n",
    "def task_factory_b(field_x: {'customized_type': {'property_a': 'value_a', 'property_b': 'value_b'}},\n",
    "        field_y: Integer(),\n",
    "        field_z: GCSPath(path_type='file', file_type='tsv')) -> {'output_model_uri': 'GcsUri'}:\n",
    "    return ContainerOp(\n",
    "        name = 'operator b',\n",
    "        image = 'gcr.io/ml-pipeline/component-a',\n",
    "        command = [\n",
    "            'python3',\n",
    "            field_x,\n",
    "        ],\n",
    "        arguments = [\n",
    "            '--field-y', field_y,\n",
    "            '--field-z', field_z,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'output_model_uri': '/schema.txt',\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type name GCSPaths is different from expected: GCSPath\n",
      "Component \"task_factory_b\" is expecting field_z to be type({'GCSPath': {'path_type': 'file', 'file_type': 'tsv'}}), but the passed argument is type({'GCSPaths': {'path_type': 'file', 'file_type': 'tsv'}})\n"
     ]
    }
   ],
   "source": [
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check_d',\n",
    "    description='')\n",
    "def pipeline_d():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "try:\n",
    "    compiler.Compiler().compile(pipeline_d, 'pipeline_d.tar.gz', type_check=True)\n",
    "except InconsistentTypeException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check with missing type information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author components(with missing types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl._component import component\n",
    "from kfp.dsl._types import Integer, GCSPath\n",
    "from kfp.dsl import ContainerOp\n",
    "@component\n",
    "def task_factory_a(field_l: Integer()) -> {'field_m': {'GCSPath': {'path_type': 'file', 'file_type':'tsv'}}, \n",
    "                                           'field_o': 'Integer'\n",
    "                                          }:\n",
    "    return ContainerOp(\n",
    "        name = 'operator a',\n",
    "        image = 'gcr.io/ml-pipeline/component-a',\n",
    "        arguments = [\n",
    "            '--field-l', field_l,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'field_m': '/schema.txt',\n",
    "            'field_n': '/feature.txt',\n",
    "            'field_o': '/output.txt'\n",
    "        }\n",
    "    )\n",
    "\n",
    "@component\n",
    "def task_factory_b(field_x: {'customized_type': {'property_a': 'value_a', 'property_b': 'value_b'}},\n",
    "        field_y,\n",
    "        field_z: GCSPath(path_type='file', file_type='tsv')) -> {'output_model_uri': 'GcsUri'}:\n",
    "    return ContainerOp(\n",
    "        name = 'operator b',\n",
    "        image = 'gcr.io/ml-pipeline/component-a',\n",
    "        command = [\n",
    "            'python3',\n",
    "            field_x,\n",
    "        ],\n",
    "        arguments = [\n",
    "            '--field-y', field_y,\n",
    "            '--field-z', field_z,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'output_model_uri': '/schema.txt',\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author a pipeline with the above components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check_e',\n",
    "    description='')\n",
    "def pipeline_e():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(field_x=a.outputs['field_n'], field_y=a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_e, 'pipeline_e.tar.gz', type_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check with both named arguments and positional arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the component as part of the pipeline\n",
    "@dsl.pipeline(name='type_check_f',\n",
    "    description='')\n",
    "def pipeline_f():\n",
    "    a = task_factory_a(field_l=12)\n",
    "    b = task_factory_b(a.outputs['field_n'], a.outputs['field_o'], field_z=a.outputs['field_m'])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_f, 'pipeline_f.tar.gz', type_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Check between pipeline parameters and component parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCSPath has a property file_type with value: csv and tsv\n",
      "Component \"task_factory_a\" is expecting field_m to be type({'GCSPath': {'path_type': 'file', 'file_type': 'tsv'}}), but the passed argument is type({'GCSPath': {'path_type': 'file', 'file_type': 'csv'}})\n"
     ]
    }
   ],
   "source": [
    "@component\n",
    "def task_factory_a(field_m: {'GCSPath': {'path_type': 'file', 'file_type':'tsv'}}, field_o: 'Integer'):\n",
    "    return ContainerOp(\n",
    "        name = 'operator a',\n",
    "        image = 'gcr.io/ml-pipeline/component-b',\n",
    "        arguments = [\n",
    "            '--field-l', field_m,\n",
    "            '--field-o', field_o,\n",
    "        ],\n",
    "    )\n",
    "@dsl.pipeline(name='type_check_g',\n",
    "    description='')\n",
    "def pipeline_g(a: {'GCSPath': {'path_type':'file', 'file_type': 'csv'}}='good', b: Integer()=12):\n",
    "    task_factory_a(field_m=a, field_o=b)\n",
    "\n",
    "try:\n",
    "    compiler.Compiler().compile(pipeline_g, 'pipeline_g.tar.gz', type_check=True)\n",
    "except InconsistentTypeException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "for p in Path(\".\").glob(\"pipeline_[a-g].tar.gz\"):\n",
    "    p.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
