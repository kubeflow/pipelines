{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Image Captioning with Attention in Tensorflow 2.0 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook modifies an example Tensorflow 2.0 notebook from\n",
    "[here](https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/image_captioning.ipynb)\n",
    "to work with kubeflow pipelines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Download dataset and upload to GCS </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to download the [MS COCO dataset](http://cocodataset.org/#download).  This sample uses both the 2014 train images and 2014 train/val annotations.  If you downloaded and extracted the dataset on your local system, you can upload it to GCS using `gsutil -m cp -r path/to/dataset/ gs://[YOUR-BUCKET-NAME]/ms-coco`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Setup project info and imports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously downloaded dataset and put onto GCS\n",
    "GCS_DATASET_PATH = 'gs://[YOUR-BUCKET-NAME]/ms-coco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow project settings\n",
    "EXPERIMENT_NAME = 'Image Captioning'\n",
    "PROJECT_NAME = '[YOUR-PROJECT-NAME]' \n",
    "PIPELINE_STORAGE_PATH = 'gs://[YOUR-BUCKET-NAME]/ms-coco/components' # path to save pipeline component images\n",
    "BASE_IMAGE = 'tensorflow/tensorflow:2.0.0b0-py3' # using tensorflow 2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to save tensorboard files to a different directory each run\n",
    "RUN_NUMBER = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import compiler\n",
    "from kfp.gcp import use_gcp_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create pipeline components </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Data preprocessing component </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='img_data_preprocessing',\n",
    "    description='preprocesses images with inceptionV3',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def preprocess(dataset_path: str, num_examples: int, OUTPUT_DIR: str, \n",
    "        batch_size: int) -> str:\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    if OUTPUT_DIR == 'default':\n",
    "        OUTPUT_DIR = dataset_path + '/preprocess/'\n",
    "    \n",
    "    annotation_file = dataset_path + '/annotations_trainval2014/annotations/captions_train2014.json'\n",
    "    PATH = dataset_path + '/train2014/train2014/'\n",
    "    \n",
    "    # Read the json file (CHANGE open() TO file_io.FileIO to use GCS)\n",
    "    with file_io.FileIO(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Store captions and image names in vectors\n",
    "    all_captions = []\n",
    "    all_img_name_vector = []\n",
    "\n",
    "    for annot in annotations['annotations']:\n",
    "        caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "        image_id = annot['image_id']\n",
    "        full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "        all_img_name_vector.append(full_coco_image_path)\n",
    "        all_captions.append(caption)\n",
    "\n",
    "    # Shuffle captions and image_names together\n",
    "    train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                              all_img_name_vector,\n",
    "                                              random_state=1)\n",
    "\n",
    "    # Select the first num_examples captions/imgs from the shuffled set\n",
    "    train_captions = train_captions[:num_examples]\n",
    "    img_name_vector = img_name_vector[:num_examples]\n",
    "    \n",
    "    # Preprocess the images before feeding into inceptionV3\n",
    "    def load_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path\n",
    "    \n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "    \n",
    "    # Save extracted features in GCS\n",
    "    # Get unique images\n",
    "    encode_train = sorted(set(img_name_vector))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "    image_dataset = image_dataset.map(\n",
    "        load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "    \n",
    "    for img, path in image_dataset:\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            \n",
    "            # Save to a different location and as numpy array\n",
    "            path_of_feature = path_of_feature.replace('.jpg', '.npy')\n",
    "            path_of_feature = path_of_feature.replace(PATH, OUTPUT_DIR)\n",
    "            np.save(file_io.FileIO(path_of_feature, 'w'), bf.numpy())\n",
    "    \n",
    "    # Create array for locations of preprocessed images\n",
    "    preprocessed_imgs = [img.replace('.jpg', '.npy') for img in img_name_vector]\n",
    "    preprocessed_imgs = [img.replace(PATH, OUTPUT_DIR) for img in preprocessed_imgs]\n",
    "    \n",
    "    # Save train_captions and preprocessed_imgs to file\n",
    "    train_cap_path = OUTPUT_DIR + 'train_captions.npy' # array of captions\n",
    "    preprocessed_imgs_path = OUTPUT_DIR + 'preprocessed_imgs.py'# array of paths to preprocessed images\n",
    "    \n",
    "    train_captions = np.array(train_captions)\n",
    "    np.save(file_io.FileIO(train_cap_path, 'w'), train_captions)\n",
    "    \n",
    "    preprocessed_imgs = np.array(preprocessed_imgs)\n",
    "    np.save(file_io.FileIO(preprocessed_imgs_path, 'w'), preprocessed_imgs)\n",
    "    \n",
    "    return (train_cap_path, preprocessed_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 17:32:56:INFO:Build an image that is based on tensorflow/tensorflow:2.0.0b0-py3 and push the image to gcr.io/intro-to-kubeflow-1/preprocessing:latest\n",
      "2019-07-10 17:32:56:INFO:Checking path: gs://artifacts.intro-to-kubeflow-1.appspot.com/ms-coco/components...\n",
      "2019-07-10 17:32:56:INFO:Generate entrypoint and serialization codes.\n",
      "2019-07-10 17:32:56:INFO:Generate build files.\n",
      "2019-07-10 17:32:56:INFO:Start a kaniko job for build.\n",
      "2019-07-10 17:32:56:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-07-10 17:32:56:INFO:Initialized with in-cluster config.\n",
      "2019-07-10 17:33:01:INFO:5 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:06:INFO:10 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:11:INFO:15 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:16:INFO:20 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:22:INFO:25 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:27:INFO:30 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:32:INFO:35 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:37:INFO:40 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:42:INFO:45 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:47:INFO:50 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:52:INFO:55 seconds: waiting for job to complete\n",
      "2019-07-10 17:33:52:INFO:Kaniko job complete.\n",
      "2019-07-10 17:33:52:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "TARGET_IMAGE = 'gcr.io/%s/preprocessing:latest' % PROJECT_NAME\n",
    "preprocessing_img_op = compiler.build_python_component(\n",
    "    component_func=preprocess,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='scikit-learn', version='0.21.2')],\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Tokenizing component </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='tokenize_captions',\n",
    "    description='Tokenize captions to create training data',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def tokenize_captions(dataset_path: str, preprocess_output: str, OUTPUT_DIR: str,\n",
    "        top_k: int) -> str:\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from io import BytesIO\n",
    "    from ast import literal_eval as make_tuple\n",
    "    \n",
    "    \n",
    "    # Convert output from string to tuple and unpack\n",
    "    preprocess_output = make_tuple(preprocess_output)\n",
    "    train_caption_path = preprocess_output[0]\n",
    "    \n",
    "    if OUTPUT_DIR == 'default':\n",
    "        OUTPUT_DIR = dataset_path + '/tokenize/'\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "    f = BytesIO(file_io.read_file_to_string(train_caption_path, \n",
    "                                            binary_mode=True))\n",
    "    train_captions = np.load(f)\n",
    "    \n",
    "    # Tokenize captions\n",
    "    tokenizer.fit_on_texts(train_captions)\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    \n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "    \n",
    "    # Find the maximum length of any caption in our dataset\n",
    "    def calc_max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "    \n",
    "    max_length = calc_max_length(train_seqs)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_file_path = OUTPUT_DIR + 'tokenizer.pickle'\n",
    "    with file_io.FileIO(tokenizer_file_path, 'wb') as output:\n",
    "        pickle.dump(tokenizer, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Save train_seqs\n",
    "    cap_vector_file_path = OUTPUT_DIR + 'cap_vector.npy'\n",
    "    np.save(file_io.FileIO(cap_vector_file_path, 'w'), cap_vector)\n",
    "    \n",
    "    \n",
    "    return str(max_length), tokenizer_file_path, cap_vector_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 17:33:52:INFO:Build an image that is based on tensorflow/tensorflow:2.0.0b0-py3 and push the image to gcr.io/intro-to-kubeflow-1/tokenizer:latest\n",
      "2019-07-10 17:33:52:INFO:Checking path: gs://artifacts.intro-to-kubeflow-1.appspot.com/ms-coco/components...\n",
      "2019-07-10 17:33:52:INFO:Generate entrypoint and serialization codes.\n",
      "2019-07-10 17:33:52:INFO:Generate build files.\n",
      "2019-07-10 17:33:52:INFO:Start a kaniko job for build.\n",
      "2019-07-10 17:33:52:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-07-10 17:33:52:INFO:Initialized with in-cluster config.\n",
      "2019-07-10 17:33:57:INFO:5 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:02:INFO:10 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:07:INFO:15 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:12:INFO:20 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:17:INFO:25 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:22:INFO:30 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:27:INFO:35 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:32:INFO:40 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:37:INFO:45 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:42:INFO:50 seconds: waiting for job to complete\n",
      "2019-07-10 17:34:42:INFO:Kaniko job complete.\n",
      "2019-07-10 17:34:43:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "TARGET_IMAGE = 'gcr.io/%s/tokenizer:latest' % PROJECT_NAME\n",
    "tokenize_captions_op = compiler.build_python_component(\n",
    "    component_func=tokenize_captions,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Component for training model (and saving it)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='model_training',\n",
    "    description='Trains image captioning model',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def train_model(dataset_path: str, preprocess_output: str, \n",
    "        tokenizing_output: str, train_output_dir: str, valid_output_dir: str, \n",
    "        batch_size: int, embedding_dim: int, units: int, EPOCHS: int, run_number : int)-> str:\n",
    "    import json\n",
    "    import time\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from io import BytesIO\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from ast import literal_eval as make_tuple\n",
    "    \n",
    "    # Convert output from string to tuple and unpack\n",
    "    preprocess_output = make_tuple(preprocess_output)\n",
    "    tokenizing_output = make_tuple(tokenizing_output)\n",
    "    \n",
    "    # Unpack tuples\n",
    "    preprocessed_imgs_path = preprocess_output[1]\n",
    "    \n",
    "    tokenizer_path = tokenizing_output[1]\n",
    "    cap_vector_file_path = tokenizing_output[2]\n",
    "    \n",
    "    if valid_output_dir == 'default':\n",
    "        valid_output_dir = dataset_path + '/valid/'\n",
    "    \n",
    "    if train_output_dir == 'default':\n",
    "        train_output_dir = dataset_path + '/train/'\n",
    "    \n",
    "    # load img_name_vector\n",
    "    f = BytesIO(file_io.read_file_to_string(preprocessed_imgs_path, binary_mode=True))\n",
    "    img_name_vector = np.load(f)\n",
    "    \n",
    "    # Load cap_vector\n",
    "    f = BytesIO(file_io.read_file_to_string(cap_vector_file_path, binary_mode=True))\n",
    "    cap_vector = np.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with file_io.FileIO(tokenizer_path, 'rb') as src:\n",
    "        tokenizer = pickle.load(src)\n",
    "    \n",
    "    # Split data into training and testing\n",
    "    img_name_train, img_name_val, cap_train, cap_val = train_test_split(\n",
    "                                                            img_name_vector,\n",
    "                                                            cap_vector,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=0)\n",
    "    \n",
    "    # Create tf.data dataset for training\n",
    "    BUFFER_SIZE = 1000 # common size used for shuffling dataset\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    num_steps = len(img_name_train) // batch_size\n",
    "    \n",
    "    # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "    features_shape = 2048\n",
    "    \n",
    "    # Load the numpy files\n",
    "    def map_func(img_name, cap):\n",
    "        f = BytesIO(file_io.read_file_to_string(img_name.decode('utf-8'), binary_mode=True))\n",
    "        img_tensor = np.load(f)\n",
    "        return img_tensor, cap\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "    # Use map to load the numpy files in parallel\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "              map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Shuffle and batch\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Create models\n",
    "    # Attention model\n",
    "    class BahdanauAttention(tf.keras.Model):\n",
    "        def __init__(self, units):\n",
    "            super(BahdanauAttention, self).__init__()\n",
    "            self.W1 = tf.keras.layers.Dense(units)\n",
    "            self.W2 = tf.keras.layers.Dense(units)\n",
    "            self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "        def call(self, features, hidden):\n",
    "            # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "            # hidden shape == (batch_size, hidden_size)\n",
    "            # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "            hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "            # score shape == (batch_size, 64, hidden_size)\n",
    "            score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "            # attention_weights shape == (batch_size, 64, 1)\n",
    "            # you get 1 at the last axis because you are applying score to self.V\n",
    "            attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "            # context_vector shape after sum == (batch_size, hidden_size)\n",
    "            context_vector = attention_weights * features\n",
    "            context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "            return context_vector, attention_weights\n",
    "    \n",
    "    # CNN Encoder model\n",
    "    class CNN_Encoder(tf.keras.Model):\n",
    "        # Since you have already extracted the features and dumped it using pickle\n",
    "        # This encoder passes those features through a Fully connected layer\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(CNN_Encoder, self).__init__()\n",
    "            # shape after fc == (batch_size, 64, embedding_dim)\n",
    "            self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "        def call(self, x):\n",
    "            x = self.fc(x)\n",
    "            x = tf.nn.relu(x)\n",
    "            return x\n",
    "    \n",
    "    # RNN Decoder model\n",
    "    class RNN_Decoder(tf.keras.Model):\n",
    "        def __init__(self, embedding_dim, units, vocab_size):\n",
    "            super(RNN_Decoder, self).__init__()\n",
    "            self.units = units\n",
    "\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "            self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer='glorot_uniform')\n",
    "            self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "            self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "            self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "        def call(self, x, features, hidden):\n",
    "            # defining attention as a separate model\n",
    "            context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "            # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "            x = self.embedding(x)\n",
    "\n",
    "            # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "            # passing the concatenated vector to the GRU\n",
    "            output, state = self.gru(x)\n",
    "\n",
    "            # shape == (batch_size, max_length, hidden_size)\n",
    "            x = self.fc1(output)\n",
    "\n",
    "            # x shape == (batch_size * max_length, hidden_size)\n",
    "            x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "            # output shape == (batch_size * max_length, vocab)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "            return x, state, attention_weights\n",
    "\n",
    "        def reset_state(self, batch_size):\n",
    "            return tf.zeros((batch_size, self.units))\n",
    "        \n",
    "    encoder = CNN_Encoder(embedding_dim)\n",
    "    decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    \n",
    "    # Create loss function\n",
    "    def loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = loss_object(real, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "\n",
    "        return tf.reduce_mean(loss_)\n",
    "    \n",
    "    # Create check point for training model\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, train_output_dir + 'checkpoints/', max_to_keep=5)\n",
    "    start_epoch = 0\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "            \n",
    "    # Create training step\n",
    "    loss_plot = []\n",
    "    @tf.function\n",
    "    def train_step(img_tensor, target):\n",
    "        loss = 0\n",
    "\n",
    "        # initializing the hidden state for each batch\n",
    "        # because the captions are not related from image to image\n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = encoder(img_tensor)\n",
    "\n",
    "            for i in range(1, target.shape[1]):\n",
    "                # passing the features through the decoder\n",
    "                predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "                loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "    \n",
    "    # Create summary writers and loss for plotting loss in tensorboard\n",
    "    tensorboard_dir = train_output_dir + 'logs' + str(run_number) + '/'\n",
    "    train_summary_writer = tf.summary.create_file_writer(tensorboard_dir)\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    \n",
    "    # Train model\n",
    "    path_to_most_recent_ckpt = None\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss\n",
    "            train_loss(t_loss)\n",
    "            if batch % 100 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Storing the epoch end loss value to plot in tensorboard\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss per epoch', train_loss.result(), step=epoch)\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            path_to_most_recent_ckpt = ckpt_manager.save()\n",
    "\n",
    "        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                             total_loss/num_steps))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    # Add plot of loss in tensorboard\n",
    "    metadata ={\n",
    "        'outputs': [{\n",
    "            'type': 'tensorboard',\n",
    "            'source': tensorboard_dir,\n",
    "        }]\n",
    "    }\n",
    "    with open('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    # Save validation data to use for predictions\n",
    "    val_cap_path = valid_output_dir + 'captions.npy'\n",
    "    np.save(file_io.FileIO(val_cap_path, 'w'), cap_val)\n",
    "    \n",
    "    val_img_path = valid_output_dir + 'images.npy'\n",
    "    np.save(file_io.FileIO(val_img_path, 'w'), img_name_val)\n",
    "    \n",
    "    return path_to_most_recent_ckpt, val_cap_path, val_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 19:23:41:INFO:Build an image that is based on tensorflow/tensorflow:2.0.0b0-py3 and push the image to gcr.io/intro-to-kubeflow-1/trainer:latest\n",
      "2019-07-10 19:23:41:INFO:Checking path: gs://artifacts.intro-to-kubeflow-1.appspot.com/ms-coco/components...\n",
      "2019-07-10 19:23:41:INFO:Generate entrypoint and serialization codes.\n",
      "2019-07-10 19:23:41:INFO:Generate build files.\n",
      "2019-07-10 19:23:41:INFO:Start a kaniko job for build.\n",
      "2019-07-10 19:23:41:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-07-10 19:23:41:INFO:Initialized with in-cluster config.\n",
      "2019-07-10 19:23:46:INFO:5 seconds: waiting for job to complete\n",
      "2019-07-10 19:23:51:INFO:10 seconds: waiting for job to complete\n",
      "2019-07-10 19:23:56:INFO:15 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:01:INFO:20 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:06:INFO:25 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:11:INFO:30 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:16:INFO:35 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:21:INFO:40 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:26:INFO:45 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:31:INFO:50 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:36:INFO:55 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:36:INFO:Kaniko job complete.\n",
      "2019-07-10 19:24:37:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "TARGET_IMAGE = 'gcr.io/%s/trainer:latest' % PROJECT_NAME\n",
    "model_train_op = compiler.build_python_component(\n",
    "    component_func=train_model,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='scikit-learn', version='0.21.2')],\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Component for model prediction </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='model_predictions',\n",
    "    description='Predicts on images in validation set',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def predict(dataset_path: str, tokenizing_output: str, \n",
    "        model_train_output: str, preprocess_output_dir: str, \n",
    "        valid_output_dir: str, embedding_dim: int, units: int,\n",
    "        run_number: int):\n",
    "    import pickle\n",
    "    import json\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from math import ceil \n",
    "    from io import BytesIO\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from ast import literal_eval as make_tuple\n",
    "    \n",
    "    tokenizing_output = make_tuple(tokenizing_output)\n",
    "    model_train_output = make_tuple(model_train_output)\n",
    "    \n",
    "    # Unpack tuples\n",
    "    max_length = int(tokenizing_output[0])\n",
    "    tokenizer_path = tokenizing_output[1]\n",
    "    model_path = model_train_output[0]\n",
    "    val_cap_path = model_train_output[1]\n",
    "    val_img_path = model_train_output[2]\n",
    "    \n",
    "    if preprocess_output_dir == 'default':\n",
    "        preprocess_output_dir = dataset_path + '/preprocess/'\n",
    "    \n",
    "    if valid_output_dir == 'default':\n",
    "        valid_output_dir = dataset_path + '/valid/'\n",
    "        \n",
    "    tensorboard_dir = valid_output_dir + 'logs' + str(run_number) + '/'\n",
    "    summary_writer = tf.summary.create_file_writer(tensorboard_dir)\n",
    "\n",
    "    # Load tokenizer, model, test_captions, and test_imgs\n",
    "    \"\"\" CHANGE: don't reuse code here: not sure how though..? \"\"\"\n",
    "    # Attention model\n",
    "    class BahdanauAttention(tf.keras.Model):\n",
    "        def __init__(self, units):\n",
    "            super(BahdanauAttention, self).__init__()\n",
    "            self.W1 = tf.keras.layers.Dense(units)\n",
    "            self.W2 = tf.keras.layers.Dense(units)\n",
    "            self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "        def call(self, features, hidden):\n",
    "            # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "            # hidden shape == (batch_size, hidden_size)\n",
    "            # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "            hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "            # score shape == (batch_size, 64, hidden_size)\n",
    "            score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "            # attention_weights shape == (batch_size, 64, 1)\n",
    "            # you get 1 at the last axis because you are applying score to self.V\n",
    "            attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "            # context_vector shape after sum == (batch_size, hidden_size)\n",
    "            context_vector = attention_weights * features\n",
    "            context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "            return context_vector, attention_weights\n",
    "    \n",
    "    # CNN Encoder model\n",
    "    class CNN_Encoder(tf.keras.Model):\n",
    "        # Since you have already extracted the features and dumped it using pickle\n",
    "        # This encoder passes those features through a Fully connected layer\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(CNN_Encoder, self).__init__()\n",
    "            # shape after fc == (batch_size, 64, embedding_dim)\n",
    "            self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "        def call(self, x):\n",
    "            x = self.fc(x)\n",
    "            x = tf.nn.relu(x)\n",
    "            return x\n",
    "    \n",
    "    # RNN Decoder model\n",
    "    class RNN_Decoder(tf.keras.Model):\n",
    "        def __init__(self, embedding_dim, units, vocab_size):\n",
    "            super(RNN_Decoder, self).__init__()\n",
    "            self.units = units\n",
    "\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "            self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer='glorot_uniform')\n",
    "            self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "            self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "            self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "        def call(self, x, features, hidden):\n",
    "            # defining attention as a separate model\n",
    "            context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "            # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "            x = self.embedding(x)\n",
    "\n",
    "            # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "            # passing the concatenated vector to the GRU\n",
    "            output, state = self.gru(x)\n",
    "\n",
    "            # shape == (batch_size, max_length, hidden_size)\n",
    "            x = self.fc1(output)\n",
    "\n",
    "            # x shape == (batch_size * max_length, hidden_size)\n",
    "            x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "            # output shape == (batch_size * max_length, vocab)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "            return x, state, attention_weights\n",
    "\n",
    "        def reset_state(self, batch_size):\n",
    "            return tf.zeros((batch_size, self.units))\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with file_io.FileIO(tokenizer_path, 'rb') as src:\n",
    "        tokenizer = pickle.load(src)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "    attention_features_shape = 64\n",
    "    features_shape = 2048\n",
    "    \n",
    "    encoder = CNN_Encoder(embedding_dim)\n",
    "    decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "    \n",
    "    # Load model from checkpoint (encoder, decoder)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder, optimizer=optimizer)\n",
    "    ckpt.restore(model_path).expect_partial()\n",
    "    \n",
    "    # Load test captions\n",
    "    f = BytesIO(file_io.read_file_to_string(val_cap_path, \n",
    "                                            binary_mode=True))\n",
    "    cap_val = np.load(f)\n",
    "    \n",
    "    # load test images\n",
    "    f = BytesIO(file_io.read_file_to_string(val_img_path, \n",
    "                                            binary_mode=True))\n",
    "    img_name_val = np.load(f)\n",
    "    \n",
    "    # To get original image locations, replace .npy extension with .jpg and \n",
    "    # replace preprocessed path with path original images\n",
    "    PATH = dataset_path + '/train2014/train2014/'\n",
    "    img_name_val = [img.replace('.npy', '.jpg') for img in img_name_val]\n",
    "    img_name_val = [img.replace(preprocess_output_dir, PATH) for img in img_name_val]\n",
    "    \n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "    \n",
    "    # Preprocess the images using InceptionV3\n",
    "    def load_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path\n",
    "    \n",
    "    # Run predictions\n",
    "    def evaluate(image):\n",
    "        attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "        hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "        temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "        img_tensor_val = image_features_extract_model(temp_input)\n",
    "        img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "        features = encoder(img_tensor_val)\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "        result = []\n",
    "\n",
    "        for i in range(max_length):\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "            if tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result, attention_plot\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        attention_plot = attention_plot[:len(result), :]\n",
    "        return result, attention_plot\n",
    "    \n",
    "    # Modified to plot images on tensorboard\n",
    "    def plot_attention(image, result, attention_plot):\n",
    "        img = tf.io.read_file(image)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        temp_image = np.array(img.numpy())\n",
    "        \n",
    "        len_result = len(result)\n",
    "        for l in range(len_result):\n",
    "            temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "            plt.title(result[l])\n",
    "            img = plt.imshow(temp_image)\n",
    "            plt.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "            \n",
    "            # Save plt to image to access in tensorboard\n",
    "            buf = BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            \n",
    "            final_im = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "            final_im = tf.expand_dims(final_im, 0)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.image(\"attention\", final_im, step=l)\n",
    "    \n",
    "    # Select a random image to caption from validation set\n",
    "    rid = np.random.randint(0, len(img_name_val))\n",
    "    image = img_name_val[rid]\n",
    "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "    result, attention_plot = evaluate(image)\n",
    "    print ('Image:', image)\n",
    "    print ('Real Caption:', real_caption)\n",
    "    print ('Prediction Caption:', ' '.join(result))\n",
    "    plot_attention(image, result, attention_plot)\n",
    "    \n",
    "    # Plot attention images on tensorboard\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'tensorboard',\n",
    "            'source': tensorboard_dir,\n",
    "        }]\n",
    "    }\n",
    "    with open('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 19:24:37:INFO:Build an image that is based on tensorflow/tensorflow:2.0.0b0-py3 and push the image to gcr.io/intro-to-kubeflow-1/predict:latest\n",
      "2019-07-10 19:24:37:INFO:Checking path: gs://artifacts.intro-to-kubeflow-1.appspot.com/ms-coco/components...\n",
      "2019-07-10 19:24:37:INFO:Generate entrypoint and serialization codes.\n",
      "2019-07-10 19:24:37:INFO:Generate build files.\n",
      "2019-07-10 19:24:37:INFO:Start a kaniko job for build.\n",
      "2019-07-10 19:24:37:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-07-10 19:24:37:INFO:Initialized with in-cluster config.\n",
      "2019-07-10 19:24:42:INFO:5 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:47:INFO:10 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:52:INFO:15 seconds: waiting for job to complete\n",
      "2019-07-10 19:24:57:INFO:20 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:02:INFO:25 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:07:INFO:30 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:12:INFO:35 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:17:INFO:40 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:22:INFO:45 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:27:INFO:50 seconds: waiting for job to complete\n",
      "2019-07-10 19:25:27:INFO:Kaniko job complete.\n",
      "2019-07-10 19:25:27:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "TARGET_IMAGE = 'gcr.io/%s/predict:latest' % PROJECT_NAME\n",
    "predict_op = compiler.build_python_component(\n",
    "    component_func=predict,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='matplotlib', version='3.1.0')],\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create and run pipeline </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Create pipeline </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Image Captioning Pipeline',\n",
    "    description='A pipeline that trains a model to caption images'\n",
    ")\n",
    "def caption_pipeline(\n",
    "dataset_path=GCS_DATASET_PATH,\n",
    "num_examples=30000,\n",
    "epochs=20,\n",
    "training_batch_size=64,\n",
    "hidden_state_size=512,\n",
    "vocab_size=5000,\n",
    "embedding_dim=256,\n",
    "preprocessing_batch_size=16,\n",
    "preprocessing_output_dir='default',\n",
    "tokenizing_output_dir='default',\n",
    "training_output_dir='default',\n",
    "validation_output_dir='default',\n",
    "run_number=0,\n",
    "): \n",
    "    \n",
    "    preprocessing_img_task = preprocessing_img_op(\n",
    "        dataset_path, \n",
    "        output_dir=preprocessing_output_dir,\n",
    "        batch_size=preprocessing_batch_size, \n",
    "        num_examples=num_examples).apply(\n",
    "        use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    tokenize_captions_task = tokenize_captions_op(\n",
    "        dataset_path, \n",
    "        preprocessing_img_task.output, \n",
    "        output_dir=tokenizing_output_dir, \n",
    "        top_k=vocab_size).apply(use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    model_train_task = model_train_op(\n",
    "        dataset_path, \n",
    "        preprocessing_img_task.output,\n",
    "        tokenize_captions_task.output,\n",
    "        train_output_dir=training_output_dir, \n",
    "        valid_output_dir=validation_output_dir,\n",
    "        batch_size=training_batch_size, \n",
    "        embedding_dim=embedding_dim, \n",
    "        units=hidden_state_size, \n",
    "        epochs=epochs,\n",
    "        run_number=run_number).apply(\n",
    "        use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    predict_task = predict_op(\n",
    "        dataset_path,\n",
    "        tokenize_captions_task.output, \n",
    "        model_train_task.output,\n",
    "        preprocess_output_dir=preprocessing_output_dir,\n",
    "        valid_output_dir=validation_output_dir,\n",
    "        embedding_dim=embedding_dim,\n",
    "        units=hidden_state_size,\n",
    "        run_number=run_number).apply(\n",
    "        use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = caption_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/04501b0c-f82d-457c-97b8-a4a9fdf3ff1a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Run pipeline </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/93bcf3ec-a348-11e9-ac63-42010a80017c\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test run to make sure all parts of the pipeline are working properly\n",
    "arguments = {\n",
    "    'dataset_path': GCS_DATASET_PATH, \n",
    "    'num_examples': 100, # Small test to make sure pipeline functions properly\n",
    "    'training_batch_size': 16, # has to be smaller since only training on 80 examples \n",
    "    'run_number': RUN_NUMBER,\n",
    "}\n",
    "run_name = pipeline_func.__name__ + ' run' + str(RUN_NUMBER)\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename,\n",
    "                                params=arguments)\n",
    "RUN_NUMBER += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model checkpoints are saved at training_output_dir, which is `GCS_DATASET_PATH/train/checkpoints/` by default."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
