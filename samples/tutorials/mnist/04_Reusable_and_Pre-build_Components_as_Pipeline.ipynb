{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable components and Pre-build components\n",
    "\n",
    "This tutorial describes the manual way of writing a full component program (in any language) and a component definition for it. Below is a summary of the steps involved in creating and using a component:\n",
    "\n",
    "- Write the program that contains your componentâ€™s logic. The program must use files and command-line arguments to pass data to and from the component.\n",
    "- Containerize the program.\n",
    "- Write a component specification in YAML format that describes the component for the Kubeflow Pipelines system.\n",
    "- Use the Kubeflow Pipelines SDK to load your component, use it in a pipeline and run that pipeline.\n",
    "\n",
    "More over, we will combine our built components together with a pre-build components and a lightweight component to compose a pipeline with three steps\n",
    "- Train a minist model and export to GCS\n",
    "- Deploy the exported tensorflow model on AI Platform\n",
    "- Test the deployment by calling the end point\n",
    "\n",
    "**Note: Make sure that you have docker installed in the local environment if you want to build the image locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameter"
    ]
   },
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='<ADD GCP PROJECT HERE>'\n",
    "GCS_BUCKET='gs://<ADD STORAGE LOCATION HERE>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameter"
    ]
   },
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside kubeflow cluster\n",
    "HOST = '<ADD HOST NAME TO TALK TO KUBEFLOW PIPELINE HERE>'\n",
    "CLIENT_ID = '<ADD OAuth CLIENT ID USED BY IAP HERE>'\n",
    "OTHER_CLIENT_ID = '<ADD OAuth CLIENT ID USED TO OBTAIN AUTH CODES HERE>'\n",
    "OTHER_CLIENT_SECRET = '<ADD OAuth CLIENT SECRET USED TO OBTAIN AUTH CODES HERE>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "**If submit outside the kubeflow cluster, need the following**\n",
    "- `host`: the host name to use to talk to Kubeflow Pipelines, i.e., \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "- For getting `other_client_id` and `other_client_secret`, you'll need to create OAuth client ID credentials of type `Other` according to the tutorial [here](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "**If you run and submit within the kubeflow cluster**, the following is enough\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build reusable components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `app.py` that contains a Python script. The script takes a GCS bucket name as an input argument, gets the lists of blobs in that bucket, prints the list of blobs and also writes them to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/reuse_components_pipeline/minist_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/reuse_components_pipeline/minist_training/app.py <<HERE\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--model_path', type=str, required=True, help='Name of the model file.')\n",
    "parser.add_argument(\n",
    "    '--bucket', type=str, required=True, help='GCS bucket name.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "bucket=args.bucket\n",
    "model_path=args.model_path\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())    \n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=bucket + '/logs/' + datetime.now().date().__str__()),\n",
    "  # Interrupt training if val_loss stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "from tensorflow import gfile\n",
    "\n",
    "gcs_path = bucket + \"/\" + model_path\n",
    "# The export require the folder is new\n",
    "if gfile.Exists(gcs_path):\n",
    "    gfile.DeleteRecursively(gcs_path)\n",
    "tf.keras.experimental.export_saved_model(model, gcs_path)\n",
    "\n",
    "with open('/output.txt', 'w') as f:\n",
    "  f.write(gcs_path)\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker container\n",
    "Create your own container image that includes your program. \n",
    "- If your component creates some outputs to be fed as inputs to the downstream components, each separate output must be written as a string to a separate local text file inside the container image. \n",
    "- For example, if a trainer component needs to output the trained model path, it can write the path to a local file `/output.txt`. \n",
    "- The string written to an output file cannot be too big. If it is too big (>> 100 kB), it is recommended to save the output to an external persistent storage and pass the storage path to the next component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a container that runs the script. Start by creating a `Dockerfile`. A `Dockerfile` contains the instructions to assemble a Docker image. The `FROM` statement specifies the Base Image from which you are building. `WORKDIR` sets the working directory. When you assemble the Docker image, `COPY` will copy the required files and directories (for example, `app.py`) to the filesystem of the container. `RUN` will execute a command (for example, install the dependencies) and commits the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Dockerfile.\n",
    "# AI platform only support tensorflow 1.14\n",
    "cat > ./tmp/reuse_components_pipeline/minist_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our Dockerfile we can create our Docker image. Then we need to push the image to a registry to host the image. \n",
    "- We are going to use the `kfp.containers.build_image_from_working_dir` to build the image and push to the Google Container Registry (GCR), which makes use of [kaniko](https://cloud.google.com/blog/products/gcp/introducing-kaniko-build-container-images-in-kubernetes-and-google-container-builder-even-without-root-access).\n",
    "- It is definitely possible to build the image using Docker and push to GCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "If you run the following code from a notebook **within kubeflow cluster** and **with kubeflow version >= 0.7**, you need to make sure that there is valid credential under your notebook's namespace, since the namespace of the notebook server is no long `kubeflow`. \n",
    "- With kubeflow version >= 0.7, the credentail is supposed to be copied automatically while creating notebook through `Configurations`, which doesn't work properly at the time of creating this notebook. \n",
    "- You can also add credentials to the new namespace by either copying them from an existing Kubeflow namespace or by creating a new service account as explained [here](https://www.kubeflow.org/docs/gke/authentication/#kubeflow-v0-6-and-before-gcp-service-account-key-as-secret).\n",
    "- The following cell demonstrate how to copy the default secret to your own namespace.\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "\n",
    "NAMESPACE=<your notebook name space>\n",
    "SOURCE=kubeflow\n",
    "NAME=user-gcp-sa\n",
    "SECRET=$(kubectl get secrets \\${NAME} -n \\${SOURCE} -o jsonpath=\"{.data.\\${NAME}\\.json}\" | base64 -D)\n",
    "kubectl create -n \\${NAMESPACE} secret generic \\${NAME} --from-literal=\"\\${NAME}.json=\\${SECRET}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:d3e28ca79dcbd61149fedfdbe7aa25b4ccf8f802da66337b61e3e1630a99e045'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME=\"minist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "GCR_IMAGE=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "builder = kfp.containers._container_builder.ContainerBuilder(\n",
    "    gcs_staging=GCS_BUCKET + \"/kfp_container_build_staging\")\n",
    "\n",
    "image_name = kfp.containers.build_image_from_working_dir(\n",
    "    image_name=GCR_IMAGE,\n",
    "    working_dir='./tmp/reuse_components_pipeline/minist_training/',\n",
    "    builder=builder\n",
    ")\n",
    "\n",
    "image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to use docker to build the image\n",
    "Run the following in a cell\n",
    "```bash\n",
    "%%bash -s \"{PROJECT_ID}\"\n",
    "\n",
    "IMAGE_NAME=\"minist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "# Create script to build docker image and push it.\n",
    "cat > ./tmp/components/minist_training/build_image.sh <<HERE\n",
    "PROJECT_ID=\"${1}\"\n",
    "IMAGE_NAME=\"${IMAGE_NAME}\"\n",
    "TAG=\"${TAG}\"\n",
    "GCR_IMAGE=\"gcr.io/\\${PROJECT_ID}/\\${IMAGE_NAME}:\\${TAG}\"\n",
    "docker build -t \\${IMAGE_NAME} .\n",
    "docker tag \\${IMAGE_NAME} \\${GCR_IMAGE}\n",
    "docker push \\${GCR_IMAGE}\n",
    "docker image rm \\${IMAGE_NAME}\n",
    "docker image rm \\${GCR_IMAGE}\n",
    "HERE\n",
    "\n",
    "cd tmp/components/minist_training\n",
    "bash build_image.sh\n",
    "```\n",
    "\n",
    "**Remember to set the image_name after the image is built**\n",
    "```python\n",
    "image_name = <the image uri>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing your component definition file\n",
    "To create a component from your containerized program you need to write component specification in YAML format that describes the component for the Kubeflow Pipelines system.\n",
    "\n",
    "For the complete definition of a Kubeflow Pipelines component, see the [component specification](https://www.kubeflow.org/docs/pipelines/reference/component-spec/). However, for this tutorial you donâ€™t need to know the full schema of the component specification. The tutorial provides enough information for the relevant the components.\n",
    "\n",
    "Start writing the component definition (component.yaml) by specifying your container image in the componentâ€™s implementation section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:d3e28ca79dcbd61149fedfdbe7aa25b4ccf8f802da66337b61e3e1630a99e045\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"{image_name}\"\n",
    "\n",
    "GCR_IMAGE=\"${1}\"\n",
    "echo ${GCR_IMAGE}\n",
    "\n",
    "# Create Yaml\n",
    "# the image uri should be changed according to the above docker image push output\n",
    "\n",
    "cat > minist_pipeline_component.yaml <<HERE\n",
    "name: Minist training\n",
    "description: Train a minist model and save to GCS\n",
    "inputs:\n",
    "  - name: model_path\n",
    "    description: 'Path of the tf model.'\n",
    "    type: String\n",
    "  - name: bucket\n",
    "    description: 'GCS bucket name.'\n",
    "    type: String\n",
    "outputs:\n",
    "  - name: gcs_model_path\n",
    "    description: 'Trained model path.'\n",
    "    type: GCSPath\n",
    "implementation:\n",
    "  container:\n",
    "    image: ${GCR_IMAGE}\n",
    "    command: [\n",
    "      python, /app/app.py,\n",
    "      --model_path, {inputValue: model_path},\n",
    "      --bucket,     {inputValue: bucket},\n",
    "    ]\n",
    "    fileOutputs:\n",
    "      gcs_model_path: /output.txt\n",
    "HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "minist_train_op = kfp.components.load_component_from_file(os.path.join('./', 'minist_pipeline_component.yaml')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComponentSpec(name='Minist training', description='Train a minist model and save to GCS', metadata=None, inputs=[InputSpec(name='model_path', type='String', description='Path of the tf model.', default=None, optional=False), InputSpec(name='bucket', type='String', description='GCS bucket name.', default=None, optional=False)], outputs=[OutputSpec(name='gcs_model_path', type='GCSPath', description='Trained model path.')], implementation=ContainerImplementation(container=ContainerSpec(image='gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:d3e28ca79dcbd61149fedfdbe7aa25b4ccf8f802da66337b61e3e1630a99e045', command=['python', '/app/app.py', '--model_path', InputValuePlaceholder(input_name='model_path'), '--bucket', InputValuePlaceholder(input_name='bucket')], args=None, env=None, file_outputs={'gcs_model_path': '/output.txt'})), version='google.com/cloud/pipelines/component/v1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minist_train_op.component_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define deployment operation on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlengine_deploy_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/gcp/ml_engine/deploy/component.yaml')\n",
    "\n",
    "def deploy(\n",
    "    project_id,\n",
    "    model_uri,\n",
    "    model_id,\n",
    "    runtime_version,\n",
    "    python_version):\n",
    "    \n",
    "    return mlengine_deploy_op(\n",
    "        model_uri=model_uri,\n",
    "        project_id=project_id, \n",
    "        model_id=model_id, \n",
    "        runtime_version=runtime_version, \n",
    "        python_version=python_version,\n",
    "        replace_existing_version=True, \n",
    "        set_default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubeflow serving deployment component as an option. **Note that, the deployed Endppoint URI is not availabe as output of this component.**\n",
    "```python\n",
    "kubeflow_deploy_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/gcp/ml_engine/deploy/component.yaml')\n",
    "\n",
    "def deploy_kubeflow(\n",
    "    model_dir,\n",
    "    tf_server_name):\n",
    "    return kubeflow_deploy_op(\n",
    "        model_dir=model_dir,\n",
    "        server_name=tf_server_name,\n",
    "        cluster_name='kubeflow', \n",
    "        namespace='kubeflow',\n",
    "        pvc_name='', \n",
    "        service_type='ClusterIP')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a lightweight component for testing the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deployment_test(project_id: str, model_name: str, version: str) -> str:\n",
    "\n",
    "    model_name = model_name.split(\"/\")[-1]\n",
    "    version = version.split(\"/\")[-1]\n",
    "    \n",
    "    import googleapiclient.discovery\n",
    "    \n",
    "    def predict(project, model, data, version=None):\n",
    "      \"\"\"Run predictions on a list of instances.\n",
    "\n",
    "      Args:\n",
    "        project: (str), project where the Cloud ML Engine Model is deployed.\n",
    "        model: (str), model name.\n",
    "        data: ([[any]]), list of input instances, where each input instance is a\n",
    "          list of attributes.\n",
    "        version: str, version of the model to target.\n",
    "\n",
    "      Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the model.\n",
    "      \"\"\"\n",
    "\n",
    "      service = googleapiclient.discovery.build('ml', 'v1')\n",
    "      name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "      if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "      response = service.projects().predict(\n",
    "          name=name, body={\n",
    "              'instances': data\n",
    "          }).execute()\n",
    "\n",
    "      if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "      return response['predictions']\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import json\n",
    "    \n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    result = predict(\n",
    "        project=project_id,\n",
    "        model=model_name,\n",
    "        data=x_test[0:2].tolist(),\n",
    "        version=version)\n",
    "    print(result)\n",
    "    \n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function with already deployed version\n",
    "# deployment_test(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     model_name=\"minist\",\n",
    "#     version='ver_bb1ebd2a06ab7f321ad3db6b3b3d83e6' # previous deployed version for testing\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_test_op = comp.func_to_container_op(\n",
    "    func=deployment_test, \n",
    "    base_image=\"tensorflow/tensorflow:1.15.0-py3\",\n",
    "    packages_to_install=[\"google-api-python-client==1.7.8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your workflow as a Python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your pipeline as a Python function. ` @kfp.dsl.pipeline` is a required decoration including `name` and `description` properties. Then compile the pipeline function. After the compilation is completed, a pipeline file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Minist pipeline',\n",
    "   description='A toy pipeline that performs minist model training.'\n",
    ")\n",
    "def minist_reuse_component_pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    model_path: str = 'mnist_model', \n",
    "    bucket: str = GCS_BUCKET\n",
    "):\n",
    "    train_task = minist_train_op(\n",
    "        model_path=model_path, \n",
    "        bucket=bucket\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    deploy_task = deploy(\n",
    "        project_id=project_id,\n",
    "        model_uri=train_task.outputs['gcs_model_path'],\n",
    "        model_id=\"minist\", \n",
    "        runtime_version=\"1.14\",\n",
    "        python_version=\"3.5\"\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))  \n",
    "    \n",
    "    deploy_test_task = deployment_test_op(\n",
    "        project_id=project_id,\n",
    "        model_name=deploy_task.outputs[\"model_name\"], \n",
    "        version=deploy_task.outputs[\"version_name\"],\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/kfp/components/_data_passing.py:154: UserWarning: There are no registered serializers from type \"bool\" to type \"Bool\", so the value will be serializers as string \"True\".\n",
      "  serialized_value),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/a1bdd769-68c9-42eb-a48b-f239f2f0a94e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/01b865f6-48f9-42fc-8fa8-c283d72b56a7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_func = minist_reuse_component_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "#Submit a pipeline run\n",
    "arguments = {\"model_path\":\"mnist_model\",\n",
    "             \"bucket\":GCS_BUCKET}\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "experiment = client.create_experiment('python-functions-minist')\n",
    "\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n",
    "\n",
    "## Submit pipeline directly from pipeline function\n",
    "# client.create_run_from_pipeline_func(pipeline_func, experiment_name={}, run_name={} arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython35",
   "language": "python",
   "name": "virtualpython35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
