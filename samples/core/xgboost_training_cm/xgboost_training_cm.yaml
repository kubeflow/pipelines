"apiVersion": |-
  argoproj.io/v1alpha1
"kind": |-
  Workflow
"metadata":
  "annotations":
    "pipelines.kubeflow.org/pipeline_spec": |-
      {"description": "A trainer that does end-to-end distributed training for XGBoost models.", "inputs": [{"default": "gs://your-gcs-bucket", "name": "output"}, {"default": "your-gcp-project", "name": "project"}, {"default": "xgb-{{workflow.uid}}", "name": "cluster_name"}, {"default": "us-central1", "name": "region"}, {"default": "gs://ml-pipeline-playground/sfpd/train.csv", "name": "train_data"}, {"default": "gs://ml-pipeline-playground/sfpd/eval.csv", "name": "eval_data"}, {"default": "gs://ml-pipeline-playground/sfpd/schema.json", "name": "schema"}, {"default": "resolution", "name": "target"}, {"default": "200", "name": "rounds"}, {"default": "2", "name": "workers"}, {"default": "ACTION", "name": "true_label"}], "name": "XGBoost Trainer"}
  "generateName": |-
    xgboost-trainer-
"spec":
  "arguments":
    "parameters":
    - "name": |-
        output
      "value": |-
        gs://your-gcs-bucket
    - "name": |-
        project
      "value": |-
        your-gcp-project
    - "name": |-
        cluster_name
      "value": |-
        xgb-{{workflow.uid}}
    - "name": |-
        region
      "value": |-
        us-central1
    - "name": |-
        train_data
      "value": |-
        gs://ml-pipeline-playground/sfpd/train.csv
    - "name": |-
        eval_data
      "value": |-
        gs://ml-pipeline-playground/sfpd/eval.csv
    - "name": |-
        schema
      "value": |-
        gs://ml-pipeline-playground/sfpd/schema.json
    - "name": |-
        target
      "value": |-
        resolution
    - "name": |-
        rounds
      "value": |-
        200
    - "name": |-
        workers
      "value": |-
        2
    - "name": |-
        true_label
      "value": |-
        ACTION
  "entrypoint": |-
    xgboost-trainer
  "onExit": |-
    dataproc-delete-cluster
  "serviceAccountName": |-
    pipeline-runner
  "templates":
  - "container":
      "args":
      - |-
        --predictions
      - |-
        {{inputs.parameters.output}}/{{workflow.uid}}/data/predict_output/part-*.csv
      - |-
        --target_lambda
      - ""
      - |-
        --output
      - |-
        {{inputs.parameters.output}}/{{workflow.uid}}/data
      "command":
      - |-
        python2
      - |-
        /ml/confusion_matrix.py
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-local-confusion-matrix:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          output
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Calculates confusion matrix", "inputs": [{"description": "GCS path of prediction file pattern.", "name": "Predictions", "type": "GCSPath"}, {"default": "", "description": "Text of Python lambda function which computes target value. For example, \"lambda x: x['a'] + x['b']\". If not set, the input must include a \"target\" column.", "name": "Target lambda", "type": "String"}, {"description": "GCS path of the output directory.", "name": "Output dir", "type": "GCSPath"}], "name": "Confusion matrix", "outputs": [{"name": "MLPipeline UI metadata", "type": "UI metadata"}, {"name": "MLPipeline Metrics", "type": "Metrics"}]}
    "name": |-
      confusion-matrix
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          mlpipeline-metrics
        "path": |-
          /mlpipeline-metrics.json
  - "container":
      "args":
      - |-
        kfp_component.google.dataproc
      - |-
        create_cluster
      - |-
        --project_id
      - |-
        {{inputs.parameters.project}}
      - |-
        --region
      - |-
        {{inputs.parameters.region}}
      - |-
        --name
      - |-
        {{inputs.parameters.cluster_name}}
      - |-
        --name_prefix
      - ""
      - |-
        --initialization_actions
      - |-
        ["gs://ml-pipeline-playground/dataproc-example/initialization_actions.sh"]
      - |-
        --config_bucket
      - ""
      - |-
        --image_version
      - |-
        1.2
      - |-
        --cluster
      - ""
      - |-
        --wait_interval
      - |-
        30
      "command": []
      "env":
      - "name": |-
          KFP_POD_NAME
        "value": |-
          {{pod.name}}
      - "name": |-
          KFP_POD_NAME
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.name
      - "name": |-
          KFP_NAMESPACE
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.namespace
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-gcp:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          project
      - "name": |-
          region
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Creates a DataProc cluster under a project.\n", "inputs": [{"description": "Required. The ID of the Google Cloud Platform project that the cluster belongs to.", "name": "project_id", "type": "GCPProjectID"}, {"description": "Required. The Cloud Dataproc region in which to handle the request.", "name": "region", "type": "GCPRegion"}, {"default": "", "description": "Optional. The cluster name. Cluster names within a project must be unique. Names of  deleted clusters can be reused", "name": "name", "type": "String"}, {"default": "", "description": "Optional. The prefix of the cluster name.", "name": "name_prefix", "type": "String"}, {"default": "", "description": "Optional. List of GCS URIs of executables to execute on each node after config is completed. By default, executables are run on master and all worker nodes.", "name": "initialization_actions", "type": "List"}, {"default": "", "description": "Optional. A Google Cloud Storage bucket used to stage job dependencies, config files, and job driver console output.", "name": "config_bucket", "type": "GCSPath"}, {"default": "", "description": "Optional. The version of software inside the cluster.", "name": "image_version", "type": "String"}, {"default": "", "description": "Optional. The full cluster config. See  [full details](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.clusters#Cluster)", "name": "cluster", "type": "Dict"}, {"default": "30", "description": "Optional. The wait seconds between polling the operation. Defaults to 30.", "name": "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "dataproc_create_cluster", "outputs": [{"description": "The cluster name of the created cluster.", "name": "cluster_name", "type": "String"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}
      "labels":
        "add-pod-env": |-
          true
    "name": |-
      dataproc-create-cluster
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          dataproc-create-cluster-cluster_name
        "path": |-
          /tmp/kfp/output/dataproc/cluster_name.txt
  - "container":
      "args":
      - |-
        kfp_component.google.dataproc
      - |-
        delete_cluster
      - |-
        --project_id
      - |-
        {{inputs.parameters.project}}
      - |-
        --region
      - |-
        {{inputs.parameters.region}}
      - |-
        --name
      - |-
        {{inputs.parameters.cluster_name}}
      - |-
        --wait_interval
      - |-
        30
      "command": []
      "env":
      - "name": |-
          KFP_POD_NAME
        "value": |-
          {{pod.name}}
      - "name": |-
          KFP_POD_NAME
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.name
      - "name": |-
          KFP_NAMESPACE
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.namespace
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-gcp:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          project
      - "name": |-
          region
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Deletes a DataProc cluster.\n", "inputs": [{"description": "Required. The ID of the Google Cloud Platform project that the cluster belongs to.", "name": "project_id", "type": "GCPProjectID"}, {"description": "Required. The Cloud Dataproc region in which to handle the request.", "name": "region", "type": "GCPRegion"}, {"description": "Required. The cluster name to delete.", "name": "name", "type": "String"}, {"default": "30", "description": "Optional. The wait seconds between polling the operation. Defaults to 30.", "name": "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "dataproc_delete_cluster"}
      "labels":
        "add-pod-env": |-
          true
    "name": |-
      dataproc-delete-cluster
  - "container":
      "args":
      - |-
        kfp_component.google.dataproc
      - |-
        submit_pyspark_job
      - |-
        --project_id
      - |-
        {{inputs.parameters.project}}
      - |-
        --region
      - |-
        {{inputs.parameters.region}}
      - |-
        --cluster_name
      - |-
        {{inputs.parameters.cluster_name}}
      - |-
        --main_python_file_uri
      - |-
        gs://ml-pipeline-playground/dataproc-example/analyze_run.py
      - |-
        --args
      - |-
        ["--output", "{{inputs.parameters.output}}/{{workflow.uid}}/data", "--train", "{{inputs.parameters.train_data}}", "--schema", "{{inputs.parameters.schema}}"]
      - |-
        --pyspark_job
      - ""
      - |-
        --job
      - ""
      - |-
        --wait_interval
      - |-
        30
      "command": []
      "env":
      - "name": |-
          KFP_POD_NAME
        "value": |-
          {{pod.name}}
      - "name": |-
          KFP_POD_NAME
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.name
      - "name": |-
          KFP_NAMESPACE
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.namespace
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-gcp:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          output
      - "name": |-
          project
      - "name": |-
          region
      - "name": |-
          schema
      - "name": |-
          train_data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Submits a Cloud Dataproc job for running Apache PySpark applications on YARN.", "inputs": [{"description": "Required. The ID of the Google Cloud Platform project that the cluster belongs to.", "name": "project_id", "type": "GCPProjectID"}, {"description": "Required. The Cloud Dataproc region in which to handle the request.", "name": "region", "type": "GCPRegion"}, {"description": "Required. The cluster to run the job.", "name": "cluster_name", "type": "String"}, {"description": "Required. The HCFS URI of the main Python file to  use as the driver. Must be a .py file.", "name": "main_python_file_uri", "type": "GCSPath"}, {"default": "", "description": "Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.", "name": "args", "type": "List"}, {"default": "", "description": "Optional. The full payload of a [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob).", "name": "pyspark_job", "type": "Dict"}, {"default": "", "description": "Optional. The full payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs).", "name": "job", "type": "Dict"}, {"default": "30", "description": "Optional. The wait seconds between polling the operation. Defaults to 30.", "name": "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "dataproc_submit_pyspark_job", "outputs": [{"description": "The ID of the created job.", "name": "job_id", "type": "String"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}
        "pipelines.kubeflow.org/task_display_name": |-
          Analyzer
      "labels":
        "add-pod-env": |-
          true
    "name": |-
      dataproc-submit-pyspark-job
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          dataproc-submit-pyspark-job-job_id
        "path": |-
          /tmp/kfp/output/dataproc/job_id.txt
  - "container":
      "args":
      - |-
        kfp_component.google.dataproc
      - |-
        submit_pyspark_job
      - |-
        --project_id
      - |-
        {{inputs.parameters.project}}
      - |-
        --region
      - |-
        {{inputs.parameters.region}}
      - |-
        --cluster_name
      - |-
        {{inputs.parameters.cluster_name}}
      - |-
        --main_python_file_uri
      - |-
        gs://ml-pipeline-playground/dataproc-example/transform_run.py
      - |-
        --args
      - |-
        ["--output", "{{inputs.parameters.output}}/{{workflow.uid}}/data", "--analysis", "{{inputs.parameters.output}}/{{workflow.uid}}/data", "--target", "{{inputs.parameters.target}}", "--train", "{{inputs.parameters.train_data}}", "--eval", "{{inputs.parameters.eval_data}}"]
      - |-
        --pyspark_job
      - ""
      - |-
        --job
      - ""
      - |-
        --wait_interval
      - |-
        30
      "command": []
      "env":
      - "name": |-
          KFP_POD_NAME
        "value": |-
          {{pod.name}}
      - "name": |-
          KFP_POD_NAME
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.name
      - "name": |-
          KFP_NAMESPACE
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.namespace
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-gcp:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          eval_data
      - "name": |-
          output
      - "name": |-
          project
      - "name": |-
          region
      - "name": |-
          target
      - "name": |-
          train_data
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Submits a Cloud Dataproc job for running Apache PySpark applications on YARN.", "inputs": [{"description": "Required. The ID of the Google Cloud Platform project that the cluster belongs to.", "name": "project_id", "type": "GCPProjectID"}, {"description": "Required. The Cloud Dataproc region in which to handle the request.", "name": "region", "type": "GCPRegion"}, {"description": "Required. The cluster to run the job.", "name": "cluster_name", "type": "String"}, {"description": "Required. The HCFS URI of the main Python file to  use as the driver. Must be a .py file.", "name": "main_python_file_uri", "type": "GCSPath"}, {"default": "", "description": "Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.", "name": "args", "type": "List"}, {"default": "", "description": "Optional. The full payload of a [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob).", "name": "pyspark_job", "type": "Dict"}, {"default": "", "description": "Optional. The full payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs).", "name": "job", "type": "Dict"}, {"default": "30", "description": "Optional. The wait seconds between polling the operation. Defaults to 30.", "name": "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "dataproc_submit_pyspark_job", "outputs": [{"description": "The ID of the created job.", "name": "job_id", "type": "String"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}
        "pipelines.kubeflow.org/task_display_name": |-
          Transformer
      "labels":
        "add-pod-env": |-
          true
    "name": |-
      dataproc-submit-pyspark-job-2
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          dataproc-submit-pyspark-job-2-job_id
        "path": |-
          /tmp/kfp/output/dataproc/job_id.txt
  - "container":
      "args":
      - |-
        kfp_component.google.dataproc
      - |-
        submit_spark_job
      - |-
        --project_id
      - |-
        {{inputs.parameters.project}}
      - |-
        --region
      - |-
        {{inputs.parameters.region}}
      - |-
        --cluster_name
      - |-
        {{inputs.parameters.cluster_name}}
      - |-
        --main_jar_file_uri
      - ""
      - |-
        --main_class
      - |-
        ml.dmlc.xgboost4j.scala.example.spark.XGBoostTrainer
      - |-
        --args
      - |-
        ["gs://ml-pipeline-playground/trainconfcla.json", "{{inputs.parameters.rounds}}", "{{inputs.parameters.workers}}", "{{inputs.parameters.output}}/{{workflow.uid}}/data", "{{inputs.parameters.target}}", "{{inputs.parameters.output}}/{{workflow.uid}}/data/train/part-*", "{{inputs.parameters.output}}/{{workflow.uid}}/data/eval/part-*", "{{inputs.parameters.output}}/{{workflow.uid}}/data/train_output"]
      - |-
        --spark_job
      - |-
        {"jarFileUris": ["gs://ml-pipeline-playground/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar"]}
      - |-
        --job
      - ""
      - |-
        --wait_interval
      - |-
        30
      "command": []
      "env":
      - "name": |-
          KFP_POD_NAME
        "value": |-
          {{pod.name}}
      - "name": |-
          KFP_POD_NAME
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.name
      - "name": |-
          KFP_NAMESPACE
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.namespace
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-gcp:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          output
      - "name": |-
          project
      - "name": |-
          region
      - "name": |-
          rounds
      - "name": |-
          target
      - "name": |-
          workers
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Submits a Cloud Dataproc job for running Apache Spark applications on YARN.", "inputs": [{"description": "Required. The ID of the Google Cloud Platform project that the cluster belongs to.", "name": "project_id", "type": "GCPProjectID"}, {"description": "Required. The Cloud Dataproc region in which to handle the request.", "name": "region", "type": "GCPRegion"}, {"description": "Required. The cluster to run the job.", "name": "cluster_name", "type": "String"}, {"default": "", "description": "The HCFS URI of the jar file that contains the main class.", "name": "main_jar_file_uri", "type": "GCSPath"}, {"default": "", "description": "The name of the driver's main class. The jar file that  contains the class must be in the default CLASSPATH or specified in  jarFileUris.", "name": "main_class", "type": "String"}, {"default": "", "description": "Optional. The arguments to pass to the driver. Do not include  arguments, such as --conf, that can be set as job properties, since a  collision may occur that causes an incorrect job submission.", "name": "args", "type": "List"}, {"default": "", "description": "Optional. The full payload of a [SparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob).", "name": "spark_job", "type": "Dict"}, {"default": "", "description": "Optional. The full payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs).", "name": "job", "type": "Dict"}, {"default": "30", "description": "Optional. The wait seconds between polling the operation. Defaults to 30.", "name": "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "dataproc_submit_spark_job", "outputs": [{"description": "The ID of the created job.", "name": "job_id", "type": "String"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}
        "pipelines.kubeflow.org/task_display_name": |-
          Trainer
      "labels":
        "add-pod-env": |-
          true
    "name": |-
      dataproc-submit-spark-job
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          dataproc-submit-spark-job-job_id
        "path": |-
          /tmp/kfp/output/dataproc/job_id.txt
  - "container":
      "args":
      - |-
        kfp_component.google.dataproc
      - |-
        submit_spark_job
      - |-
        --project_id
      - |-
        {{inputs.parameters.project}}
      - |-
        --region
      - |-
        {{inputs.parameters.region}}
      - |-
        --cluster_name
      - |-
        {{inputs.parameters.cluster_name}}
      - |-
        --main_jar_file_uri
      - ""
      - |-
        --main_class
      - |-
        ml.dmlc.xgboost4j.scala.example.spark.XGBoostPredictor
      - |-
        --args
      - |-
        ["{{inputs.parameters.output}}/{{workflow.uid}}/data/train_output", "{{inputs.parameters.output}}/{{workflow.uid}}/data/eval/part-*", "{{inputs.parameters.output}}/{{workflow.uid}}/data", "{{inputs.parameters.target}}", "{{inputs.parameters.output}}/{{workflow.uid}}/data/predict_output"]
      - |-
        --spark_job
      - |-
        {"jarFileUris": ["gs://ml-pipeline-playground/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar"]}
      - |-
        --job
      - ""
      - |-
        --wait_interval
      - |-
        30
      "command": []
      "env":
      - "name": |-
          KFP_POD_NAME
        "value": |-
          {{pod.name}}
      - "name": |-
          KFP_POD_NAME
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.name
      - "name": |-
          KFP_NAMESPACE
        "valueFrom":
          "fieldRef":
            "fieldPath": |-
              metadata.namespace
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-gcp:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          output
      - "name": |-
          project
      - "name": |-
          region
      - "name": |-
          target
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Submits a Cloud Dataproc job for running Apache Spark applications on YARN.", "inputs": [{"description": "Required. The ID of the Google Cloud Platform project that the cluster belongs to.", "name": "project_id", "type": "GCPProjectID"}, {"description": "Required. The Cloud Dataproc region in which to handle the request.", "name": "region", "type": "GCPRegion"}, {"description": "Required. The cluster to run the job.", "name": "cluster_name", "type": "String"}, {"default": "", "description": "The HCFS URI of the jar file that contains the main class.", "name": "main_jar_file_uri", "type": "GCSPath"}, {"default": "", "description": "The name of the driver's main class. The jar file that  contains the class must be in the default CLASSPATH or specified in  jarFileUris.", "name": "main_class", "type": "String"}, {"default": "", "description": "Optional. The arguments to pass to the driver. Do not include  arguments, such as --conf, that can be set as job properties, since a  collision may occur that causes an incorrect job submission.", "name": "args", "type": "List"}, {"default": "", "description": "Optional. The full payload of a [SparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob).", "name": "spark_job", "type": "Dict"}, {"default": "", "description": "Optional. The full payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs).", "name": "job", "type": "Dict"}, {"default": "30", "description": "Optional. The wait seconds between polling the operation. Defaults to 30.", "name": "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "dataproc_submit_spark_job", "outputs": [{"description": "The ID of the created job.", "name": "job_id", "type": "String"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}
        "pipelines.kubeflow.org/task_display_name": |-
          Predictor
      "labels":
        "add-pod-env": |-
          true
    "name": |-
      dataproc-submit-spark-job-2
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          dataproc-submit-spark-job-2-job_id
        "path": |-
          /tmp/kfp/output/dataproc/job_id.txt
  - "dag":
      "tasks":
      - "arguments":
          "parameters":
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
        "dependencies":
        - |-
          dataproc-submit-spark-job-2
        "name": |-
          confusion-matrix
        "template": |-
          confusion-matrix
      - "arguments":
          "parameters":
          - "name": |-
              cluster_name
            "value": |-
              {{inputs.parameters.cluster_name}}
          - "name": |-
              project
            "value": |-
              {{inputs.parameters.project}}
          - "name": |-
              region
            "value": |-
              {{inputs.parameters.region}}
        "name": |-
          dataproc-create-cluster
        "template": |-
          dataproc-create-cluster
      - "arguments":
          "parameters":
          - "name": |-
              cluster_name
            "value": |-
              {{inputs.parameters.cluster_name}}
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
          - "name": |-
              project
            "value": |-
              {{inputs.parameters.project}}
          - "name": |-
              region
            "value": |-
              {{inputs.parameters.region}}
          - "name": |-
              schema
            "value": |-
              {{inputs.parameters.schema}}
          - "name": |-
              train_data
            "value": |-
              {{inputs.parameters.train_data}}
        "dependencies":
        - |-
          dataproc-create-cluster
        "name": |-
          dataproc-submit-pyspark-job
        "template": |-
          dataproc-submit-pyspark-job
      - "arguments":
          "parameters":
          - "name": |-
              cluster_name
            "value": |-
              {{inputs.parameters.cluster_name}}
          - "name": |-
              eval_data
            "value": |-
              {{inputs.parameters.eval_data}}
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
          - "name": |-
              project
            "value": |-
              {{inputs.parameters.project}}
          - "name": |-
              region
            "value": |-
              {{inputs.parameters.region}}
          - "name": |-
              target
            "value": |-
              {{inputs.parameters.target}}
          - "name": |-
              train_data
            "value": |-
              {{inputs.parameters.train_data}}
        "dependencies":
        - |-
          dataproc-submit-pyspark-job
        "name": |-
          dataproc-submit-pyspark-job-2
        "template": |-
          dataproc-submit-pyspark-job-2
      - "arguments":
          "parameters":
          - "name": |-
              cluster_name
            "value": |-
              {{inputs.parameters.cluster_name}}
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
          - "name": |-
              project
            "value": |-
              {{inputs.parameters.project}}
          - "name": |-
              region
            "value": |-
              {{inputs.parameters.region}}
          - "name": |-
              rounds
            "value": |-
              {{inputs.parameters.rounds}}
          - "name": |-
              target
            "value": |-
              {{inputs.parameters.target}}
          - "name": |-
              workers
            "value": |-
              {{inputs.parameters.workers}}
        "dependencies":
        - |-
          dataproc-submit-pyspark-job-2
        "name": |-
          dataproc-submit-spark-job
        "template": |-
          dataproc-submit-spark-job
      - "arguments":
          "parameters":
          - "name": |-
              cluster_name
            "value": |-
              {{inputs.parameters.cluster_name}}
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
          - "name": |-
              project
            "value": |-
              {{inputs.parameters.project}}
          - "name": |-
              region
            "value": |-
              {{inputs.parameters.region}}
          - "name": |-
              target
            "value": |-
              {{inputs.parameters.target}}
        "dependencies":
        - |-
          dataproc-submit-spark-job
        "name": |-
          dataproc-submit-spark-job-2
        "template": |-
          dataproc-submit-spark-job-2
      - "arguments":
          "parameters":
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
          - "name": |-
              true_label
            "value": |-
              {{inputs.parameters.true_label}}
        "dependencies":
        - |-
          dataproc-submit-spark-job-2
        "name": |-
          roc-curve
        "template": |-
          roc-curve
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          eval_data
      - "name": |-
          output
      - "name": |-
          project
      - "name": |-
          region
      - "name": |-
          rounds
      - "name": |-
          schema
      - "name": |-
          target
      - "name": |-
          train_data
      - "name": |-
          true_label
      - "name": |-
          workers
    "name": |-
      exit-handler-1
  - "container":
      "args":
      - |-
        --predictions
      - |-
        {{inputs.parameters.output}}/{{workflow.uid}}/data/predict_output/part-*.csv
      - |-
        --trueclass
      - |-
        {{inputs.parameters.true_label}}
      - |-
        --true_score_column
      - |-
        {{inputs.parameters.true_label}}
      - |-
        --target_lambda
      - ""
      - |-
        --output
      - |-
        {{inputs.parameters.output}}/{{workflow.uid}}/data
      "command":
      - |-
        python2
      - |-
        /ml/roc.py
      "image": |-
        gcr.io/ml-pipeline/ml-pipeline-local-confusion-matrix:9670cc1aadfbbed9c52b84ea859ea97aa81213ad
    "inputs":
      "parameters":
      - "name": |-
          output
      - "name": |-
          true_label
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "Calculates Receiver Operating Characteristic curve. See https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "inputs": [{"description": "GCS path of prediction file pattern.", "name": "Predictions dir", "type": "GCSPath"}, {"default": "true", "description": "The true class label for the sample. Default is \"true\".", "name": "True class", "type": "String"}, {"default": "true", "description": "The name of the column for positive probability.", "name": "True score column", "type": "String"}, {"default": "", "description": "Text of Python lambda function which returns boolean value indicating whether the classification result is correct.\\nFor example, \"lambda x: x['a'] and x['b']\". If missing, input must have a \"target\" column.", "name": "Target lambda", "type": "String"}, {"description": "GCS path of the output directory.", "name": "Output dir", "type": "GCSPath"}], "name": "ROC curve", "outputs": [{"name": "MLPipeline UI metadata", "type": "UI metadata"}, {"name": "MLPipeline Metrics", "type": "Metrics"}]}
    "name": |-
      roc-curve
    "outputs":
      "artifacts":
      - "name": |-
          mlpipeline-ui-metadata
        "path": |-
          /mlpipeline-ui-metadata.json
      - "name": |-
          mlpipeline-metrics
        "path": |-
          /mlpipeline-metrics.json
  - "dag":
      "tasks":
      - "arguments":
          "parameters":
          - "name": |-
              cluster_name
            "value": |-
              {{inputs.parameters.cluster_name}}
          - "name": |-
              eval_data
            "value": |-
              {{inputs.parameters.eval_data}}
          - "name": |-
              output
            "value": |-
              {{inputs.parameters.output}}
          - "name": |-
              project
            "value": |-
              {{inputs.parameters.project}}
          - "name": |-
              region
            "value": |-
              {{inputs.parameters.region}}
          - "name": |-
              rounds
            "value": |-
              {{inputs.parameters.rounds}}
          - "name": |-
              schema
            "value": |-
              {{inputs.parameters.schema}}
          - "name": |-
              target
            "value": |-
              {{inputs.parameters.target}}
          - "name": |-
              train_data
            "value": |-
              {{inputs.parameters.train_data}}
          - "name": |-
              true_label
            "value": |-
              {{inputs.parameters.true_label}}
          - "name": |-
              workers
            "value": |-
              {{inputs.parameters.workers}}
        "name": |-
          exit-handler-1
        "template": |-
          exit-handler-1
    "inputs":
      "parameters":
      - "name": |-
          cluster_name
      - "name": |-
          eval_data
      - "name": |-
          output
      - "name": |-
          project
      - "name": |-
          region
      - "name": |-
          rounds
      - "name": |-
          schema
      - "name": |-
          target
      - "name": |-
          train_data
      - "name": |-
          true_label
      - "name": |-
          workers
    "name": |-
      xgboost-trainer
