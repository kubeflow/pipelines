apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  creationTimestamp: null
  generateName: nb-mixed-
spec:
  arguments:
    parameters:
    - name: components-43c0d13fb84d9ca119bdf5124174bd3678698b085ea5c314339233390502736a
      value: '{"executorLabel":"exec-evaluate-model","inputDefinitions":{"artifacts":{"model_text":{"artifactType":{"schemaTitle":"system.Model","schemaVersion":"0.0.1"}}}},"outputDefinitions":{"artifacts":{"metrics":{"artifactType":{"schemaTitle":"system.Metrics","schemaVersion":"0.0.1"}}}}}'
    - name: implementations-43c0d13fb84d9ca119bdf5124174bd3678698b085ea5c314339233390502736a
      value: '{"args":["--executor_input","{{$}}","--function_to_execute","evaluate_model"],"command":["sh","-c","\nif
        ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3
        -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location ''nbclient\u003e=0.10,\u003c1''
        ''ipykernel\u003e=6,\u003c7'' ''jupyter_client\u003e=7,\u003c9''  \u0026\u0026  python3
        -m pip install --quiet --no-warn-script-location ''kfp==2.14.2'' ''--no-deps''
        ''typing-extensions\u003e=3.7.4,\u003c5; python_version\u003c\"3.9\"'' \u0026\u0026
        \"$0\" \"$@\"\n","sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\"
        \u003e \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3
        -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport
        kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\n__KFP_EMBEDDED_ARCHIVE_B64
        = ''H4sIACSp1mgC/+2Vy46bMBSGWecpLG9IJEogQGAiRequXXbR3WSEHGISN2BbtukkivLuNYQkk05HlarOqJfzLQz8x+f3wRfwx/74/Sey+0jJiirnVQhOvHQNgii63rd6GEzC0EE75w1otCHKDu/8n0xSVBtW03mYJll2lwVZ6meTOEzvBg7w78OXOf1KqrymRrFC+0zu+fIVzv80jp0gCqIkTK7XE2GcJk6YBNMkmUTTpD3/8TSNHBS85fmvpSKb6uV+P4v/pRwGCOGCVpXGM3RvHxA6dG0v52YvqQ3hQqwo9s4huqNFY5jgeSEabmwH3lTVJWx3E1kRQ6x+drOqIevrKL0miSK2N1UaX+SH/u548RONkY3pkh8uohaNKuiNIa5tlbZmujNojlx3wc+uJ8/e8Te/4S/UyWoplEFCe+iLFtzW6V2DQvs12dIVU3rojk0tx9tS5vak9vauh+iOaZOL7fyzaujoNr0/ynYCDq4uhKLuDJWVIGZYUT68ztBodLxNlIpxM1zgD8KgLhEtsId6u/sF7rQFfvhuvEdmNkhI6/2jYsfnL0v7nrZy97Etn9vJZnw9dxtTvsvcESIalbNb37Zpk/xVU8thb+OhcvRsVW3bTffzfYcrwtcNWdOc8VI82Y6Y223Xrrrcm43oDVujbjExX5ZC1aRd9vhGyGvGhbLyZHCEPyQAAAAAAAAAAAAAAAAAAAAAAAAAAADwp/ANkcfbmgAoAAA=''\n__KFP_NOTEBOOK_REL_PATH
        = ''nb_eval_metrics.ipynb''\n\nimport base64 as __kfp_b64\nimport gzip as
        __kfp_gzip\nimport io as __kfp_io\nimport os as __kfp_os\nimport sys as __kfp_sys\nimport
        tarfile as __kfp_tarfile\nimport tempfile as __kfp_tempfile\nfrom nbclient
        import NotebookClient\n\n# Extract embedded archive at import time to ensure
        sys.path and globals are set\nprint(''[KFP] Extracting embedded notebook archive...'',
        flush=True)\n__kfp_tmpdir = __kfp_tempfile.TemporaryDirectory()\n__KFP_EMBEDDED_ASSET_DIR
        = __kfp_tmpdir.name\ntry:\n    __kfp_bytes = __kfp_b64.b64decode(__KFP_EMBEDDED_ARCHIVE_B64.encode(''ascii''))\n    with
        __kfp_tarfile.open(fileobj=__kfp_io.BytesIO(__kfp_bytes), mode=''r:gz'') as
        __kfp_tar:\n        __kfp_tar.extractall(path=__KFP_EMBEDDED_ASSET_DIR)\n    print(f''[KFP]
        Notebook archive extracted to: {__KFP_EMBEDDED_ASSET_DIR}'', flush=True)\nexcept
        Exception as __kfp_e:\n    raise RuntimeError(f''Failed to extract embedded
        notebook archive: {__kfp_e}'')\n\n# Always prepend the extracted directory
        to sys.path for import resolution\nif __KFP_EMBEDDED_ASSET_DIR not in __kfp_sys.path:\n    __kfp_sys.path.insert(0,
        __KFP_EMBEDDED_ASSET_DIR)\n    print(f''[KFP] Added notebook archive directory
        to Python path'', flush=True)\n\n# Optional convenience for generic embedded
        file variable name\n__KFP_EMBEDDED_ASSET_FILE = __kfp_os.path.join(__KFP_EMBEDDED_ASSET_DIR,
        __KFP_NOTEBOOK_REL_PATH)\n\n\nclass KFPStreamingNotebookClient(NotebookClient):\n    #
        Streams outputs in real-time by emitting outputs during message processing.\n    def
        process_message(self, msg, cell, cell_index):\n        # Call the parent implementation
        to handle the message normally\n        output = super().process_message(msg,
        cell, cell_index)\n\n        # If an output was created, stream it immediately\n        if
        output is not None:\n            _kfp_stream_single_output(output, cell_index)\n\n        return
        output\n\ndef __kfp_write_parameters_cell(nb, params):\n    \"\"\"Inject parameters
        following Papermill semantics.\n\n    - If a cell tagged with ''parameters''
        exists, insert an overriding\n      ''injected-parameters'' cell immediately
        after it.\n    - Otherwise, insert the ''injected-parameters'' cell at the
        top.\n    \"\"\"\n    import json\n\n    import nbformat\n\n    if not params:\n        return\n\n    #
        Build the injected parameters cell\n    assignments = []\n    for key, value
        in params.items():\n        serialized = json.dumps(value)\n        assignments.append(key
        + '' = json.loads('' + repr(serialized) + '')'')\n    source = ''import json\\n''
        + ''\\n''.join(assignments) + ''\\n''\n    cell = nbformat.v4.new_code_cell(source=source)\n    cell.metadata.setdefault(''tags'',
        [])\n    if ''injected-parameters'' not in cell.metadata[''tags'']:\n        cell.metadata[''tags''].append(''injected-parameters'')\n\n    #
        Locate the first ''parameters'' tagged cell\n    insert_idx = 0\n    for idx,
        existing in enumerate(nb.get(''cells'', [])):\n        if existing.get(''cell_type'')
        != ''code'':\n            continue\n        tags = existing.get(''metadata'',
        {}).get(''tags'', []) or []\n        if ''parameters'' in tags:\n            insert_idx
        = idx + 1\n            break\n\n    nb.cells.insert(insert_idx, cell)\n\ndef
        _kfp_stream_single_output(output, cell_idx):\n    \"\"\"Stream a single notebook
        output immediately during execution.\n\n    Prints stdout/stderr and text/plain
        display outputs to the console so users\n    see cell output as it happens
        (no need to wait until the notebook finishes).\n    \"\"\"\n    import sys\n    output_type
        = output.get(''output_type'')\n\n    if output_type == ''stream'':\n        text
        = output.get(''text'', '''')\n        if text:\n            try:\n                print(f''[nb
        cell {cell_idx} stream] '', end='''', flush=False)\n            except Exception:\n                pass\n            print(text,
        end='''' if text.endswith(''\\n'') else ''\\n'', flush=True)\n    elif output_type
        == ''error'':\n        for line in output.get(''traceback'', []):\n            print(line,
        file=sys.stderr, flush=True)\n    else:\n        # Handle display_data and
        execute_result\n        data = output.get(''data'', {})\n        if ''text/plain''
        in data:\n            print(data[''text/plain''], flush=True)\n        elif
        ''application/json'' in data:\n            try:\n                import json
        as __kfp_json\n                parsed = data[''application/json'']\n                #
        Some kernels send JSON as string; try to parse if needed\n                if
        isinstance(parsed, str):\n                    try:\n                        parsed
        = __kfp_json.loads(parsed)\n                    except Exception:\n                        pass\n                print(__kfp_json.dumps(parsed,
        indent=2, ensure_ascii=False), flush=True)\n            except Exception:\n                #
        Fallback to raw\n                print(str(data.get(''application/json'')),
        flush=True)\n        elif ''text/markdown'' in data:\n            # Print
        markdown as-is; frontends may render, logs will show raw markdown\n            print(data[''text/markdown''],
        flush=True)\n\ndef kfp_run_notebook(**kwargs):\n    \"\"\"Execute the embedded
        notebook with injected parameters.\n\n    Parameters provided via kwargs are
        injected into the notebook following\n    Papermill semantics (after a parameters
        cell if present, otherwise at top).\n    Execution uses a Python kernel; nbclient
        and ipykernel must be available at\n    runtime (installed via packages_to_install
        for notebook components).\n    \"\"\"\n    import os\n    import subprocess\n    import
        sys\n\n    from nbclient import NotebookClient\n    import nbformat\n\n    #
        Ensure a usable ''python3'' kernel is present; install kernelspec if missing\n    print(''[KFP
        Notebook] Checking for Python kernel...'', flush=True)\n    try:\n        from
        jupyter_client.kernelspec import KernelSpecManager  # type: ignore\n        ksm
        = KernelSpecManager()\n        have_py3 = ''python3'' in ksm.find_kernel_specs()\n        if
        not have_py3:\n            print(\n                ''[KFP Notebook] Python3
        kernel not found, installing...'',\n                flush=True)\n            try:\n                subprocess.run([\n                    sys.executable,
        ''-m'', ''ipykernel'', ''install'', ''--user'',\n                    ''--name'',
        ''python3'', ''--display-name'', ''Python 3''\n                ],\n                               check=True,\n                               stdout=subprocess.DEVNULL,\n                               stderr=subprocess.DEVNULL)\n                print(\n                    ''[KFP
        Notebook] Python3 kernel installed successfully'',\n                    flush=True)\n            except
        subprocess.CalledProcessError as e:\n                raise RuntimeError(\n                    \"Failed
        to install ''python3'' kernelspec for ipykernel. \"\n                    \"Ensure
        ipykernel is available in the environment or include it via packages_to_install.
        \"\n                    f\"Error: {e}\") from e\n        else:\n            print(''[KFP
        Notebook] Python3 kernel found'', flush=True)\n    except ImportError as e:\n        raise
        RuntimeError(\n            \"jupyter_client is not available. Ensure it''s
        installed in the environment or include it via packages_to_install. \"\n            f\"Error:
        {e}\") from e\n\n    nb_path = os.path.join(__KFP_EMBEDDED_ASSET_DIR, __KFP_NOTEBOOK_REL_PATH)\n\n    try:\n        nb
        = nbformat.read(nb_path, as_version=4)\n    except Exception as e:\n        raise
        RuntimeError(\n            f''Failed to read notebook {nb_path}. Ensure it
        is a valid Jupyter notebook. Error: {e}''\n        ) from e\n\n    try:\n        __kfp_write_parameters_cell(nb,
        kwargs)\n        print(\n            f''[KFP Notebook] Executing notebook
        with {len(nb.get(\"cells\", []))} cells'',\n            flush=True)\n\n        #
        Use our custom streaming client for real-time output (defined in the\n        #
        generated ephemeral source)\n        client = KFPStreamingNotebookClient(\n            nb,\n            timeout=None,\n            allow_errors=False,\n            store_widget_state=False,\n            kernel_name=''python3'')\n        client.execute(cwd=__KFP_EMBEDDED_ASSET_DIR)\n\n        print(''[KFP
        Notebook] Execution complete'', flush=True)\n\n    except Exception as e:\n        raise
        RuntimeError(f''Notebook execution failed. Error: {e}'') from e\n\n\n# Bind
        helper into dsl namespace so user code can call dsl.run_notebook(...)\ndsl.run_notebook
        = kfp_run_notebook\n\n\ndef evaluate_model(model_text: dsl.Input[dsl.Model],
        metrics: dsl.Output[dsl.Metrics]):\n    import json\n\n    with open(model_text.path,
        \"r\", encoding=\"utf-8\") as f:\n        model_text = f.read()\n\n    dsl.run_notebook(model_text=model_text)\n    with
        open(\"/tmp/kfp_nb_outputs/metrics.json\", \"r\", encoding=\"utf-8\") as f:\n        metrics_dict
        = json.load(f)\n\n    assert metrics_dict == {\"score\": float(len(model_text))}\n\n    for
        metric_name, metric_value in metrics_dict.items():\n        metrics.log_metric(metric_name,
        metric_value)\n\n"],"image":"python:3.9"}'
    - name: components-de426933840df1d5b1966245bfd35532f6a11ccaffa3dbb2311a6b7dbf3ec609
      value: '{"executorLabel":"exec-preprocess","inputDefinitions":{"parameters":{"text":{"parameterType":"STRING"}}},"outputDefinitions":{"artifacts":{"dataset":{"artifactType":{"schemaTitle":"system.Dataset","schemaVersion":"0.0.1"}}}}}'
    - name: implementations-de426933840df1d5b1966245bfd35532f6a11ccaffa3dbb2311a6b7dbf3ec609
      value: '{"args":["--executor_input","{{$}}","--function_to_execute","preprocess"],"command":["sh","-c","\nif
        ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3
        -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location ''kfp==2.14.2'' ''--no-deps''
        ''typing-extensions\u003e=3.7.4,\u003c5; python_version\u003c\"3.9\"'' \u0026\u0026
        \"$0\" \"$@\"\n","sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\"
        \u003e \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3
        -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport
        kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\n\n\ndef
        preprocess(text: str, dataset: dsl.Output[dsl.Dataset]):\n    import re\n\n    cleaned_text
        = re.sub(r\"\\s+\", \" \", text).strip()\n    with open(dataset.path, \"w\",
        encoding=\"utf-8\") as f:\n        f.write(cleaned_text)\n\n"],"image":"python:3.9"}'
    - name: components-0e704495704924f0f854d4c513ac164f987781625d13c72aabf7e58c7fc1d08c
      value: '{"executorLabel":"exec-train-model","inputDefinitions":{"artifacts":{"cleaned_text":{"artifactType":{"schemaTitle":"system.Dataset","schemaVersion":"0.0.1"}}}},"outputDefinitions":{"artifacts":{"model":{"artifactType":{"schemaTitle":"system.Model","schemaVersion":"0.0.1"}}}}}'
    - name: implementations-0e704495704924f0f854d4c513ac164f987781625d13c72aabf7e58c7fc1d08c
      value: '{"args":["--executor_input","{{$}}","--function_to_execute","train_model"],"command":["sh","-c","\nif
        ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3
        -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location ''nbclient\u003e=0.10,\u003c1''
        ''ipykernel\u003e=6,\u003c7'' ''jupyter_client\u003e=7,\u003c9''  \u0026\u0026  python3
        -m pip install --quiet --no-warn-script-location ''kfp==2.14.2'' ''--no-deps''
        ''typing-extensions\u003e=3.7.4,\u003c5; python_version\u003c\"3.9\"'' \u0026\u0026
        \"$0\" \"$@\"\n","sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\"
        \u003e \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3
        -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport
        kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\n__KFP_EMBEDDED_ARCHIVE_B64
        = ''H4sIACSp1mgC/+2VTY/aMBCGc+ZXWLkEJBqSQAJdCanH9tZDpR7KKjKJAxaJbTmTErTiv9cJ4auwWrXartR2noMT3onnneAZcEfu6MNnWn9kNGXa+iN4B567et54cr5vdN8L/MAitfUGVCVQbeyt/5NgRgrgBZv703A2ez/zosgNwsgfh0HPQv55xDIGTbmItxzWsaKaFqXL1U4sX3f+o4mZ8bE3Dv3wfD3gT73Q8kMvCsNgHAWBmf9wGkws4r3l/BdK03X+/HMvxf9SnnqE2AnL89J+IN/MB0Ke2rWTY9gpZkJ2IlNmD48hVrOkAi5FnMhKgHlAVHl+ChcMaEqBGv2YzahAV2eXTms7jgHTpX2SH7u7/SmfrEBV0G5+PImlrHTCrhLaSc6oYGkMrAYyJ06xI51EGslZiKPNwaSzeOVX/o3CeaGkBiJLU+HwLMvSLeiGpVyXfWcEhRptMhWboe0SO0PCal5CLDfzL7pig+vtSnMB/YX9VXPgYkVAkjtJRoV50dyFGhb2TwmaXwUiFRN33c8bTR3OtilGmC/NOM2dCrJ3M2dAaEmyh+ukzZK5W1MT618emFspxXR/MPilU9J0a7/QeN/L9igvNaPmVKwqumKf0mOaU3R/04H3zuymTLO2p3xbxsks5iKTF5XYwrR/4692sJZdwiZRa22LZSZ1QZtum1wJccGF1EYOenv8o0YQBEEQBEEQBEEQBEEQBEEQBEEQBEF+APurSW0AKAAA''\n__KFP_NOTEBOOK_REL_PATH
        = ''nb_train_with_params.ipynb''\n\nimport base64 as __kfp_b64\nimport gzip
        as __kfp_gzip\nimport io as __kfp_io\nimport os as __kfp_os\nimport sys as
        __kfp_sys\nimport tarfile as __kfp_tarfile\nimport tempfile as __kfp_tempfile\nfrom
        nbclient import NotebookClient\n\n# Extract embedded archive at import time
        to ensure sys.path and globals are set\nprint(''[KFP] Extracting embedded
        notebook archive...'', flush=True)\n__kfp_tmpdir = __kfp_tempfile.TemporaryDirectory()\n__KFP_EMBEDDED_ASSET_DIR
        = __kfp_tmpdir.name\ntry:\n    __kfp_bytes = __kfp_b64.b64decode(__KFP_EMBEDDED_ARCHIVE_B64.encode(''ascii''))\n    with
        __kfp_tarfile.open(fileobj=__kfp_io.BytesIO(__kfp_bytes), mode=''r:gz'') as
        __kfp_tar:\n        __kfp_tar.extractall(path=__KFP_EMBEDDED_ASSET_DIR)\n    print(f''[KFP]
        Notebook archive extracted to: {__KFP_EMBEDDED_ASSET_DIR}'', flush=True)\nexcept
        Exception as __kfp_e:\n    raise RuntimeError(f''Failed to extract embedded
        notebook archive: {__kfp_e}'')\n\n# Always prepend the extracted directory
        to sys.path for import resolution\nif __KFP_EMBEDDED_ASSET_DIR not in __kfp_sys.path:\n    __kfp_sys.path.insert(0,
        __KFP_EMBEDDED_ASSET_DIR)\n    print(f''[KFP] Added notebook archive directory
        to Python path'', flush=True)\n\n# Optional convenience for generic embedded
        file variable name\n__KFP_EMBEDDED_ASSET_FILE = __kfp_os.path.join(__KFP_EMBEDDED_ASSET_DIR,
        __KFP_NOTEBOOK_REL_PATH)\n\n\nclass KFPStreamingNotebookClient(NotebookClient):\n    #
        Streams outputs in real-time by emitting outputs during message processing.\n    def
        process_message(self, msg, cell, cell_index):\n        # Call the parent implementation
        to handle the message normally\n        output = super().process_message(msg,
        cell, cell_index)\n\n        # If an output was created, stream it immediately\n        if
        output is not None:\n            _kfp_stream_single_output(output, cell_index)\n\n        return
        output\n\ndef __kfp_write_parameters_cell(nb, params):\n    \"\"\"Inject parameters
        following Papermill semantics.\n\n    - If a cell tagged with ''parameters''
        exists, insert an overriding\n      ''injected-parameters'' cell immediately
        after it.\n    - Otherwise, insert the ''injected-parameters'' cell at the
        top.\n    \"\"\"\n    import json\n\n    import nbformat\n\n    if not params:\n        return\n\n    #
        Build the injected parameters cell\n    assignments = []\n    for key, value
        in params.items():\n        serialized = json.dumps(value)\n        assignments.append(key
        + '' = json.loads('' + repr(serialized) + '')'')\n    source = ''import json\\n''
        + ''\\n''.join(assignments) + ''\\n''\n    cell = nbformat.v4.new_code_cell(source=source)\n    cell.metadata.setdefault(''tags'',
        [])\n    if ''injected-parameters'' not in cell.metadata[''tags'']:\n        cell.metadata[''tags''].append(''injected-parameters'')\n\n    #
        Locate the first ''parameters'' tagged cell\n    insert_idx = 0\n    for idx,
        existing in enumerate(nb.get(''cells'', [])):\n        if existing.get(''cell_type'')
        != ''code'':\n            continue\n        tags = existing.get(''metadata'',
        {}).get(''tags'', []) or []\n        if ''parameters'' in tags:\n            insert_idx
        = idx + 1\n            break\n\n    nb.cells.insert(insert_idx, cell)\n\ndef
        _kfp_stream_single_output(output, cell_idx):\n    \"\"\"Stream a single notebook
        output immediately during execution.\n\n    Prints stdout/stderr and text/plain
        display outputs to the console so users\n    see cell output as it happens
        (no need to wait until the notebook finishes).\n    \"\"\"\n    import sys\n    output_type
        = output.get(''output_type'')\n\n    if output_type == ''stream'':\n        text
        = output.get(''text'', '''')\n        if text:\n            try:\n                print(f''[nb
        cell {cell_idx} stream] '', end='''', flush=False)\n            except Exception:\n                pass\n            print(text,
        end='''' if text.endswith(''\\n'') else ''\\n'', flush=True)\n    elif output_type
        == ''error'':\n        for line in output.get(''traceback'', []):\n            print(line,
        file=sys.stderr, flush=True)\n    else:\n        # Handle display_data and
        execute_result\n        data = output.get(''data'', {})\n        if ''text/plain''
        in data:\n            print(data[''text/plain''], flush=True)\n        elif
        ''application/json'' in data:\n            try:\n                import json
        as __kfp_json\n                parsed = data[''application/json'']\n                #
        Some kernels send JSON as string; try to parse if needed\n                if
        isinstance(parsed, str):\n                    try:\n                        parsed
        = __kfp_json.loads(parsed)\n                    except Exception:\n                        pass\n                print(__kfp_json.dumps(parsed,
        indent=2, ensure_ascii=False), flush=True)\n            except Exception:\n                #
        Fallback to raw\n                print(str(data.get(''application/json'')),
        flush=True)\n        elif ''text/markdown'' in data:\n            # Print
        markdown as-is; frontends may render, logs will show raw markdown\n            print(data[''text/markdown''],
        flush=True)\n\ndef kfp_run_notebook(**kwargs):\n    \"\"\"Execute the embedded
        notebook with injected parameters.\n\n    Parameters provided via kwargs are
        injected into the notebook following\n    Papermill semantics (after a parameters
        cell if present, otherwise at top).\n    Execution uses a Python kernel; nbclient
        and ipykernel must be available at\n    runtime (installed via packages_to_install
        for notebook components).\n    \"\"\"\n    import os\n    import subprocess\n    import
        sys\n\n    from nbclient import NotebookClient\n    import nbformat\n\n    #
        Ensure a usable ''python3'' kernel is present; install kernelspec if missing\n    print(''[KFP
        Notebook] Checking for Python kernel...'', flush=True)\n    try:\n        from
        jupyter_client.kernelspec import KernelSpecManager  # type: ignore\n        ksm
        = KernelSpecManager()\n        have_py3 = ''python3'' in ksm.find_kernel_specs()\n        if
        not have_py3:\n            print(\n                ''[KFP Notebook] Python3
        kernel not found, installing...'',\n                flush=True)\n            try:\n                subprocess.run([\n                    sys.executable,
        ''-m'', ''ipykernel'', ''install'', ''--user'',\n                    ''--name'',
        ''python3'', ''--display-name'', ''Python 3''\n                ],\n                               check=True,\n                               stdout=subprocess.DEVNULL,\n                               stderr=subprocess.DEVNULL)\n                print(\n                    ''[KFP
        Notebook] Python3 kernel installed successfully'',\n                    flush=True)\n            except
        subprocess.CalledProcessError as e:\n                raise RuntimeError(\n                    \"Failed
        to install ''python3'' kernelspec for ipykernel. \"\n                    \"Ensure
        ipykernel is available in the environment or include it via packages_to_install.
        \"\n                    f\"Error: {e}\") from e\n        else:\n            print(''[KFP
        Notebook] Python3 kernel found'', flush=True)\n    except ImportError as e:\n        raise
        RuntimeError(\n            \"jupyter_client is not available. Ensure it''s
        installed in the environment or include it via packages_to_install. \"\n            f\"Error:
        {e}\") from e\n\n    nb_path = os.path.join(__KFP_EMBEDDED_ASSET_DIR, __KFP_NOTEBOOK_REL_PATH)\n\n    try:\n        nb
        = nbformat.read(nb_path, as_version=4)\n    except Exception as e:\n        raise
        RuntimeError(\n            f''Failed to read notebook {nb_path}. Ensure it
        is a valid Jupyter notebook. Error: {e}''\n        ) from e\n\n    try:\n        __kfp_write_parameters_cell(nb,
        kwargs)\n        print(\n            f''[KFP Notebook] Executing notebook
        with {len(nb.get(\"cells\", []))} cells'',\n            flush=True)\n\n        #
        Use our custom streaming client for real-time output (defined in the\n        #
        generated ephemeral source)\n        client = KFPStreamingNotebookClient(\n            nb,\n            timeout=None,\n            allow_errors=False,\n            store_widget_state=False,\n            kernel_name=''python3'')\n        client.execute(cwd=__KFP_EMBEDDED_ASSET_DIR)\n\n        print(''[KFP
        Notebook] Execution complete'', flush=True)\n\n    except Exception as e:\n        raise
        RuntimeError(f''Notebook execution failed. Error: {e}'') from e\n\n\n# Bind
        helper into dsl namespace so user code can call dsl.run_notebook(...)\ndsl.run_notebook
        = kfp_run_notebook\n\n\ndef train_model(cleaned_text: dsl.Input[dsl.Dataset],
        model: dsl.Output[dsl.Model]):\n    import shutil\n\n    with open(cleaned_text.path,
        \"r\", encoding=\"utf-8\") as f:\n        cleaned_text = f.read()\n\n    dsl.run_notebook(cleaned_text=cleaned_text)\n\n    #
        Notebook writes its model into /tmp/kfp_nb_outputs/model.txt\n    shutil.copy(\"/tmp/kfp_nb_outputs/model.txt\",
        model.path)\n\n    with open(model.path, \"r\", encoding=\"utf-8\") as f:\n        model_text
        = f.read()\n\n    assert model_text == cleaned_text.upper()\n\n"],"image":"python:3.9"}'
    - name: components-root
      value: '{"dag":{"tasks":{"evaluate-model":{"cachingOptions":{},"componentRef":{"name":"comp-evaluate-model"},"dependentTasks":["train-model"],"inputs":{"artifacts":{"model_text":{"taskOutputArtifact":{"outputArtifactKey":"model","producerTask":"train-model"}}}},"taskInfo":{"name":"evaluate-model"}},"preprocess":{"cachingOptions":{},"componentRef":{"name":"comp-preprocess"},"inputs":{"parameters":{"text":{"componentInputParameter":"text"}}},"taskInfo":{"name":"preprocess"}},"train-model":{"cachingOptions":{},"componentRef":{"name":"comp-train-model"},"dependentTasks":["preprocess"],"inputs":{"artifacts":{"cleaned_text":{"taskOutputArtifact":{"outputArtifactKey":"dataset","producerTask":"preprocess"}}}},"taskInfo":{"name":"train-model"}}}},"inputDefinitions":{"parameters":{"text":{"defaultValue":"Hello   world","isOptional":true,"parameterType":"STRING"}}}}'
  entrypoint: entrypoint
  podMetadata:
    annotations:
      pipelines.kubeflow.org/v2_component: "true"
    labels:
      pipelines.kubeflow.org/v2_component: "true"
  serviceAccountName: pipeline-runner
  templates:
  - container:
      args:
      - --type
      - CONTAINER
      - --pipeline_name
      - nb-mixed
      - --run_id
      - '{{workflow.uid}}'
      - --run_name
      - '{{workflow.name}}'
      - --run_display_name
      - ""
      - --dag_execution_id
      - '{{inputs.parameters.parent-dag-id}}'
      - --component
      - '{{inputs.parameters.component}}'
      - --task
      - '{{inputs.parameters.task}}'
      - --task_name
      - '{{inputs.parameters.task-name}}'
      - --container
      - '{{inputs.parameters.container}}'
      - --iteration_index
      - '{{inputs.parameters.iteration-index}}'
      - --cached_decision_path
      - '{{outputs.parameters.cached-decision.path}}'
      - --pod_spec_patch_path
      - '{{outputs.parameters.pod-spec-patch.path}}'
      - --condition_path
      - '{{outputs.parameters.condition.path}}'
      - --kubernetes_config
      - '{{inputs.parameters.kubernetes-config}}'
      - --http_proxy
      - ""
      - --https_proxy
      - ""
      - --no_proxy
      - ""
      command:
      - driver
      image: ghcr.io/kubeflow/kfp-driver:latest
      name: ""
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 64Mi
    inputs:
      parameters:
      - name: component
      - name: task
      - name: container
      - name: task-name
      - name: parent-dag-id
      - default: "-1"
        name: iteration-index
      - default: ""
        name: kubernetes-config
    metadata: {}
    name: system-container-driver
    outputs:
      parameters:
      - name: pod-spec-patch
        valueFrom:
          default: ""
          path: /tmp/outputs/pod-spec-patch
      - default: "false"
        name: cached-decision
        valueFrom:
          default: "false"
          path: /tmp/outputs/cached-decision
      - name: condition
        valueFrom:
          default: "true"
          path: /tmp/outputs/condition
  - dag:
      tasks:
      - arguments:
          parameters:
          - name: pod-spec-patch
            value: '{{inputs.parameters.pod-spec-patch}}'
        name: executor
        template: system-container-impl
        when: '{{inputs.parameters.cached-decision}} != true'
    inputs:
      parameters:
      - name: pod-spec-patch
      - default: "false"
        name: cached-decision
    metadata: {}
    name: system-container-executor
    outputs: {}
  - container:
      command:
      - should-be-overridden-during-runtime
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: KFP_POD_UID
        valueFrom:
          fieldRef:
            fieldPath: metadata.uid
      envFrom:
      - configMapRef:
          name: metadata-grpc-configmap
          optional: true
      image: gcr.io/ml-pipeline/should-be-overridden-during-runtime
      name: ""
      resources: {}
      volumeMounts:
      - mountPath: /kfp-launcher
        name: kfp-launcher
      - mountPath: /gcs
        name: gcs-scratch
      - mountPath: /s3
        name: s3-scratch
      - mountPath: /minio
        name: minio-scratch
      - mountPath: /.local
        name: dot-local-scratch
      - mountPath: /.cache
        name: dot-cache-scratch
      - mountPath: /.config
        name: dot-config-scratch
    initContainers:
    - args:
      - --copy
      - /kfp-launcher/launch
      command:
      - launcher-v2
      image: ghcr.io/kubeflow/kfp-launcher:latest
      name: kfp-launcher
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 100m
      volumeMounts:
      - mountPath: /kfp-launcher
        name: kfp-launcher
    inputs:
      parameters:
      - name: pod-spec-patch
    metadata: {}
    name: system-container-impl
    outputs: {}
    podSpecPatch: '{{inputs.parameters.pod-spec-patch}}'
    volumes:
    - emptyDir: {}
      name: kfp-launcher
    - emptyDir: {}
      name: gcs-scratch
    - emptyDir: {}
      name: s3-scratch
    - emptyDir: {}
      name: minio-scratch
    - emptyDir: {}
      name: dot-local-scratch
    - emptyDir: {}
      name: dot-cache-scratch
    - emptyDir: {}
      name: dot-config-scratch
  - dag:
      tasks:
      - arguments:
          parameters:
          - name: component
            value: '{{workflow.parameters.components-43c0d13fb84d9ca119bdf5124174bd3678698b085ea5c314339233390502736a}}'
          - name: task
            value: '{"cachingOptions":{},"componentRef":{"name":"comp-evaluate-model"},"dependentTasks":["train-model"],"inputs":{"artifacts":{"model_text":{"taskOutputArtifact":{"outputArtifactKey":"model","producerTask":"train-model"}}}},"taskInfo":{"name":"evaluate-model"}}'
          - name: container
            value: '{{workflow.parameters.implementations-43c0d13fb84d9ca119bdf5124174bd3678698b085ea5c314339233390502736a}}'
          - name: task-name
            value: evaluate-model
          - name: parent-dag-id
            value: '{{inputs.parameters.parent-dag-id}}'
        depends: train-model.Succeeded
        name: evaluate-model-driver
        template: system-container-driver
      - arguments:
          parameters:
          - name: pod-spec-patch
            value: '{{tasks.evaluate-model-driver.outputs.parameters.pod-spec-patch}}'
          - default: "false"
            name: cached-decision
            value: '{{tasks.evaluate-model-driver.outputs.parameters.cached-decision}}'
        depends: evaluate-model-driver.Succeeded
        name: evaluate-model
        template: system-container-executor
      - arguments:
          parameters:
          - name: component
            value: '{{workflow.parameters.components-de426933840df1d5b1966245bfd35532f6a11ccaffa3dbb2311a6b7dbf3ec609}}'
          - name: task
            value: '{"cachingOptions":{},"componentRef":{"name":"comp-preprocess"},"inputs":{"parameters":{"text":{"componentInputParameter":"text"}}},"taskInfo":{"name":"preprocess"}}'
          - name: container
            value: '{{workflow.parameters.implementations-de426933840df1d5b1966245bfd35532f6a11ccaffa3dbb2311a6b7dbf3ec609}}'
          - name: task-name
            value: preprocess
          - name: parent-dag-id
            value: '{{inputs.parameters.parent-dag-id}}'
        name: preprocess-driver
        template: system-container-driver
      - arguments:
          parameters:
          - name: pod-spec-patch
            value: '{{tasks.preprocess-driver.outputs.parameters.pod-spec-patch}}'
          - default: "false"
            name: cached-decision
            value: '{{tasks.preprocess-driver.outputs.parameters.cached-decision}}'
        depends: preprocess-driver.Succeeded
        name: preprocess
        template: system-container-executor
      - arguments:
          parameters:
          - name: component
            value: '{{workflow.parameters.components-0e704495704924f0f854d4c513ac164f987781625d13c72aabf7e58c7fc1d08c}}'
          - name: task
            value: '{"cachingOptions":{},"componentRef":{"name":"comp-train-model"},"dependentTasks":["preprocess"],"inputs":{"artifacts":{"cleaned_text":{"taskOutputArtifact":{"outputArtifactKey":"dataset","producerTask":"preprocess"}}}},"taskInfo":{"name":"train-model"}}'
          - name: container
            value: '{{workflow.parameters.implementations-0e704495704924f0f854d4c513ac164f987781625d13c72aabf7e58c7fc1d08c}}'
          - name: task-name
            value: train-model
          - name: parent-dag-id
            value: '{{inputs.parameters.parent-dag-id}}'
        depends: preprocess.Succeeded
        name: train-model-driver
        template: system-container-driver
      - arguments:
          parameters:
          - name: pod-spec-patch
            value: '{{tasks.train-model-driver.outputs.parameters.pod-spec-patch}}'
          - default: "false"
            name: cached-decision
            value: '{{tasks.train-model-driver.outputs.parameters.cached-decision}}'
        depends: train-model-driver.Succeeded
        name: train-model
        template: system-container-executor
    inputs:
      parameters:
      - name: parent-dag-id
    metadata: {}
    name: root
    outputs: {}
  - container:
      args:
      - --type
      - '{{inputs.parameters.driver-type}}'
      - --pipeline_name
      - nb-mixed
      - --run_id
      - '{{workflow.uid}}'
      - --run_name
      - '{{workflow.name}}'
      - --run_display_name
      - ""
      - --dag_execution_id
      - '{{inputs.parameters.parent-dag-id}}'
      - --component
      - '{{inputs.parameters.component}}'
      - --task
      - '{{inputs.parameters.task}}'
      - --task_name
      - '{{inputs.parameters.task-name}}'
      - --runtime_config
      - '{{inputs.parameters.runtime-config}}'
      - --iteration_index
      - '{{inputs.parameters.iteration-index}}'
      - --execution_id_path
      - '{{outputs.parameters.execution-id.path}}'
      - --iteration_count_path
      - '{{outputs.parameters.iteration-count.path}}'
      - --condition_path
      - '{{outputs.parameters.condition.path}}'
      - --http_proxy
      - ""
      - --https_proxy
      - ""
      - --no_proxy
      - ""
      command:
      - driver
      image: ghcr.io/kubeflow/kfp-driver:latest
      name: ""
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 64Mi
    inputs:
      parameters:
      - name: component
      - default: ""
        name: runtime-config
      - default: ""
        name: task
      - default: ""
        name: task-name
      - default: "0"
        name: parent-dag-id
      - default: "-1"
        name: iteration-index
      - default: DAG
        name: driver-type
    metadata: {}
    name: system-dag-driver
    outputs:
      parameters:
      - name: execution-id
        valueFrom:
          path: /tmp/outputs/execution-id
      - name: iteration-count
        valueFrom:
          default: "0"
          path: /tmp/outputs/iteration-count
      - name: condition
        valueFrom:
          default: "true"
          path: /tmp/outputs/condition
  - dag:
      tasks:
      - arguments:
          parameters:
          - name: component
            value: '{{workflow.parameters.components-root}}'
          - name: runtime-config
            value: '{"parameterValues":{"text":"Hello   world"}}'
          - name: driver-type
            value: ROOT_DAG
        name: root-driver
        template: system-dag-driver
      - arguments:
          parameters:
          - name: parent-dag-id
            value: '{{tasks.root-driver.outputs.parameters.execution-id}}'
          - name: condition
            value: ""
        depends: root-driver.Succeeded
        name: root
        template: root
    inputs: {}
    metadata: {}
    name: entrypoint
    outputs: {}
status:
  finishedAt: null
  startedAt: null
