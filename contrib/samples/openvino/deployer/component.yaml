name: OpenVINO model server deployer
description: Deploys OpenVINO Model Server instance to Kubernetes and runs model evaluation
inputs:
- name: Batch size
  default: auto
- name: Log level
  default: DEBUG
- name: Model export path
  default: 'gs://intelai_public_models/resnet_50_i8'
- name: Model version policy
  default: '{"latest": { "num_versions":2 }}'
- name: Replicas
  default: '1'
- name: Server name
  default: resnet
- name: Evaluation images list
  default: 'https://raw.githubusercontent.com/IntelAI/OpenVINO-model-server/master/example_client/input_images.txt'
- name: Image path prefix
  default: 'https://github.com/IntelAI/OpenVINO-model-server/raw/master/example_client/'
- name: Model input name
  default: 'data'
- name: Model output name
  default: 'prob'
- name: Model input size
  default: 224
outputs:
- name: Server endpoint
metrics:
  - name: latency
  - name: accuracy
implementation:
  container:
    image: gcr.io/constant-cubist-173123/inference_server/ml_deployer:13
    command: [./deploy.sh]
    args: [
      --model-export-path, {inputValue: Model export path},
      --server-name, {inputValue: Server name},
      --log-level, {inputValue: Log level},
      --batch-size, {inputValue: Batch size},
      --model-version-policy, {inputValue: Model version policy},
      --replicas, {inputValue: Replicas},
      --server-endpoint-output-file, {outputPath: Server endpoint},
    ]
  container:
    image: gcr.io/constant-cubist-173123/inference_server/ml_deployer:13
    command: [./evaluate.py]
    args: [
      --images_list, {inputValue: Evaluation images list},
      --image_path_prefix, {inputValue: Image path prefix},
      --grpc_endpoint, {outputValue: Server endpoint},
      --input_name, {inputValue: Model input name},
      --output_name, {inputValue: Model output name},
      --size, {inputValue: Model input size},
      --replicas, {inputValue: Replicas},
    ]
